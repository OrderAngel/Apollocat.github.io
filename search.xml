<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>EE5907-CA1</title>
    <url>/EE5907-CA1/</url>
    <content><![CDATA[<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><h2 id="1-数据描述"><a href="#1-数据描述" class="headerlink" title="1. 数据描述"></a>1. 数据描述</h2><ol>
<li><p>数据集：SPAM E-mail Database。包含4601封邮件，每个样本有57维特征和一个标签（1=spam，0=!spam）</p>
</li>
<li><p>具体：</p>
</li>
</ol>
<ul>
<li>Xtrain<ul>
<li>3065x57 double</li>
</ul>
</li>
<li>ytrain<ul>
<li>3065x1 double</li>
</ul>
</li>
<li>Xtest<ul>
<li>1536x57 double</li>
</ul>
</li>
<li>ytest<ul>
<li>1536x1 double</li>
</ul>
</li>
</ul>
<ol start="3">
<li>描述：<a href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.info.txt">详情</a> </li>
</ol>
<ul>
<li><p>包括：</p>
<ul>
<li>Spam      1813  (39.4%)</li>
<li>Non-Spam  2788  (60.6%)</li>
</ul>
</li>
<li><p>特征：</p>
<ul>
<li>48 个word_freq_WORD类型的连续实数[0,100]属性 = 电子邮件中匹配WORD的单词的百分比<ul>
<li>即100 *(“word”在邮件中出现的次数)/电子邮件的总字数。在本例中，“word”是由非字母数字字符或字符串结束符限定的任何字母数字字符字符串。</li>
</ul>
</li>
<li>6 个char_freq_CHAR类型的连续实数[0,100]属性 = 电子邮件中匹配CHAR的字符百分比<ul>
<li>即100 *(“char”出现次数)/电子邮件中的字符总数</li>
</ul>
</li>
<li>1 个连续实数[1，…]属性类型为capital_run_length_average = 连续大写字母序列的平均长度</li>
<li>1 个连续整数[1，…]属性类型为capital_run_length_longest = 最长连续大写字母序列的长度</li>
<li>1 个连续整数[1，…]属性类型为capital_run_length_total = 连续的大写字母序列的长度之和 = 电子邮件中大写字母的总数</li>
</ul>
</li>
<li><p>缺失：None</p>
</li>
</ul>
<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2. 数据处理"></a>2. 数据处理</h2><p>可以尝试对特征进行不同的预处理：</p>
<ol>
<li>log-transform：对于每个特征使用ln(xij + 0.1)进行转换</li>
<li>binarization: 对特征进行二值化，如果大于0就设置为1，否则设置为0</li>
</ol>
<h1 id="Beta-binomial朴素贝叶斯"><a href="#Beta-binomial朴素贝叶斯" class="headerlink" title="Beta-binomial朴素贝叶斯"></a>Beta-binomial朴素贝叶斯</h1><h2 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h2><p>在数据处理部分的<strong>二值化数据</strong>上拟合一个beta-二项朴素贝叶斯分类器。</p>
<p>由于存在大量的垃圾邮件和非垃圾邮件，您不需要在类标签上假设任何先验。换句话说，<strong>类别标签的先验λ可以用ML估计</strong>，在测试时可以用λ ML作为插值估计。</p>
<p>另一方面，需要为特征分布假设Beta(α,α)，对于每个α={0,0.5,1,1.5,2,…,100}，在训练数据上拟合分类器并计算其错误率(即，电子邮件分类错误的百分比)。</p>
<p>对于特征（比如计算p(x|y)），请使用朴素贝叶斯（如后验预测）训练并测试。</p>
<p>报告至少需要包含以下内容：</p>
<ul>
<li>绘制关于α的训练和测试的错误率</li>
<li>当α改变时，你观察到的训练和测试的错误率</li>
<li>α=1,10,100时，训练和测试的错误率</li>
</ul>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>1: class 1=spam；</p>
<p>0: class2=!spam</p>
<ul>
<li><p>特征xn经过二项处理后，只有0，1</p>
</li>
<li><p>数据（x,y）是58二项分布；</p>
</li>
<li><p>在训练集上建模：</p>
<p>特征xn是二项，使用每一类的训练样本的特征xn，使用“beta-二项后验预测公式”计算估计x的第n维特征的pdf，即我们会得到class1:2，feature1:57这2*57个二项特征对数似然（放到2x57维矩阵中，每个元素(a,b)表示当y=a时，xb的概率密度）；</p>
<p>以及ML估计的类别先验的对数；</p>
</li>
<li><p>训练+测试（分别在训练集和测试集上进行）：</p>
<p>针对每一个训练/测试数据，分别计算给定特征条件下，类别为1或0的概率。</p>
<p>即，57个特征对应的二项特征对数似然之和+ML估计类别先验对数；</p>
<p>将y=1的概率和y=0的概率对比，选择大概率的作为估计的标签y。</p>
</li>
<li><p>错误率：</p>
<p>对于估计的标签计算分错的样本数量/总样本；</p>
<p>这个操作对测试集和验证集分别搞一遍</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928161328.png"></p>
</li>
</ul>
<h1 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a>高斯朴素贝叶斯</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>在数据处理部分的<strong>log转换数据</strong>上拟合一个高斯朴素贝叶斯分类器。</p>
<p>由于存在大量的垃圾邮件和非垃圾邮件，您不需要在类标签上假设任何先验。换句话说，<strong>类别标签的先验λ可以用ML估计</strong>，在测试时可以用λ ML作为插值估计。</p>
<p>对于本练习，只需使用ML估计每个特征的类条件均值和方差（eta），并使用ML估计作为测试的插值估计</p>
<p>报告至少需要包含以下内容：</p>
<ul>
<li>关于log转换数据的训练和测试的错误率</li>
</ul>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>单变量高斯的ML估计可以得到μ_ML和σ^2_ML</p>
<p>因为#class=2，λ的ML估计可以用二项的N1/N；</p>
<p>eta是给定class=j时，特征的pdf=N(μ_ML,σ^2_ML)</p>
<p>和上一题类似，还是要先获得eta矩阵，矩阵中的每个元素(i,j)对应给定j类别时，第i维特征的条件概率。该值由ML估计的正态分布得到。</p>
<h2 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928161435.png"></p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>##题目<br>对于<strong>log转换数据</strong>，拟合一个l2正则的逻辑回归模型（牛顿方法+l2正则+从l2正则中排除偏移项）。</p>
<p>对于每个正则参数值λ={1,2,…,9,10,15,20,…95,100}，在训练集上拟合逻辑回归模型，并在测试集上计算它的错误率。</p>
<p>报告至少需要包含以下内容：</p>
<ul>
<li>绘制关于λ的训练和测试的错误率</li>
<li>当λ改变时，你观察到的训练和测试的错误率</li>
<li>λ=1,10,100时，训练和测试的错误率</li>
</ul>
<p>不要忘记在逻辑回归中包括偏倚项，而在l2正则化中排除偏移项。</p>
<h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><ul>
<li><p>变量</p>
<ul>
<li>W：(D+1)x1</li>
<li>w：Dx1</li>
<li>X：Nx(D+1)</li>
<li>Xi：(D+1)X1（每个样本的特征）Xi=[1,xi]</li>
<li>μ：里面的Xi和W都是D+1维的，Nx1</li>
<li>y：Nx1</li>
</ul>
</li>
<li><p>函数</p>
<ul>
<li>求μ(W,Xi)—(特征矩阵，w)</li>
<li>求NLLreg(特征矩阵,W，λ)</li>
<li>求greg(特征矩阵,类别矩阵,w,λ)</li>
<li>求Hreg(特征矩阵,类别矩阵,w，λ)</li>
</ul>
</li>
</ul>
<p>数据预处理</p>
<p>初始化：给X加一维’1’；步长1；阈值0.01</p>
<p>大循环：λ；</p>
<p>初始化w0—57+1维0向量（这个不是偏移向量）</p>
<p>中循环：while NLL&lt;阈值【目的是get：W】</p>
<ol>
<li> k+1</li>
<li> greg</li>
<li>hreg</li>
<li>W[k+1]—-要存记录</li>
<li>NLL</li>
</ol>
<p>end</p>
<p>用W预测：训练集+测试集（k，#样本数）</p>
<p>end</p>
<p>计算错误率</p>
<p>画图</p>
<h2 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928161533.png"></p>
<h1 id="k最近邻居"><a href="#k最近邻居" class="headerlink" title="k最近邻居"></a>k最近邻居</h1><h2 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h2><p>在<strong>log转换数据</strong>上，实现KNN分类器。用欧式距离来测量邻居之间的距离。</p>
<p>对于每个K={1, 2, …9,10,15,20,…95,100}，计算训练和测试的错误率。</p>
<p>报告至少需要包含以下内容：</p>
<ul>
<li>绘制关于K的训练和测试的错误率</li>
<li>当K改变时，你观察到的训练和测试的错误率</li>
<li>K=1,10,100时，训练和测试的错误率</li>
</ul>
<h2 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h2><p>先搞个函数来计算两个特征之间的距离；<br>再搞个函数来计算新样本和矩阵（训练集）中每个样本的距离</p>
<p>测试和训练时调用算距离函数，生成距离矩阵，每个点(i，j)表示新样本i和训练集样本j之间的距离。</p>
<p>对于每一个新样本，根据不同的K，得到距离该样本最近的K个训练点，对比其中类0和类1的比例，将新样本的标签预测为占比大的那个类。</p>
<p>训练集上的测试或许可以有一些tricky，因为生产的距离矩阵是对称的，且对角线值为0。</p>
<p>训练集：dist—3065<em>3065<br>测试集：dist—1536</em>3065</p>
<h2 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928161622.png"></p>
]]></content>
      <categories>
        <category>project</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>V-SLAM整理3</title>
    <url>/V-SLAM%E6%95%B4%E7%90%863/</url>
    <content><![CDATA[<h1 id="第9讲-后端1"><a href="#第9讲-后端1" class="headerlink" title="第9讲 后端1"></a>第9讲 后端1</h1><p>状态估计问题：从带噪声的数据估计内在的状态</p>
<p>前端：短时间的状态估计问题。从相机的相邻图像中估计相机运动，可以给出一个短时间内的轨迹和地图。</p>
<p>后端：长时间的状态估计问题。希望构建一个尺度、规模更大的优化问题，来获得长时间内的最优轨迹和地图。</p>
<p>主要方案：</p>
<ul>
<li>渐进：保持当前状态的估计，在加入新信息时，更新已有的估计（滤波器）</li>
<li>批量：给定一定规模的数据，计算该批量数据下的最优估计（优化）【第6讲】<ul>
<li>根据第6讲的玩法，批量状态估计问题可以转换成最大似然估计问题，再用最小二乘法求解。</li>
</ul>
</li>
</ul>
<h2 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h2><h3 id="9-1-1-状态估计的概率解释"><a href="#9-1-1-状态估计的概率解释" class="headerlink" title="9.1.1 状态估计的概率解释"></a>9.1.1 状态估计的概率解释</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930173301.png"></p>
<ol>
<li>已知某些运动数据和观测数据z，如何确定状态量x,y的分布？</li>
<li>如果得到了新时刻的数据，x和y的分布又将怎么变化？</li>
</ol>
<p>假设状态量和噪声项服从高斯分布，问题转变为：已知一些运动数据和观测数据，如何估计状态量的高斯分布？</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930173948.png"></p>
<hr>
<p>改变记号——待估计量xk表示k时刻所有未知量，包括位姿和路标点：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930181033.png"></p>
<p>改写后的SLAM方程：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930181238.png"></p>
<p>目标是用过去0-k时刻的数据直接从x0估计xk：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930181840.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930182259.png"></p>
<p>后续操作有两种方法：</p>
<ul>
<li>假设<code>马尔可夫性</code>：k时刻状态只与k-1时刻状态有关，而与再之前的状态无关<ul>
<li>滤波器方法，如EKF</li>
</ul>
</li>
<li>不假设<code>马尔可夫性</code>，k时刻的状态与之前所有状态有关<ul>
<li>非线性优化</li>
</ul>
</li>
</ul>
<hr>
<h3 id="9-1-2-线性系统和KF"><a href="#9-1-2-线性系统和KF" class="headerlink" title="9.1.2 线性系统和KF"></a>9.1.2 线性系统和KF</h3><p>有关滤波器的部分先略过，有需要再整理</p>
<h3 id="9-1-3-非线性系统和EKF"><a href="#9-1-3-非线性系统和EKF" class="headerlink" title="9.1.3 非线性系统和EKF"></a>9.1.3 非线性系统和EKF</h3><p>EKF：在某段时间内估计某个不确定的量</p>
<h3 id="9-1-4-EKF的讨论"><a href="#9-1-4-EKF的讨论" class="headerlink" title="9.1.4 EKF的讨论"></a>9.1.4 EKF的讨论</h3><hr>
<h2 id="9-2-BA与图优化"><a href="#9-2-BA与图优化" class="headerlink" title="9.2 BA与图优化"></a>9.2 BA与图优化</h2><p>Bundle Adjustment：从视觉图像中提炼出最优的3D模型和相机参数（内参+外参），即带有相机位姿和空间点的图优化称为BA。</p>
<p>BA能够精准的优化每个相机位姿与特征点的位置。</p>
<p>形象点解释，就是通过调整相机位姿和特征点的空间位置，使得从任意特征点发出的光线最终收束到相机的光心。</p>
<p>注意：BA只有观测方程</p>
<h3 id="9-2-1-投影模型和BA代价函数"><a href="#9-2-1-投影模型和BA代价函数" class="headerlink" title="9.2.1 投影模型和BA代价函数"></a>9.2.1 投影模型和BA代价函数</h3><p>投影过程：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930183745.png"></p>
<p>所谓观测数据，指的是相机得到的图像中的像素坐标[u,v]^T</p>
<p>实际观测数据z 与 根据投影过程估计的观测点的<strong>误差</strong>：请参考7.7.3</p>
<ul>
<li><p>这里的T是相机位姿x(R,t)对应的李群T，p是路标</p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930183911.png"></p>
</li>
</ul>
<p>整体的代价函数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930184817.png"></p>
<p>对这个最小二乘求解，相当于同时调整位姿和路标，也就是BA。</p>
<h3 id="9-2-2-BA的求解"><a href="#9-2-2-BA的求解" class="headerlink" title="9.2.2 BA的求解"></a>9.2.2 BA的求解</h3><p>请参考第6讲的非线性优化的思路。</p>
<p>待优化变量：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930185614.png"></p>
<p>目标函数：F是整体代价函数对相机位姿的偏导，E是整体代价函数对路标的偏导：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930185634.png"></p>
<p>F和E的推导请参考7.7.3</p>
<p>化简：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930190230.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930190246.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930190258.png"></p>
<p>增量方程：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930190333.png"></p>
<p>J = [F E]</p>
<ul>
<li>高斯牛顿：H=JJ</li>
<li>阻尼牛顿：H=JJ+λI</li>
</ul>
<p>高斯牛顿的H：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930190656.png"></p>
<p>这个H很庞大，对它求逆看上去非常艰难，但是好在H具有一定特殊结构，会好求一些。下面就说说H的特殊结构。</p>
<h3 id="9-2-3-稀疏性和边缘化"><a href="#9-2-3-稀疏性和边缘化" class="headerlink" title="9.2.3 稀疏性和边缘化"></a>9.2.3 稀疏性和边缘化</h3><p>H具有稀疏结构，并且可以用图优化来表示！</p>
<p>由于在Ti看到pj这件事与其他轨迹和路标点无关，这项的误差也与其他无关，因此求导是0。</p>
<p>i在相机位姿中取值，j在路标点中取值：</p>
<p>H11只与相机位姿有关，H22只与路标点有关。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930191753.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930191646.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930191550.png"></p>
<p>以下事实成立：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930191941.png"></p>
<p><strong>Ace. 现在有两个相机位姿和6个路标点：</strong></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930192106.png"></p>
<p>以e11为例，这是C1处看到P1这件事的误差项，其雅克比矩阵为J11：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930192341.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930192454.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930192614.png"></p>
<p>邻接矩阵和H矩阵除了对角线元素之外的其余部分结构一致:</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930193635.png"></p>
<p>H矩阵的非对角部分可以体现约束关系：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930193252.png"></p>
<p><strong>一般的H矩阵长这样：</strong></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930193336.png"></p>
<hr>
<p>求解这样稀疏结构的H矩阵：<code>边缘化</code></p>
<ul>
<li><code>Schur消元</code></li>
<li><code>Cholesky分解</code></li>
</ul>
<p>以下是<code>Schur消元</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930193720.png"></p>
<p>那么H△x=g变成：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930193852.png"></p>
<p>经过一波化简：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930194012.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930194148.png"></p>
<ul>
<li>此方程系数记为S：  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930194422.png"></li>
</ul>
<p>先求出△xc再求△xp：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930194304.png"></p>
<ul>
<li><p>关于S：S矩阵的非对角线上的非0矩阵块，表示该处对应的两个相机变量之间存在着共同观测的路标点，又名<code>共视</code>。</p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930194826.png"></p>
</li>
<li><p>S矩阵的稀疏性结构取决于实际观测的结果</p>
<ul>
<li><code>ORB-SLAM</code>中的<code>Local Mapping</code>，在做BA的时候刻意选择那些具有共同观测的帧作为关键帧（这帧被多个相机看到），在这种情况下爱，Schur消元后得到的S就是<code>稠密矩阵</code>，计算开销大。但是该模块不是实时进行的，所以也ok。</li>
<li>然而，对于别的一些需要实时算BA的方法，S的稠密就变得不可忍受了。比如DSO、OKVIS等采用<code>滑动窗口</code>，该方法对每一帧都要求做一次BA来防止误差累计，因此必须采取一些技巧来保持S矩阵的稀疏性。</li>
</ul>
</li>
</ul>
<h3 id="9-2-4-鲁棒核函数"><a href="#9-2-4-鲁棒核函数" class="headerlink" title="9.2.4 鲁棒核函数"></a>9.2.4 鲁棒核函数</h3><p>如果某个误差项的数据是错误的，意味着突然观测到了一次不可能产生的数据，在图优化中会有一条误差很大的边，会严重影响到其他正确的边。</p>
<p>解决方法：<code>鲁棒核函数</code>——保证每条边的误差不会大的离谱而掩盖其他的边</p>
<p>具体操作是把目标函数中的<code>二范数度量</code>换成其他增长没那么快的函数。</p>
<p>鲁棒核函数：</p>
<ul>
<li>Huber核</li>
<li>Cauchy核</li>
<li>Tukey核</li>
</ul>
<p><code>g2o和Ceres中有提供一些核函数</code></p>
<h2 id="9-3-编程：Ceres-BA"><a href="#9-3-编程：Ceres-BA" class="headerlink" title="9.3 编程：Ceres BA"></a>9.3 编程：Ceres BA</h2><h3 id="9-3-1-BAL数据集"><a href="#9-3-1-BAL数据集" class="headerlink" title="9.3.1 BAL数据集"></a>9.3.1 BAL数据集</h3><p>BAL数据集提供若干个场景</p>
<p><code>Meshlab软件</code>可以查看<code>ply</code>点云文件</p>
<h3 id="9-3-2-Ceres-BA的书写"><a href="#9-3-2-Ceres-BA的书写" class="headerlink" title="9.3.2 Ceres BA的书写"></a>9.3.2 Ceres BA的书写</h3><h2 id="9-4-编程：g2o求解BA"><a href="#9-4-编程：g2o求解BA" class="headerlink" title="9.4 编程：g2o求解BA"></a>9.4 编程：g2o求解BA</h2><hr>
<h1 id="第10讲-后端2"><a href="#第10讲-后端2" class="headerlink" title="第10讲 后端2"></a>第10讲 后端2</h1><p>BA能够精准优化每个相机位姿和特征点位置，有效求解大规模定位与建图问题。但是在更大的场景中，大量特征点的存在会导致庞大的计算开销，难以实时化。</p>
<p>在SLAM中，为了保证实时性，需要控制BA的规模。</p>
<p>例如，有一种简化的BA：位姿图。</p>
<h2 id="10-1-滑动窗口滤波和优化"><a href="#10-1-滑动窗口滤波和优化" class="headerlink" title="10.1 滑动窗口滤波和优化"></a>10.1 滑动窗口滤波和优化</h2><h3 id="10-1-1-实际环境下的BA结构"><a href="#10-1-1-实际环境下的BA结构" class="headerlink" title="10.1.1 实际环境下的BA结构"></a>10.1.1 实际环境下的BA结构</h3><p>控制BA规模的方法：从连续的视频中抽取一部分作为<code>关键帧</code>，仅构造关键帧与路标点之间的BA，非关键帧只用于定位，对建图无贡献。</p>
<ul>
<li><code>滑动窗口法</code>：取N个离当前时刻最近的<code>关键帧</code>，去掉更早的关键帧，将BA控制在一个时间窗口内。</li>
<li><code>共视图法</code>：在BA优化时，按照某种原则在共视图内取一些<code>关键帧</code>和路标进行优化（例如，仅优化与当前帧有20个以上共视路标的关键帧，固定其他部分）<ul>
<li>共视图是指与现在的相机存在共同观测的关键帧构成的图。</li>
<li>例如<code>ORB-SLAM</code></li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930204101.png"></p>
<hr>
<h3 id="10-1-2-滑动窗口法"><a href="#10-1-2-滑动窗口法" class="headerlink" title="10.1.2 滑动窗口法"></a>10.1.2 滑动窗口法</h3><p>有需要的时候再整理</p>
<h4 id="1-新增一个关键帧和路标点"><a href="#1-新增一个关键帧和路标点" class="headerlink" title="1) 新增一个关键帧和路标点"></a>1) 新增一个关键帧和路标点</h4><h4 id="2-删除一个旧的关键帧"><a href="#2-删除一个旧的关键帧" class="headerlink" title="2) 删除一个旧的关键帧"></a>2) 删除一个旧的关键帧</h4><h4 id="3-SWF中边缘化的直观解释"><a href="#3-SWF中边缘化的直观解释" class="headerlink" title="3) SWF中边缘化的直观解释"></a>3) SWF中边缘化的直观解释</h4><hr>
<h2 id="10-2-位姿图"><a href="#10-2-位姿图" class="headerlink" title="10.2 位姿图"></a>10.2 位姿图</h2><p>有需要的时候再整理</p>
<h3 id="10-2-1-位姿图的意义"><a href="#10-2-1-位姿图的意义" class="headerlink" title="10.2.1 位姿图的意义"></a>10.2.1 位姿图的意义</h3><h3 id="10-2-2-位姿图的优化"><a href="#10-2-2-位姿图的优化" class="headerlink" title="10.2.2 位姿图的优化"></a>10.2.2 位姿图的优化</h3><h2 id="10-3-编程：位姿图优化"><a href="#10-3-编程：位姿图优化" class="headerlink" title="10.3 编程：位姿图优化"></a>10.3 编程：位姿图优化</h2><h3 id="10-3-1-g2o原生位姿图"><a href="#10-3-1-g2o原生位姿图" class="headerlink" title="10.3.1 g2o原生位姿图"></a>10.3.1 g2o原生位姿图</h3><h3 id="10-3-2-李代数上的位姿图优化"><a href="#10-3-2-李代数上的位姿图优化" class="headerlink" title="10.3.2 李代数上的位姿图优化"></a>10.3.2 李代数上的位姿图优化</h3><h3 id="10-3-3-小结"><a href="#10-3-3-小结" class="headerlink" title="10.3.3 小结"></a>10.3.3 小结</h3>]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>V-SLAM整理4</title>
    <url>/V-SLAM%E6%95%B4%E7%90%864/</url>
    <content><![CDATA[<h1 id="第11讲-回环检测"><a href="#第11讲-回环检测" class="headerlink" title="第11讲 回环检测"></a>第11讲 回环检测</h1><h2 id="11-1-概述"><a href="#11-1-概述" class="headerlink" title="11.1 概述"></a>11.1 概述</h2><h3 id="11-1-1-回环检测的意义"><a href="#11-1-1-回环检测的意义" class="headerlink" title="11.1.1 回环检测的意义"></a>11.1.1 回环检测的意义</h3><p><code>累积误差</code>在所难免，导致<code>漂移</code>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930230521.png"></p>
<p>回环检测模块能够给出除了相邻帧的一些更久远的约束（经过同一处），其关键在于如何有效的检测出相机经过了同一个地方。</p>
<p>意义：</p>
<ul>
<li>关系到估计的轨迹和地图在长时间下的正确性</li>
<li>回环检测可以提供当前数据与历史数据的关系，因此可以利用回环检测做<code>重定位</code></li>
</ul>
<h3 id="11-1-2-回环检测的方法"><a href="#11-1-2-回环检测的方法" class="headerlink" title="11.1.2 回环检测的方法"></a>11.1.2 回环检测的方法</h3><p>一些方法：</p>
<ul>
<li>基于外观的回环检测<ul>
<li>核心是计算图像间的相似度</li>
</ul>
</li>
<li>室外：GPS提供全局信息</li>
</ul>
<p>两个概念：</p>
<ul>
<li>感知偏差</li>
<li>感知变异</li>
</ul>
<h3 id="11-1-3-准确率和召回率"><a href="#11-1-3-准确率和召回率" class="headerlink" title="11.1.3 准确率和召回率"></a>11.1.3 准确率和召回率</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930235258.png"></p>
<p>准确率 &amp; 召回率：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930235338.png"></p>
<ul>
<li>准确率：提取的所有回环中确实是真实回环的概率</li>
<li>召回率：真实回环中被正确检测出来的概率</li>
</ul>
<p>一般为了评价算法的好坏，会做Precision-Recall曲线。</p>
<ul>
<li>SLAM中对准确率要求更高，对召回率相对宽容。<ul>
<li>回环检测算法参数设置更严格</li>
<li>检测之后加上回环验证步骤</li>
</ul>
</li>
</ul>
<h2 id="11-2-词袋模型BoW"><a href="#11-2-词袋模型BoW" class="headerlink" title="11.2 词袋模型BoW"></a>11.2 词袋模型BoW</h2><p>词袋的目的是用“图像上有哪几种特征”来描述一幅图像，重点是“是否出现某特征”，而不是“在哪出现某特征”</p>
<p><strong>【试试NLP？】</strong></p>
<p>步骤：</p>
<ul>
<li>单词-字典</li>
<li>确定图像中出现了字典中的哪些单词：图像转换成一个向量的<code>描述</code></li>
<li>比较上一步中的<code>描述</code>的相似度</li>
</ul>
<h2 id="11-3-字典"><a href="#11-3-字典" class="headerlink" title="11.3 字典"></a>11.3 字典</h2><h3 id="11-3-1-字典的结构"><a href="#11-3-1-字典的结构" class="headerlink" title="11.3.1 字典的结构"></a>11.3.1 字典的结构</h3><p>字典生成问题类似于一个<code>聚类</code>问题。</p>
<p>已知对大量图像提取的特征点，目标是找一个含k个单词的字典，每个单词是局部相邻特征点的集合。也就是说，有N和数据，想把他们归成k个类，用<code>K均值算法</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001001713.png"></p>
<p>进阶：层次聚类法、K-mean++等</p>
<hr>
<p>我们已经将提取到的大量特征点聚类成了含有k个单词的字典，现在的问题是：如何根据图像中某个特征点查找字典中相应的单词。</p>
<p>方法：</p>
<ul>
<li>Fabmap</li>
<li>Chou-Liu tree</li>
<li>k叉树</li>
</ul>
<p>为了提高查找速度，在K-mean基础上使用<code>k叉树</code>来表达字典:</p>
<p>k叉树：N个特征点，目标是构建一个深度为d、每次分叉为k的数。该字典有k^d个单词。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001002457.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001002622.png"></p>
<h3 id="11-3-2-编程：创建字典"><a href="#11-3-2-编程：创建字典" class="headerlink" title="11.3.2 编程：创建字典"></a>11.3.2 编程：创建字典</h3><p>生成、使用ORB字典</p>
<p>BoW库：DBoW3</p>
<h2 id="11-4-相似度计算"><a href="#11-4-相似度计算" class="headerlink" title="11.4 相似度计算"></a>11.4 相似度计算</h2><h3 id="11-4-1-理论部分"><a href="#11-4-1-理论部分" class="headerlink" title="11.4.1 理论部分"></a>11.4.1 理论部分</h3><p>对于任意特征fx，都可以在字典树中找到与之对应的单词wj。</p>
<p>我们还希望对单词的区分性或重要性加以评估——附权值：</p>
<p>方案：<code>TF-IDF</code></p>
<ul>
<li>TF：某单词在<code>一幅图像</code>中越经常出现，它的区分度就越高</li>
<li>IDF：某单词在<code>字典</code>中出现的频率越低，用它分类图像时区分度越高</li>
</ul>
<p><code>IDF</code>：在建立字典时计算<code>IDF</code>，统计某叶子节点wi中特征数量相对于所有特征数量的比例</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001005450.png"></p>
<p><code>TF</code>：某个特征在单幅图像中出现的频率</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001010522.png"></p>
<p>wi权重：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001010543.png"></p>
<p>对于某图像A，通过词袋，可以用单个向量vA来描述它：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001010639.png"></p>
<p>A、B图像差异性计算（相似度=1-差异性）：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001010701.png"></p>
<h3 id="11-4-2-编程：相似度的计算"><a href="#11-4-2-编程：相似度的计算" class="headerlink" title="11.4.2 编程：相似度的计算"></a>11.4.2 编程：相似度的计算</h3><p>代码没有出错，但结果并不令人满意：相似图像的相似度5%，无关图像相似度2%</p>
<h2 id="11-5-实验分析与评述"><a href="#11-5-实验分析与评述" class="headerlink" title="11.5 实验分析与评述"></a>11.5 实验分析与评述</h2><h3 id="11-5-1-增加字典规模"><a href="#11-5-1-增加字典规模" class="headerlink" title="11.5.1 增加字典规模"></a>11.5.1 增加字典规模</h3><p>使用更大的字典检测图像相似度，效果能好一点</p>
<h3 id="11-5-2-相似性评分的处理"><a href="#11-5-2-相似性评分的处理" class="headerlink" title="11.5.2 相似性评分的处理"></a>11.5.2 相似性评分的处理</h3><p>引入<code>先验相似度</code>：表示某时刻关键帧图像与上一时刻关键帧的相似性</p>
<p>用<code>先验相似度</code>对相似度进行归一化处理：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211001011357.png"></p>
<p>如果当前帧与历史关键帧的相似度 超过了 当前帧与上一个关键相似度的3倍——&gt;存在回环</p>
<ul>
<li>避免引入绝对相似性阈值</li>
</ul>
<h3 id="11-5-3-关键帧的处理"><a href="#11-5-3-关键帧的处理" class="headerlink" title="11.5.3 关键帧的处理"></a>11.5.3 关键帧的处理</h3><p>如果关键帧选的太近，会导致两相近帧的相似度过高，不利于在历史帧中检测回环。因此，用于回环检测的帧最好稀疏一些，彼此区别，且覆盖整个环境。</p>
<p>另外，为了避免重复检测，会把”相近”的的回环聚成一类，以免算法总重复检测回环n帧的邻居们。</p>
<h3 id="11-5-4-检测之后的验证"><a href="#11-5-4-检测之后的验证" class="headerlink" title="11.5.4 检测之后的验证"></a>11.5.4 检测之后的验证</h3><p>为了降低假阳性（感知偏差），在回环检测之后加设验证步骤。</p>
<p>方法：</p>
<ul>
<li>时间一致性检测：设立回环的缓存机制，只有一段时间中一直检测到的回环，才是正确的回环</li>
<li>空间一致性检测：对回环检测到的两帧进行特征匹配，估计相机运动。在之前的位姿图中，检测运动是否有很大变化。<h3 id="11-5-5-与机器学习的关系"><a href="#11-5-5-与机器学习的关系" class="headerlink" title="11.5.5 与机器学习的关系"></a>11.5.5 与机器学习的关系</h3>聚类问题</li>
</ul>
<p>可以引入CNN，比如BoW的改进版VLAD</p>
<hr>
<h1 id="第12讲-建图"><a href="#第12讲-建图" class="headerlink" title="第12讲 建图"></a>第12讲 建图</h1><p>在实际使用SLAM时，除了对相机本体进行定位，还有许多其他需求，</p>
<ul>
<li>定位</li>
<li>导航</li>
<li>避障</li>
<li>交互</li>
</ul>
<p>显然，特征点地图不能满足所有需求，需要根据需求引入各种形式的地图。</p>
<h2 id="12-1-概述"><a href="#12-1-概述" class="headerlink" title="12.1 概述"></a>12.1 概述</h2><p>有需要再整理</p>
<h2 id="12-2-单目稠密重建"><a href="#12-2-单目稠密重建" class="headerlink" title="12.2 单目稠密重建"></a>12.2 单目稠密重建</h2><h3 id="12-2-1-立体视觉"><a href="#12-2-1-立体视觉" class="headerlink" title="12.2.1 立体视觉"></a>12.2.1 立体视觉</h3><h3 id="12-2-2-极线搜索与块匹配"><a href="#12-2-2-极线搜索与块匹配" class="headerlink" title="12.2.2 极线搜索与块匹配"></a>12.2.2 极线搜索与块匹配</h3><h3 id="12-2-3-高斯分布的深度滤波器"><a href="#12-2-3-高斯分布的深度滤波器" class="headerlink" title="12.2.3 高斯分布的深度滤波器"></a>12.2.3 高斯分布的深度滤波器</h3><h2 id="12-3-编程：单目稠密重建"><a href="#12-3-编程：单目稠密重建" class="headerlink" title="12.3 编程：单目稠密重建"></a>12.3 编程：单目稠密重建</h2><h3 id="12-3-1-实验分析与讨论"><a href="#12-3-1-实验分析与讨论" class="headerlink" title="12.3.1 实验分析与讨论"></a>12.3.1 实验分析与讨论</h3><h3 id="12-3-2-像素梯度的问题"><a href="#12-3-2-像素梯度的问题" class="headerlink" title="12.3.2 像素梯度的问题"></a>12.3.2 像素梯度的问题</h3><h3 id="12-3-3-逆深度"><a href="#12-3-3-逆深度" class="headerlink" title="12.3.3 逆深度"></a>12.3.3 逆深度</h3><h3 id="12-3-4-图像间的变换"><a href="#12-3-4-图像间的变换" class="headerlink" title="12.3.4 图像间的变换"></a>12.3.4 图像间的变换</h3><h3 id="12-3-5-并行化：效率的问题"><a href="#12-3-5-并行化：效率的问题" class="headerlink" title="12.3.5 并行化：效率的问题"></a>12.3.5 并行化：效率的问题</h3><h3 id="12-3-6-其他的改进"><a href="#12-3-6-其他的改进" class="headerlink" title="12.3.6 其他的改进"></a>12.3.6 其他的改进</h3><h2 id="12-4-RGB-D稠密建图"><a href="#12-4-RGB-D稠密建图" class="headerlink" title="12.4 RGB-D稠密建图"></a>12.4 RGB-D稠密建图</h2><h3 id="12-4-1-编程：点云地图"><a href="#12-4-1-编程：点云地图" class="headerlink" title="12.4.1 编程：点云地图"></a>12.4.1 编程：点云地图</h3><p>点云库PCL</p>
<h3 id="12-4-2-从点云重建网络"><a href="#12-4-2-从点云重建网络" class="headerlink" title="12.4.2 从点云重建网络"></a>12.4.2 从点云重建网络</h3><h3 id="12-4-3-八叉树地图"><a href="#12-4-3-八叉树地图" class="headerlink" title="12.4.3 八叉树地图"></a>12.4.3 八叉树地图</h3><h3 id="12-4-4-编程：八叉树地图"><a href="#12-4-4-编程：八叉树地图" class="headerlink" title="12.4.4 编程：八叉树地图"></a>12.4.4 编程：八叉树地图</h3><h2 id="12-5-TSDF地图和Fusion系列"><a href="#12-5-TSDF地图和Fusion系列" class="headerlink" title="12.5 TSDF地图和Fusion系列"></a>12.5 TSDF地图和Fusion系列</h2><h2 id="12-6-小结"><a href="#12-6-小结" class="headerlink" title="12.6 小结"></a>12.6 小结</h2>]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>本科知识整理</title>
    <url>/%E6%9C%AC%E7%A7%91%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h1 id="算法设计与分析"><a href="#算法设计与分析" class="headerlink" title="算法设计与分析"></a>算法设计与分析</h1><p>《算法设计与分析第三版-王晓东》</p>
<ol>
<li>算法引论<ol>
<li>算法与程序</li>
<li>表达算法的抽象机制</li>
<li>描述算法</li>
<li>算法复杂性分析</li>
</ol>
</li>
<li>递归与分治策略<ol>
<li>递归的概念</li>
<li>分治法的基本思想</li>
<li>二分搜索技术</li>
<li>大整数的乘法</li>
<li>Strassen矩阵乘法</li>
<li>棋盘覆盖</li>
<li>合并排序</li>
<li>快速排序</li>
<li>线性时间选择</li>
<li>最接近点对问题</li>
<li>循环赛日程表</li>
</ol>
</li>
<li>动态规划<ol>
<li> 矩阵连乘问题</li>
<li> 动态规划算法的基本要素</li>
<li> 最长公共子序列</li>
<li> 凸多边形最优三角划分</li>
<li> 多边形游戏</li>
<li> 图像压缩</li>
<li> 电路布线</li>
<li> 流水作业调度</li>
<li> 0-1背包问题</li>
<li>最优二叉搜索树</li>
</ol>
</li>
<li>贪心算法<ol>
<li> 活动安排问题</li>
<li>贪心算法的基本要素<ol>
<li> 贪心选择性质</li>
<li> 最优子结构性质</li>
<li> 贪心算法与动态规划算法的差异</li>
</ol>
</li>
<li>最优装载</li>
<li>哈夫曼编码<ol>
<li>前编码</li>
<li>构造哈弗曼编码</li>
<li>哈夫曼算法的正确性</li>
</ol>
</li>
<li>单源最短路径<ol>
<li>算法基本思想</li>
<li>算法的正确性和计算复杂性</li>
</ol>
</li>
<li>最小生成树<ol>
<li>最小生成树性质</li>
<li>Prim算法</li>
<li>Kruskal算法</li>
<li>多机调度问题</li>
<li>贪心算法的理论基础<ol>
<li>拟阵</li>
<li>带权拟阵的贪心算法</li>
<li>任务时间表问题</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>回溯法<ol>
<li>回溯法的算法框架<ol>
<li>问题的解空间</li>
<li>回溯法的基本思想</li>
<li>递归回溯</li>
<li>迭代回溯</li>
<li>子集树与排列数</li>
</ol>
</li>
<li>装载问题</li>
<li>批处理作业调度</li>
<li>符号三角形问题</li>
<li>n后问题</li>
<li>0-1背包问题</li>
<li>最大团问题</li>
<li>图的m着色问题</li>
<li>旅行售货员问题</li>
<li>圆排列问题</li>
<li>电路板排列问题</li>
<li>连续邮资问题</li>
<li>回溯法的效率分析</li>
</ol>
</li>
<li>分支限界法<ol>
<li>分支限界法的基本思想</li>
<li>单源最短路径问题</li>
<li>装载问题</li>
<li>布线问题</li>
<li>0-1背包问题</li>
<li>最大团问题</li>
<li>旅行售货员问题</li>
<li>电路板排列问题</li>
<li>批处理作业调度</li>
</ol>
</li>
<li>概率算法<ol>
<li> 随机数</li>
<li>数值概率算法<ol>
<li> 用随机投点法计算π值</li>
<li> 计算定积分</li>
<li> 解非线性方程组</li>
</ol>
</li>
<li>舍伍德算法<ol>
<li>线性时间选择算法</li>
<li>跳跃表</li>
</ol>
</li>
<li>拉斯维加斯算法<ol>
<li>n后问题</li>
<li>整数因子分解</li>
</ol>
</li>
<li>蒙特卡罗算法<ol>
<li>蒙特卡罗算法的基本思想</li>
<li>主元素问题</li>
<li>素数测试</li>
</ol>
</li>
</ol>
</li>
<li>NP完全性理论<ol>
<li>计算模型<ol>
<li>随机存取机RAM</li>
<li>随机存取存储程序机RASP</li>
<li>RAM，模型的变形与简化</li>
<li>图灵机</li>
<li>图灵机模型与RAM模型的关系</li>
<li>问题变换与计算复杂性归约</li>
</ol>
</li>
<li>P类与NP类问题<ol>
<li>非确定性图灵机</li>
<li>P类与NP类语言</li>
<li>多项式时间验证</li>
</ol>
</li>
<li>NP完全问题<ol>
<li>多项式时间变换</li>
<li>COok定理</li>
</ol>
</li>
<li>一些典型的NP完全问题<ol>
<li>合取范式的可满足性问题</li>
<li>3元合取范式的可满足性问题</li>
<li>团问题</li>
<li>顶点覆盖问题</li>
<li>子集和问题</li>
<li>哈密顿回路问题</li>
<li>旅行售货员问题</li>
</ol>
</li>
</ol>
</li>
<li> 近似算法</li>
<li>近似算法的性能</li>
<li>顶点覆盖问题的近似算法</li>
<li>旅行售货员问题的近似算法<ol>
<li>具有三角不等式性质的旅行售货员问题</li>
<li>一般的旅行售货员问题</li>
</ol>
</li>
<li>集合覆盖问题的近似算法</li>
<li>子集和问题的近似算法<ol>
<li>子集和问题的指数时间算法</li>
<li>子集和问题的完全多项式时间近似格式</li>
</ol>
</li>
<li>算法优化策略<ol>
<li>算法设计策略的比较与选择<ol>
<li>最大子段和问题的简单算法</li>
<li>最大子段和问题的分治算法</li>
<li>最大子段和问题的动态规划算法</li>
<li>最大子段和问题与动态规划算法的推广</li>
</ol>
</li>
<li>动态规划加速原理<ol>
<li>货物储运问题</li>
<li>算法及其优化</li>
</ol>
</li>
<li>问题的算法特性<ol>
<li>贪心策略</li>
<li>对贪心策略的改进</li>
<li>算法三部曲</li>
<li>算法实现</li>
<li>算法复杂性</li>
</ol>
</li>
<li>优化数据结构<ol>
<li>带权区间最短路问题</li>
<li>算法设计思想</li>
<li>算法实现方案</li>
<li>并查集</li>
<li>可并优先队列</li>
</ol>
</li>
<li>优化搜索策略</li>
</ol>
</li>
<li>在线算法设计<ol>
<li> 在线算法设计的基本概念</li>
<li> 页调度问题</li>
<li> 势函数分析</li>
<li>k服务问题竞争比的下界<ol>
<li> 平衡算法</li>
<li> 对称移动算法</li>
</ol>
</li>
<li> Steiner树问题</li>
<li> 在线任务调度</li>
<li> 负载平衡</li>
</ol>
</li>
</ol>
<h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p>《概率论与数理统计应用第二版-施雨+概率统计辅导书-魏平、王宁》</p>
<ol>
<li>随机事件与概率<ol>
<li>随机事件</li>
<li>概率</li>
<li>古典概率的计算</li>
<li>条件概率 事件的相互独立性</li>
</ol>
</li>
<li>随机变量与概率分布<ol>
<li>一维随机变量</li>
<li>二维随机变量</li>
<li>条件分布</li>
<li>随机变量的相互独立性</li>
<li>随机变量的函数的概率分布</li>
</ol>
</li>
<li>随机变量的数字特性<ol>
<li>数学期望</li>
<li>方差</li>
<li>协方差与相关系数</li>
<li>条件期望与条件方差</li>
</ol>
</li>
<li>大数定律与中心极限定理<ol>
<li>大数定律</li>
<li>中心极限定理</li>
</ol>
</li>
<li>数理统计学的基本概念<ol>
<li>总体与样本</li>
<li>样本分布</li>
<li>统计量</li>
<li>抽样分布</li>
</ol>
</li>
<li>参数估计<ol>
<li>点估计</li>
<li>估计量与评选标准</li>
<li>区间估计</li>
<li>正态总体参数的区间估计</li>
</ol>
</li>
<li>假设检验<ol>
<li>假设检验的基本概念</li>
<li>正态总体参数的假设检验</li>
<li>单边假设检验</li>
<li>多数假设的大样本检验</li>
<li>分布假设检验</li>
</ol>
</li>
<li>方差分析<ol>
<li>单因素方差分析</li>
<li>双因素方差分析</li>
</ol>
</li>
<li>回归分析<ol>
<li>一元线性回归</li>
<li>可线性化的一元非线性回归</li>
<li>多元线性回归</li>
</ol>
</li>
<li>随机过程的基本知识<ol>
<li> 随机过程的概念和记号</li>
<li> 随机过程的概率特性</li>
<li> 随机过程的基本类型</li>
<li> 泊松过程与布朗运动</li>
</ol>
</li>
<li>平稳过程<ol>
<li> 平稳过程的概念</li>
<li> 平稳过程的性质</li>
<li> 平稳过程的谱密度</li>
<li> 各态历经性</li>
</ol>
</li>
<li>附录<ol>
<li> 标准正态分布表</li>
<li> 泊松分布表</li>
<li> t分布表</li>
<li> X^2分布表</li>
<li> F分布表</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>健身清单</title>
    <url>/%E5%81%A5%E8%BA%AB%E6%B8%85%E5%8D%95/</url>
    <content><![CDATA[<p>背：拉，胸：推+宽于肩<br>史密斯：直背-肩，平躺：胸，45度，胳膊？</p>
<p>参考链接：<a href="https://www.hiyd.com/dongzuo/">https://www.hiyd.com/dongzuo/</a></p>
<h1 id="腿部训练"><a href="#腿部训练" class="headerlink" title="腿部训练"></a>腿部训练</h1><table>
<thead>
<tr>
<th align="left">标号</th>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">哑铃高脚杯深蹲</td>
<td align="left">30lbs</td>
<td align="left">16*4</td>
<td align="left">臀+大腿前后</td>
<td align="left"><a href="https://www.gotokeep.com/exercises/595f4d3aff247f33e297ea5a?gender=f">keep视频</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">倒蹬</td>
<td align="left">80kg</td>
<td align="left">16*4</td>
<td align="left">大腿</td>
<td align="left"><a href="https://www.163.com/dy/article/F7S7PBNQ0533BETQ.html">文字配图</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">腿屈伸</td>
<td align="left">23kg</td>
<td align="left">16*4</td>
<td align="left">大腿面</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5763d3b011fc5077c3acf58d?gender=f">keep</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">腿弯举</td>
<td align="left">32kg</td>
<td align="left">16*4</td>
<td align="left">大腿后侧</td>
<td align="left">我练的是坐姿版，如果只有卧姿，可以用硬拉替代</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">箭步蹲</td>
<td align="left">20lbs</td>
<td align="left">12*4</td>
<td align="left">大腿面</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/57e34ab509a2d8a73b790705?gender=f">keep视频</a>，可以双手提俩哑铃，也可以原地</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">壶铃硬拉</td>
<td align="left">45lbs</td>
<td align="left">16*4</td>
<td align="left">臀部+大腿后侧</td>
<td align="left"><a href="https://keep.com/exercises/5c0e44cfa29e340410e9761f">keep视频</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">坐姿腿外展</td>
<td align="left">50lbs</td>
<td align="left">16*4</td>
<td align="left">大腿外侧</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5763d3b011fc5077c3acf580?gender=f">keep图片</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">坐姿腿内收</td>
<td align="left">50lbs</td>
<td align="left">16*4</td>
<td align="left">大腿内侧</td>
<td align="left"><a href="https://www.gotokeep.com/exercises/5763d3b011fc5077c3acf595?gender=f">keep图片</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">自由深蹲</td>
<td align="left">-</td>
<td align="left">16*4</td>
<td align="left">臀+大腿面</td>
<td align="left"><a href="https://www.gotokeep.com/exercises/564b0b02a9f7e955480feff2?gender=f">keep视频</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">倒蹬</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">直腿硬拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">臀+大腿后侧</td>
<td align="left"><a href="http://show.gotokeep.com/exercises/595f4ee1ff247f33e297eb64">keep视频</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">腿屈伸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">大腿面</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5763d3b011fc5077c3acf58d?gender=f">keep</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">腿弯举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">大腿后侧</td>
<td align="left">我练的是坐姿版，如果只有卧姿，可以用硬拉替代</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿腿外展</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">大腿外侧</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5763d3b011fc5077c3acf580?gender=f">keep图片</a></td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">史密斯深蹲</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">臀+大腿</td>
<td align="left"><a href="https://www.gotokeep.com/exercises/5763d3b011fc5077c3acf581">keep图片</a></td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">臀桥</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://keep.com/exercises/55cc42cd5f9bd587372f64c9?gender=f">keep图片</a>??????我感觉不是这个啊</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">腿屈伸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">腿弯举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">坐姿腿外展</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">-</td>
</tr>
</tbody></table>
<h1 id="背部训练"><a href="#背部训练" class="headerlink" title="背部训练"></a>背部训练</h1><table>
<thead>
<tr>
<th align="left">标号</th>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">器械坐姿划船</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">高位下拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">背部左右下侧</td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5763d3b011fc5077c3acf5b6?gender=f">keep图片</a>，我感觉我好像是站着做的？问一下</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">T杠划船</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">中背部</td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/234/">视频</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">绳索划船</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">斜方肌</td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/944/">视频</a>，需要问一下</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">屈腿硬拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">？</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">直臂下压</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://gotokeep.com/exercises/5763d3b011fc5077c3acf578">keep图片</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">助力引体向上(fall)</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">高位下拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">T杠划船</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">中背部</td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/234/">视频</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿划船(挺胸版)</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">？</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿划船(含胸版)</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">？</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">屈腿硬拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h1 id="胸部训练"><a href="#胸部训练" class="headerlink" title="胸部训练"></a>胸部训练</h1><table>
<thead>
<tr>
<th align="left">标号</th>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">史密斯上斜卧推</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="http://gotokeep.com/exercises/5763d3b011fc5077c3acf559">keep图片</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">史密斯平板卧推</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://gotokeep.com/exercises/5763d3b011fc5077c3acf562">keep图片</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">哑铃飞鸟</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://gotokeep.com/exercises/5763d3b011fc5077c3acf59d">keep图片</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">坐姿夹胸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/1104/">视频</a></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">坐姿推胸2</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/844/">视频</a>，双臂一起</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">杠铃卧推</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/852/">视频</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">上斜杠铃卧推</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/277/">视频</a></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿推胸1</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/251/">视频</a>，双臂分开</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿推胸2</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/844/">视频</a>，双臂一起</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">坐姿夹胸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/1104/">视频</a></td>
</tr>
</tbody></table>
<h1 id="肩部训练（三角肌）"><a href="#肩部训练（三角肌）" class="headerlink" title="肩部训练（三角肌）"></a>肩部训练（三角肌）</h1><table>
<thead>
<tr>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">肩上推举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">??? <a href="https://www.hiyd.com/dongzuo/1523/">视频</a></td>
</tr>
<tr>
<td align="left">前平举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/1516/">视频</a></td>
</tr>
<tr>
<td align="left">侧平举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/1519/">视频</a></td>
</tr>
<tr>
<td align="left">颈前提拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">？</td>
</tr>
<tr>
<td align="left">俯身飞鸟</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">区别？</td>
</tr>
<tr>
<td align="left">反向飞鸟</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">区别？</td>
</tr>
</tbody></table>
<h1 id="手臂训练（二三头）"><a href="#手臂训练（二三头）" class="headerlink" title="手臂训练（二三头）"></a>手臂训练（二三头）</h1><table>
<thead>
<tr>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">仰卧臂屈伸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.hiyd.com/dongzuo/833/">视频</a></td>
</tr>
<tr>
<td align="left">直杆二头弯举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">所以这些弯举区别是啥</td>
</tr>
<tr>
<td align="left">绳索下拉</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">曲杆弯举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://www.gotokeep.com/exercises/5763d3b011fc5077c3acf59b">视频</a> ？</td>
</tr>
<tr>
<td align="left">凳上臂屈伸</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">锤式弯举</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><a href="https://show.gotokeep.com/exercises/5979cae611fc50467bd124ed?gender=f">视频</a>，不确定是不是哑铃，大概是绳索？</td>
</tr>
</tbody></table>
<h1 id="体能训练（循环4-6组）"><a href="#体能训练（循环4-6组）" class="headerlink" title="体能训练（循环4-6组）"></a>体能训练（循环4-6组）</h1><table>
<thead>
<tr>
<th align="left">标号</th>
<th align="left">名称</th>
<th align="left">指标</th>
<th align="left">次数</th>
<th align="left">部位</th>
<th align="left">链接/图示</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">俯卧撑</td>
<td align="left">-</td>
<td align="left">20</td>
<td align="left">肩、胸</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">登山</td>
<td align="left">-</td>
<td align="left">60s</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">站立划船</td>
<td align="left"></td>
<td align="left">15</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">卷腹</td>
<td align="left">-</td>
<td align="left">30</td>
<td align="left">上腹</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">站立举桶</td>
<td align="left"></td>
<td align="left">30</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">举腿</td>
<td align="left"></td>
<td align="left">25</td>
<td align="left">小腹</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">开合跳</td>
<td align="left"></td>
<td align="left">40</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">平板支撑</td>
<td align="left"></td>
<td align="left">1.5min-3min</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">登山</td>
<td align="left">-</td>
<td align="left">60s</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">跳台阶</td>
<td align="left">-</td>
<td align="left">30</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">硬拉</td>
<td align="left"></td>
<td align="left">20</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">波比跳</td>
<td align="left">-</td>
<td align="left">20</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">弓箭步举桶</td>
<td align="left"></td>
<td align="left">20</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">平板支撑</td>
<td align="left"></td>
<td align="left">3min</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>健身</tag>
      </tags>
  </entry>
  <entry>
    <title>博客简介</title>
    <url>/%E5%8D%9A%E5%AE%A2%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Hi, guys！</p>
<p>这里是 阿波罗猫 的博客——四界。</p>
<p>主要作用是记录个人生活，整理所学知识，放飞自我， 天天向上！</p>
<p>这里先许下一个心愿：<strong>希望未来能顺利毕业！</strong></p>
<span id="more"></span>
<h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><table>
<thead>
<tr>
<th align="center"><strong>侧栏名称</strong></th>
<th align="center"><strong>简介</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>主页</strong></td>
<td align="center">博客主页</td>
</tr>
<tr>
<td align="center"><strong>归档</strong></td>
<td align="center">罗列所有文章</td>
</tr>
<tr>
<td align="center"><strong>分类</strong></td>
<td align="center">按类型分类文章</td>
</tr>
<tr>
<td align="center"><strong>标签</strong></td>
<td align="center">按标签分类文章</td>
</tr>
<tr>
<td align="center"><strong>随笔</strong></td>
<td align="center">日常相关的吐槽</td>
</tr>
<tr>
<td align="center"><strong>关于我</strong></td>
<td align="center">个人简历</td>
</tr>
</tbody></table>
<h1 id="关于“四界”"><a href="#关于“四界”" class="headerlink" title="关于“四界”"></a>关于“四界”</h1><p>“四界”这个名字来自于我初高中时的自产小说，小说内容就不说了，满满都是中二，而且还坑了2333。</p>
<p>Anyway，这个博客名字可以理解为致敬青春吧。</p>
<p>祝愿我的未来无论走出多远，归来仍是少年。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu18.04+VINS-MONO+RealsenseD453i 配置教程</title>
    <url>/ubuntu18-04-VINS-MONO-RealsenseD453i-%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="VINS-MONO"><a href="#VINS-MONO" class="headerlink" title="VINS-MONO"></a>VINS-MONO</h1><p>主要参考链接：<br><a href="https://blog.csdn.net/github_53583003/article/details/111187288">Ubuntu18配置VINS-MONO</a></p>
<p>初始配置：<br>ubuntu18.04+cmake+engine</p>
<h2 id="1-ROS配置"><a href="#1-ROS配置" class="headerlink" title="1. ROS配置"></a>1. ROS配置</h2><p><a href="http://wiki.ros.org/melodic/Installation/Ubuntu">ROS官方安装教程</a></p>
<p><em>前面的主要参考链接用的是16.04下的<code>kinetic</code>内核，20.04用的是<code>Neotic</code>内核，我用的是18.04下的<code>melodic</code>内核，所以使用官方教程</em></p>
<h2 id="2-opencv配置"><a href="#2-opencv配置" class="headerlink" title="2. opencv配置"></a>2. opencv配置</h2><p>根据mono的建议，cv版本用的是<a href="https://github.com/opencv/opencv/releases/tag/3.3.1">opencv-3.3.1</a>和<a href="https://github.com/opencv/opencv_contrib/releases/tag/3.3.1">opencv-contrib-3.3.1</a></p>
<p>location: <code>~/library/opencv-3.3.1</code></p>
<p>后续根据<a href="https://blog.csdn.net/github_53583003/article/details/111187288">参考链接</a>完成编译。</p>
<p>进行opencv的编译环境配置：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install build-essential libgtk2.0-dev libvtk6-dev libjpeg-dev libtiff4-dev libjasper-dev libopenexr-dev libtbb-dev</span><br></pre></td></tr></tbody></table></figure>
<p><em>注意，opencv的依赖包虽然是参考SLAM十四讲，但是由于版本区别，有的包要装升级后的版本，比如书里是libvtk5-dev，而我们需要装6</em></p>
<h2 id="3-安装Ceres库"><a href="#3-安装Ceres库" class="headerlink" title="3. 安装Ceres库"></a>3. 安装Ceres库</h2><p>Ceres solver 是谷歌开发的一款用于非线性优化的库，在谷歌的开源激光雷达slam项目cartographer中被大量使用</p>
<p>首先，安装依赖</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install liblapack-dev </span><br><span class="line">sudo apt-get install libsuitesparse-dev </span><br><span class="line">sudo apt-get install libcxsparse3.1.2</span><br><span class="line">sudo apt-get install libgflags-dev </span><br><span class="line">sudo apt-get install libgoogle-glog-dev libgtest-dev</span><br></pre></td></tr></tbody></table></figure>
<p><em>如果有的包无法定位，可能是因为更换了版本名，<a href="https://packages.ubuntu.com/">https://packages.ubuntu.com/</a> <code>Search package directories</code>搜一下keyword，注意与当前ubuntu版本对应；也可以参考<a href="https://www.cnblogs.com/qilai/p/13654810.html">无法定位lib3.1.2</a>，我用的第二种方法</em></p>
<p>然后，下载<a href="https://github.com/ceres-solver/ceres-solver/releases">Ceres</a></p>
<p><em>我下的是cere2.0.0，解压后location: <code>~/library/ceres-solver-2.0.0</code></em></p>
<p>进入目录下编译ceres:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">sudo make install</span><br></pre></td></tr></tbody></table></figure>
<h2 id="4-VINS-MONO编译"><a href="#4-VINS-MONO编译" class="headerlink" title="4. VINS-MONO编译"></a>4. VINS-MONO编译</h2><p>VINS-MONO，2017年港科老师开源的一个单目视觉惯导的实时SLAM方案，是视觉与IMU的经典融合，定位精度可以媲美OKVIS，在Linux上运行，并与ROS完全集成。</p>
<p>该算法基于优化和滑动窗口的 VIO ，使用 IMU 预积分构建紧耦合框架，同时还有自动初始化，在线外参标定，重定位，闭环检测，以及全局位姿图优化功能。</p>
<ul>
<li>准备数据</li>
</ul>
<p>首先下载<a href="https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets#the_euroc_mav_dataset">EuRoC数据集</a>，该数据集是微型飞行器 (MAV) 上收集的视觉惯性数据集。数据集包含立体图像、同步 IMU 测量以及精确的运动和结构地面实况。</p>
<p><em>我下的是<code>Machine Hall 01 (ROS bag)</code>用来测试，location: <code>~/DATA/VINS-MONO/MH_01_easy.bag</code></em></p>
<p>此外还要下载<a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono">VINS-MONO代码</a>。</p>
<ul>
<li><p>创建ROS工作空间</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">mkdir -p ~/catkin_ws/src              </span><br><span class="line">cd ~/catkin_ws/src</span><br><span class="line">catkin_init_workspace</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>编译VINS</p>
</li>
</ul>
<p>将前面下载的Vins-mono的代码移动到<code>~/catkin_ws/src</code>文件夹下。</p>
<p>编译：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">cd ~/catkin_ws</span><br><span class="line">catkin_make</span><br><span class="line">source ~/catkin_ws/devel/setup.bash</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>修改配置文件内容在VINS-Mono-master主工程目录下的config子目录中找到euroc文件夹，并打开其中的euroc_config.yaml文件，将其中结果输出保存路径修改为本地的绝对路径或相对路径<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">output_path: "/home/yj/HZL/VINS-MONO-REALSENSE/output/"</span><br><span class="line">pose_graph_save_path: "/home/yj/HZL/VINS-MONO-REALSENSE/output/pose_graph/"</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<ul>
<li>修改source如果每次运行前需要都需要用source来指定文件位置未免太麻烦，可以直接在<code>.bashrc</code>中添加source<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sudo  gedit ~/.bashrc</span><br></pre></td></tr></tbody></table></figure>
在.bashrc中加上：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">source ~/catkin_ws/devel/setup.bash</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<ul>
<li>运行<ul>
<li>1号终端启动ROS核心：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roscore</span><br></pre></td></tr></tbody></table></figure></li>
<li>2号终端：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roslaunch vins_estimator euroc.launch</span><br></pre></td></tr></tbody></table></figure></li>
<li>3号终端启动rviz软件<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roslaunch vins_estimator vins_rviz.launch</span><br></pre></td></tr></tbody></table></figure></li>
<li>4号终端输入数据：注意把数据包地址改成自己的<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">rosbag play /home/yj/DATA/VINS-MONO/MH_01_easy.bag </span><br></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
</ul>
<h1 id="使用Realsense-D453i运行VINS-MONO"><a href="#使用Realsense-D453i运行VINS-MONO" class="headerlink" title="使用Realsense D453i运行VINS-MONO"></a>使用Realsense D453i运行VINS-MONO</h1><p><a href="https://blog.csdn.net/weixin_44580210/article/details/89789416">参考链接</a></p>
<h2 id="1-安装librealsense-SDK-2-0"><a href="#1-安装librealsense-SDK-2-0" class="headerlink" title="1. 安装librealsense SDK 2.0"></a>1. 安装librealsense SDK 2.0</h2><p><a href="https://github.com/IntelRealSense/librealsense/releases/tag/v2.48.0">v2.48.0</a></p>
<p><a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md">官方指南</a></p>
<p>librealsense SDK相当于相机的驱动，D435i是librealsense SDK 2.0；SDK的安装方式有两种，一是从源码编译安装，二是直接命令行安装，官方指南的公钥法用的2。</p>
<p><em>我装的是librealsense SDK v2.48.0，支持18.04</em></p>
<p>测试：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">realsense-viewer</span><br></pre></td></tr></tbody></table></figure>
<p><em>注意：使用3.0的usb口</em></p>
<h2 id="2-安装测试realsense相机对应的ROS包"><a href="#2-安装测试realsense相机对应的ROS包" class="headerlink" title="2. 安装测试realsense相机对应的ROS包"></a>2. 安装测试realsense相机对应的ROS包</h2><p><a href="https://github.com/IntelRealSense/realsense-ros">官方指南</a><br>（用的是指南里的realsense distribution方案）</p>
<p>进入ros空间: </p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">cd ~/catkin_ws/src</span><br><span class="line">git clone https://github.com/IntelRealSense/realsense-ros.git</span><br><span class="line">cd realsense-ros/</span><br><span class="line">git checkout `git tag | sort -V | grep -P "^2.\d+\.\d+" | tail -1`</span><br></pre></td></tr></tbody></table></figure>
<p>补一些依赖: </p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">mkdir -p ~/catkin_ws/src/realsense</span><br><span class="line">rosdep install --from-paths src --ignore-src -r -y    # 这步第二次安装的时候失败</span><br><span class="line">sudo apt purge ros-melodic-librealsense2</span><br></pre></td></tr></tbody></table></figure>
<p>继续安装：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">cd ~/catkin_ws</span><br><span class="line">catkin_make clean</span><br><span class="line">catkin_make -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release</span><br><span class="line">catkin_make install</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>出现报错，原因是缺少ddynamic_reconfigure，解决方案是从<a href="https://github.com/pal-robotics/ddynamic_reconfigure/tree/kinetic-devel">这里</a>将其下载，克隆到工作区<code>~/catkin_ws/src/</code>中</li>
</ul>
<p>改<code>.bashrc</code>：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">echo "source ~/catkin_ws/devel/setup.bash" &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></tbody></table></figure>
<p>运行rviz：</p>
<ul>
<li>1号终端启动ROS核心：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roscore</span><br></pre></td></tr></tbody></table></figure></li>
<li>2号终端：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">rosrun rviz rviz</span><br></pre></td></tr></tbody></table></figure>
运行realsense：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roslaunch realsense2_camera rs_camera.launch filters:=pointcloud</span><br></pre></td></tr></tbody></table></figure>
这时候应该无图像，需要在rviz中修改一些配置，当然配置有很多选择，对于新手来说有图就行orz，以下只是举个例子：</li>
<li>把fixed frame改成<code>camera_...</code>，不能是<code>map</code>，Global Status由红色变绿</li>
<li>Add -&gt; 上方点击 By topic -&gt; /depth_registered 下的 /points 下的/PointCloud2，这是针对上面命令中<code>filters:=pointcloud</code>部分选择的type</li>
<li>Add -&gt; 上方点击 By topic -&gt; /color 下的 /image_raw 下的image</li>
</ul>
<h2 id="3-在D453i上运行VINS-MONO"><a href="#3-在D453i上运行VINS-MONO" class="headerlink" title="3. 在D453i上运行VINS-MONO"></a>3. 在D453i上运行VINS-MONO</h2><p>这里建议先把catkin_ws备份一下<br><a href="https://zhuanlan.zhihu.com/p/390933690">参考链接0</a><br><a href="https://blog.csdn.net/weixin_44580210/article/details/89789416">参考链接1</a><br><a href="https://blog.csdn.net/qq_41839222/article/details/86552367">参考链接2</a></p>
<h3 id="1）修改rs-camera-launch文件"><a href="#1）修改rs-camera-launch文件" class="headerlink" title="1）修改rs_camera.launch文件"></a>1）修改rs_camera.launch文件</h3><p>location：<code>~/catkin_ws/src/realsense-ros/realsense2_camera/launch</code></p>
<ul>
<li>修改unite_imu_method如下，这里是让IMU的角速度和加速度作为一个topic输出<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arg name="unite_imu_method"      default="copy"/&gt;</span><br></pre></td></tr></tbody></table></figure></li>
<li>修改enable_sync参数为true，这里是开启相机和IMU的同步<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arg name="enable_sync"           default="true"/&gt;</span><br></pre></td></tr></tbody></table></figure></li>
<li>打开陀螺仪和加速度计<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&lt;arg name="enable_gyro"         default="true"/&gt;</span><br><span class="line">&lt;arg name="enable_accel"        default="true"/&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2）修改realsense-color-config-yaml文件"><a href="#2）修改realsense-color-config-yaml文件" class="headerlink" title="2）修改realsense_color_config.yaml文件"></a>2）修改realsense_color_config.yaml文件</h3></li>
</ul>
<p>location：<code>~/catkin_ws/src/VINS-Mono-master/config/realsense/realsense_color_config.yaml</code></p>
<ul>
<li>修改订阅的topic<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">imu_topic: "/camera/imu"</span><br><span class="line">image_topic: "/camera/color/image_raw"</span><br><span class="line">output_path: "/home/yj/HZL/VINS-MONO-REALSENSE/output/"  #自己的地址</span><br><span class="line"></span><br><span class="line">pose_graph_save_path: "/home/yj/HZL/VINS-MONO-REALSENSE/output/pose_graph/"</span><br></pre></td></tr></tbody></table></figure></li>
<li>修改相机的内参<br><a href="https://zhuanlan.zhihu.com/p/390933690">指南</a><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">rostopic echo /camera/color/camera_info #查看内参命令</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#camera calibration </span><br><span class="line">model_type: PINHOLE</span><br><span class="line">camera_name: camera</span><br><span class="line">image_width: 1280</span><br><span class="line">image_height: 720</span><br><span class="line">distortion_parameters:</span><br><span class="line">   k1: 0 #9.2615504465028850e-02</span><br><span class="line">   k2: 0 #-1.8082438825995681e-01</span><br><span class="line">   p1: 0 #-6.5484100374765971e-04</span><br><span class="line">   p2: 0 #-3.5829351558557421e-04</span><br><span class="line">projection_parameters:</span><br><span class="line">   fx: 9.194400024414062e+02 #6.0970550296798035e+02</span><br><span class="line">   fy: 9.189413452148438e+02 #6.0909579671294716e+02</span><br><span class="line">   cx: 6.383787841796875e+02 #3.1916667152289227e+02</span><br><span class="line">   cy: 3.5089337158203125e+02 #2.3558360480225772e+02</span><br></pre></td></tr></tbody></table></figure></li>
<li>IMU到相机的变换矩阵，这里我根据注释的提示修改成2</li>
</ul>
<p>初始为0，选0的话，IMU和camera之间的外参矩阵建议使用Kalibr工具进行离线标定，也可以改成1或者2让估计器自己标定和优化。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># Extrinsic parameter between IMU and Camera.</span><br><span class="line">estimate_extrinsic: 2   </span><br><span class="line">                # 0  Have an accurate extrinsic parameters. We will trust the following imu^R_cam, imu^T_cam, don't change it.</span><br><span class="line">                # 1  Have an initial guess about extrinsic parameters. We will optimize around your initial guess.</span><br><span class="line">                # 2  Don't know anything about extrinsic parameters. You don't need to give R,T. We will try to calibrate it. Do some rotation movement at beginning.                        </span><br><span class="line">                # If you choose 0 or 1, you should write down the following matrix.</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>IMU参数，这里我全部修改注释给的参数<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#imu parameters       The more accurate parameters you provide, the better performance</span><br><span class="line">acc_n: 0.2          # accelerometer measurement noise standard deviation. #0.2</span><br><span class="line">gyr_n: 0.05         # gyroscope measurement noise standard deviation.     #0.05</span><br><span class="line">acc_w: 0.02         # accelerometer bias random work noise standard deviation.  #0.02</span><br><span class="line">gyr_w: 4.0e-5       # gyroscope bias random work noise standard deviation.     #4.0e-5</span><br><span class="line">g_norm: 9.80       # gravity magnitude</span><br></pre></td></tr></tbody></table></figure></li>
<li>是否需要在线估计同步时差，根据上述博主的建议这里选择不需要<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#unsynchronization parameters</span><br><span class="line">estimate_td: 0                      # online estimate time offset between camera and imu</span><br><span class="line">td: 0.000                           # initial value of time offset. unit: s. readed image clock + td = real image clock (IMU clock)</span><br></pre></td></tr></tbody></table></figure></li>
<li>相机曝光改成全局曝光<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#rolling shutter parameters</span><br><span class="line">rolling_shutter: 0                      # 0: global shutter camera, 1: rolling shutter camera</span><br><span class="line">rolling_shutter_tr: 0               # unit: s. rolling shutter read out time per frame (from data sheet). </span><br></pre></td></tr></tbody></table></figure>
<h3 id="3）运行"><a href="#3）运行" class="headerlink" title="3）运行"></a>3）运行</h3><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roscore</span><br><span class="line">roslaunch realsense2_camera rs_camera.launch  #启动realsense驱动sdk，输出相应图像和IMU话题</span><br><span class="line">roslaunch vins_estimator realsense_color.launch #启动vins-mono，数据来源是相机</span><br><span class="line">roslaunch vins_estimator vins_rviz.launch #启动rviz界面</span><br></pre></td></tr></tbody></table></figure>
在global option里把word改成camera_link</li>
</ul>
<h1 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h1><p>不显示特征点和轨迹，报错如下：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[ INFO] [1629687186.636584239]: init begins</span><br><span class="line">[ INFO] [1629687186.672642097]: Loaded config_file: /home/yj/catkin_ws/src/VINS-Mono-master/feature_tracker/../config/realsense/realsense_color_config.yaml</span><br><span class="line">vocabulary_file/home/yj/catkin_ws/src/VINS-Mono-master/pose_graph/../support_files/brief_k10L6.bin</span><br><span class="line">result path /home/yj/HZL/VINS-MONO-REALSENSE/output//vins_result_no_loop.csv</span><br><span class="line">[ INFO] [1629687186.683639120]: ROW: 480.000000 COL: 640.000000 </span><br><span class="line">[ WARN] [1629687186.683652444]: have no prior about extrinsic param, calibrate extrinsic param</span><br><span class="line"></span><br><span class="line">[ INFO] [1629688187.806394607]: Synchronized sensors, fix time offset: 0</span><br><span class="line">[ WARN] [1629688187.806432117]: waiting for image and imu...</span><br><span class="line">vocabulary_file/home/yj/catkin_ws/src/VINS-Mono-master/pose_graph/../support_files/brief_k10L6.bin</span><br><span class="line">loop start load vocabulary</span><br><span class="line">BRIEF_PATTERN_FILE/home/yj/catkin_ws/src/VINS-Mono-master/pose_graph/../support_files/brief_pattern.yml</span><br><span class="line">no previous pose graph</span><br><span class="line">OpenCV Error: Assertion failed (_mask.empty() || (_mask.type() == CV_8UC1 &amp;&amp; _mask.sameSize(_image))) in goodFeaturesToTrack, file /build/opencv-L2vuMj/opencv-3.2.0+dfsg/modules/imgproc/src/featureselect.cpp, line 366</span><br><span class="line">terminate called after throwing an instance of 'cv::Exception'</span><br><span class="line">  what():  /build/opencv-L2vuMj/opencv-3.2.0+dfsg/modules/imgproc/src/featureselect.cpp:366: error: (-215) _mask.empty() || (_mask.type() == CV_8UC1 &amp;&amp; _mask.sameSize(_image)) in function goodFeaturesToTrack</span><br><span class="line"></span><br><span class="line">[feature_tracker-1] process has died [pid 1331, exit code -6, cmd /home/yj/catkin_ws/devel/lib/feature_tracker/feature_tracker __name:=feature_tracker __log:=/home/yj/.ros/log/8f26fcba-03bf-11ec-9c16-b07b250c6f90/feature_tracker-1.log].</span><br><span class="line">log file: /home/yj/.ros/log/8f26fcba-03bf-11ec-9c16-b07b250c6f90/feature_tracker-1*.log</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>解决策略：</p>
<ul>
<li><p>第一次跑的时候没配置opencv，现在补充配置opencv，按照《高翔SLAM》p108中的配置安装依赖包（这部分已补充在上文），以下是安装以上依赖时附带安装的依赖，仅作为记录，不作为参考：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get install build-essential libopenmpi1.6 libvtk5.8 mpi-default-dev</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>还是不显示，参考<a href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono/issues/26">链接</a>，发现是相机参数未同步的原因：</p>
<p><strong>最终解决方案</strong>：除了要在上述的<code>realsense_color_config.yaml</code>中修改相机参数，还需要在<code>/home/yj/catkin_ws/src/VINS-Mono-master/config/euroc/euroc_config.yaml</code>文件中修改<code>#camera calibration</code></p>
<p>我的problem还有，在<code>realsense_color_config.yaml</code>中只修改了<code>distortion_parameters</code>和<code>projection_parameters</code>，而未修改分辨率<code>(image_width: 1280, image_height: 720)</code></p>
<ul>
<li>查看相机内参指令(不同相机内参是不一样的)：</li>
</ul>
  <figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">roscore</span><br><span class="line">roslaunch realsense2_camera rs_camera.launch  #打开相机节点</span><br><span class="line">rostopic echo /camera/color/camera_info   #查看相机内参</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github博客搭建教程</title>
    <url>/Hexo-Github%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>新手小白，第一次搭博客，先向前辈们大佬们隔空致敬。</p>
<p>搭建博客有很多种方法，我选择的是Hexo静态博客框架，利用Github Page服务器，比较节省时间和金钱，可以更加专注于内容本身。</p>
<span id="more"></span>

<p>首先附上参考链接：</p>
<table>
<thead>
<tr>
<th align="center">类型</th>
<th align="left">链接</th>
</tr>
</thead>
<tbody><tr>
<td align="center">概述</td>
<td align="left">1. <a href="https://zoomyale.com/2016/why_blogging/">为什么要在老掉牙的独立博客上写东西</a></td>
</tr>
<tr>
<td align="center">参考教程</td>
<td align="left">1. <a href="https://godweiyang.com/2018/04/13/hexo-blog/">超详细Hexo+Github博客搭建小白教程-韦阳</a><br>2. <a href="https://zhuanlan.zhihu.com/p/26625249">GitHub+Hexo 搭建个人网站详细教程 - 知乎</a><br>3. <a href="https://www.cnblogs.com/shwee/p/11421156.html">Hexo+Github: 个人博客网站搭建完全教程- 博客园</a><br>4. <a href="https://segmentfault.com/a/1190000021979631">超级详细Hexo+GitHub+阿里云域名的博客搭建教程，新手也能轻松学会</a><br>5. <a href="https://www.cnblogs.com/shwee/p/11421156.html">Hexo+Github: 个人博客网站搭建完全教程</a><br>6. <a href="https://winney07.github.io/2018/08/02/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%8F%91%E5%B8%83%E6%96%87%E7%AB%A0/">在Hexo博客中发布文章</a></td>
</tr>
<tr>
<td align="center">个性化</td>
<td align="left">1. <a href="https://shen-yu.gitee.io/2019/ayer/">Ayer主题</a><br>2. <a href="https://yuchen-lea.github.io/2016-01-18-hexo-dir-struct/">hexo目录结构小探</a></td>
</tr>
<tr>
<td align="center">写作</td>
<td align="left">1. <a href="http://itmyhome.com/markdown/index.html">Markdown 入门参考</a><br>2. <a href="https://zhuanlan.zhihu.com/p/56943330">使用vscode开始Markdown写作之旅 - 知乎</a><br>3. <a href="https://hexo.io/zh-cn/docs/">Hexo-文档</a></td>
</tr>
<tr>
<td align="center">图床</td>
<td align="left">1. <a href="https://www.jianshu.com/p/bff6638e450f">图床工具推荐</a><br>2. <a href="https://sunhwee.com/posts/1788dd4a.html">PicGo+GitHub：你的最佳免费图床选择-洪卫</a></td>
</tr>
</tbody></table>
<h1 id="搭建博客框架"><a href="#搭建博客框架" class="headerlink" title="搭建博客框架"></a>搭建博客框架</h1><h2 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h2><p>Hexo基于Node.js，首先要下载<a href="https://nodejs.org/en/download/">Node.js安装包</a>（建议选LTS版本，current版本或许无法安装一些插件），改一下安装路径，其他安装选项选择默认就行。</p>
<p>安装完成后，<code>win+R</code>打开命令行，输入<code>node -v</code>和<code>npm -v</code>，显示版本号则安装成功。</p>
<h2 id="添加镜像"><a href="#添加镜像" class="headerlink" title="添加镜像"></a>添加镜像</h2><p>如果没有梯子，npm部分可以用阿里国内镜像加速。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></tbody></table></figure>

<h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><p>为了将本地网页文件上传到Github上，需要安装分布式版本控制工具<a href="https://git-scm.com/download/win">Git</a>。</p>
<p>有兴趣深入了解Git的话，可以参考廖雪峰老师的<a href="https://www.liaoxuefeng.com/wiki/896043488029600">Git教程</a></p>
<p>安装完成后在命令行输入git –version验证是否安装成功。</p>
<h2 id="Github部分"><a href="#Github部分" class="headerlink" title="Github部分"></a>Github部分</h2><p>每个程序猿都该有个Github啦，毕竟这就是猿的浪漫了~如果没有也没有关系，你可以从现在开始拥有这份独特的浪漫。</p>
<ol>
<li><p>注册一个<a href="https://github.com/">Github</a>账号，强烈建议名字正常一点。</p>
</li>
<li><p>新建一个仓库：</p>
<div align="center">
<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github-Picbed20210415174145.png" width="300/">
</div></li>
<li><p>仓库基本配置如下：</p>
<p>这里要注意的是：仓库的命名一定要跟用户名一样！</p>
<div align="center">
<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github-Picbed20210415222443.png">
</div></li>
<li><p>在新建好的仓库界面选择<code>setting</code>–<code>Pages</code>–<code>Choose a theme</code>，这里的主题可以随便选一个，后期可以换成别的。</p>
</li>
<li><p>刷新一下，<code>Pages</code>界面会出现你自己的网页链接，比如：<code>Your site is ready to be published at http://OrderAngel.com/OrderAngel.github.io</code></p>
<p>虽然简陋，但是这就是你的博客啦~</p>
</li>
</ol>
<h2 id="安装Hexo，生成静态框架"><a href="#安装Hexo，生成静态框架" class="headerlink" title="安装Hexo，生成静态框架"></a>安装Hexo，生成静态框架</h2><p>Hexo是一款基于Node.js的静态博客框架，生成的静态网页可以借助Github服务器发布。</p>
<ol>
<li><p>在合适的地方（这里说的一定不是C盘!）新建一个文件夹,用于存放博客文件，比如<code>E:\Study\Apollocat\blog</code>。</p>
</li>
<li><p>在该目录下右键，点击<code>Git Bash Here</code>，如果右键没有就直接搜Git bash，<code>cd</code>过去也行。</p>
</li>
<li><p>在该目录下输入<code>npm i hexo-cli -g</code>安装Hexo，或许有报错，直接无视就行。</p>
</li>
<li><p>安装完输入<code>hexo v</code>出现版本号则安装成功。</p>
</li>
<li><p>输入<code>hexo init</code>初始化文件夹，接着输入<code>nmp install</code>安装必要组件。</p>
</li>
<li><p>为了检测网站雏形，输入<code>hexo new "hello world"</code>新建文章。打开<code>E:\Study\Apollocat\blog\source\_posts</code>目录，可以看到一个<code>hello world.md</code>，这就是你新建的文章文件。</p>
</li>
<li><p>输入<code>hexo g</code>生成静态网页，再输入<code>hexo s</code>打开本地服务器。（Actually, 不生成静态网页，直接输入<code>hexo s</code>进行预览测试也可以）</p>
<p>这时用浏览器打开<code>localhost:4000</code>就能看到你的第一篇博客了，即便它只有一个标题，且只是静态的。</p>
<p>最后<code>ctrl+c</code>关闭本地服务器。</p>
</li>
</ol>
<h2 id="链接Github与本地Git"><a href="#链接Github与本地Git" class="headerlink" title="链接Github与本地Git"></a>链接Github与本地Git</h2><p>我们的网站在本地搭建完毕，如果想要发布到Github上就需要用到Git了。前面我们已经在本地安装Git了，现在需要将你的本地Git与Github链接，该步骤我们通过设置ssh秘钥来完成。</p>
<ol>
<li><p>鼠标右键打开Git Bash，设置本地Git的用户名和邮箱，注意要与Github相同，别输错了：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git config --global user.name "GitHub用户名"</span><br><span class="line">git config --global user.email "GitHub注册邮箱"</span><br></pre></td></tr></tbody></table></figure>
<p>可以用以下两条命令，检查一下你有没有输对:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git config user.name</span><br><span class="line">git config user.email</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>生成ssh秘钥：</p>
<p>ssh简单而言是一个秘钥，<code>id_rsa</code>是本地电脑的私人秘钥，<code>id_rsa.pub</code>是公共秘钥。公钥放在Github上，当链接本地与Github时，会根据公钥匹配私钥，匹配上则成功链接，之后可以用Git上传本地文件到Github上。</p>
<p>为啥需要设置ssh秘钥？由于Github要求每次上传文件的都是合法用户，因此每次上传都需要输入账号和密码来进行验证。这里设置ssh秘钥就是为了省去手动验证的步骤，上传时由Git自行匹配私钥与公钥完成验证。</p>
<p>输入命令生成ssh秘钥：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C "GitHub注册邮箱"</span><br></pre></td></tr></tbody></table></figure>
<p>直接回车，默认不设密码。</p>
</li>
<li><p>找到生成的.ssh文件夹中的id_rsa.pub秘钥，复制全部内容。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github-Picbed20210416095437.png"><br>回到Github，打开<code>Settings</code>（是头像处的settings）– <code>SSH and GPG Keys</code> – <code>New SSH Key</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github-Picbed20210416101521.png"></p>
<p>Title随便写，Key部分将刚才复制的id_rsa.pub秘钥粘进去，最后点<code>Add SHH Key</code>。</p>
</li>
<li><p>在本地的Git Bash里检测Github公钥是否成功设置，输入<code>ssh git@github.com</code>，如果输出部分出现你的用户名，则设置成功。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github-Picbed20210416102021.png"></p>
</li>
</ol>
<h2 id="发布网站"><a href="#发布网站" class="headerlink" title="发布网站"></a>发布网站</h2><p>此时我们的网站已经在本地搭好了，而且本地Git也和Github成功链接，接下来要做的就是把本地搭的网站发布出去，让网站可以被其他人访问。</p>
<ol>
<li><p>打开本地blog根目录下的站点配置文件_config.yml，该文件用于配置博客的基本信息。</p>
<p>需要注意的是，theme文件夹中有个同名文件，我们称之为主题配置文件，用于配置博客的主题信息。</p>
</li>
<li><p>修改站点配置文件_config.yml的最后一行配置并保存：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210416105511.png"></p>
<p>这里的<code>repository</code>可以简写为<code>repo</code>，是你Github上创建的博客仓库的路径。</p>
<p><strong>注意</strong>：这里的branch可以设置为main或者master，但务必与Github仓库中<code>Settings</code>–<code>Pages</code>–<code>Source</code>–<code>Branch</code>相匹配，否则即使上传成功也看不到你的网站。</p>
<p>修改deploy其实是在给网站部署命令<code>hexo d</code>做相应配置，使用该命令进行部署时，hexo就能知道要把blog部署到Github的博客仓库中。</p>
</li>
<li><p>安装Git部署插件：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>分别输入以下三条命令：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line">hexo clean <span class="comment">//清除缓存，若是网页正常情况下可以忽略这条命令</span></span><br><span class="line">hexo g <span class="comment">//生成静态网页</span></span><br><span class="line">hexo d <span class="comment">//部署</span></span><br></pre></td></tr></tbody></table></figure>
<p>之后打开浏览器，输入部分博客仓库路径<code>OrderAngel.github.io</code>，就能看到你发布的博客了。</p>
</li>
</ol>
<h1 id="优化博客框架"><a href="#优化博客框架" class="headerlink" title="优化博客框架"></a>优化博客框架</h1><h2 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h2><p>如果嫌弃<code>balabala.github.io</code>这种统一标识的域名，可以绑定自己的个性化域名。</p>
<ol>
<li><p>没有域名的话要先买个域名，百度云阿里云腾讯云之类的都能买，我个人用的是<a href="https://homenew.console.aliyun.com/home/dashboard/ProductAndService">阿里云</a>。</p>
<p>登陆后进入右上角的<code>控制台</code>，点<code>产品与服务</code> – <code>域名</code> –<code>域名注册</code>。</p>
<p>把喜欢的域名买下来吧~</p>
</li>
<li><p>解析域名：在域名控制台界面下，点击<code>解析</code>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418093157.png"></p>
<p>在解析设置界面，点击<code>添加记录</code>，添加两条记录：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418093651.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418093743.png"></p>
<p>记录值这里是：<code>用户名.github.io</code>这个现在可以直接访问的部分仓库路径。</p>
</li>
<li><p>回到你的Github博客仓库中，选择<code>Settings</code> – <code>Pages</code>，在<code>Custom domain</code>这里， 输入你购买的域名后保存。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418094423.png"></p>
<p>这里的<code>www.</code>输不输都行。</p>
</li>
<li><p>在本地blog文件夹中，blog/source目录下，创建一个记事本文件，输入你购买的域名。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418095345.png"></p>
<p>这里的域名建议不带<code>www.</code>，如果带上以后必须有完整的<code>www.balabala..</code>才能访问，如果不带，以后域名有没有<code>www.</code>都能访问。</p>
<p>文件命名为<code>CNAME</code>，保存类型为<code>所有文件(*.*)</code>。</p>
</li>
<li><p>最后，将我们改动的配置同步到Github中就行了：在本地blog目录下，进入Git Bash命令行(cmd和powershell也行)，输入：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></tbody></table></figure>

<p>现在，打开浏览器输入你自己的域名，就可以直接进入你的博客了~</p>
</li>
</ol>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>之前随便选的Hexo主题或许不合心意，但不用care，因为我们还可以更换其他主题。</p>
<p>可以在<a href="https://hexo.io/themes/">Theme</a>中Pick心水的主题。我个人用的是<a href="https://shen-yu.gitee.io/">Ayer</a>，这位博主的写的主题配置教程很nice。其他常用的，比如Next主题，也很棒。</p>
<ol>
<li>如果Theme的博主有相关教程，按说明进行安装配置；如果没有的话，先去Github里下载下来，放在本地的<code>blog\themes</code>目录下：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418105207.png"></li>
</ol>
<ol start="2">
<li><p>在本地blog目录下，打开站点配置文件<code>_config.yml</code>，修改theme为你选的主题名，比如ayer。这里注意要与<code>blog\themes</code>目录下该主题命名相同。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418105658.png"></p>
</li>
<li><p>一般关于该主题的使用教程会在Theme博主的Blog里给出；如果没有的话，搜下有无相关教程，或者自行研究一下本地<code>blog\themes\主题名</code>目录下的主题配置文件<code>_config.yml</code>，主题相关的主要配置都在该文件中。</p>
<p>关于主题文件夹的目录说明，可以参考<a href="https://hexo.io/zh-cn/docs/themes.html">主题hexo</a>。</p>
</li>
<li><p>最后，再次部署网站，还是：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line">hexo s <span class="comment">//本地预览测试：可以先打开本地服务器，在localhost:4000中看看效果</span></span><br><span class="line">hexo clean <span class="comment">//清除缓存</span></span><br><span class="line">hexo g <span class="comment">//生成</span></span><br><span class="line">hexo d <span class="comment">//部署</span></span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h2 id="hexo私人定制"><a href="#hexo私人定制" class="headerlink" title="hexo私人定制"></a>hexo私人定制</h2><p>工欲善其事，必先利其器。在进行一些更高级的私人配置前，我们有必要先了解一下blog文件夹里的文件都是用来干啥的。</p>
<h3 id="hexo目录说明"><a href="#hexo目录说明" class="headerlink" title="hexo目录说明"></a>hexo目录说明</h3><p>这是我blog文件夹下的目录，由于使用了一段时间，因此也有些私设，请大致参考。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">|──_config.yml</span><br><span class="line">|──package.json</span><br><span class="line">|──.deploy_git</span><br><span class="line">|──node_modules</span><br><span class="line">|──public</span><br><span class="line">|──scaffolds</span><br><span class="line">|──source</span><br><span class="line">      |──_posts</span><br><span class="line">      |──categories</span><br><span class="line">      |──pictures</span><br><span class="line">      |──tags</span><br><span class="line">|──themes</span><br><span class="line">      |──landscape</span><br><span class="line">      |──ayer</span><br></pre></td></tr></tbody></table></figure>
<h4 id="1-config-yml"><a href="#1-config-yml" class="headerlink" title="1. _config.yml"></a>1. _config.yml</h4><p>   站点配置文件，初始化时自动创建。</p>
<p>   具体配置可参考<a href="https://hexo.io/zh-cn/docs/configuration.html">配置hexo</a></p>
<h4 id="2-package-json"><a href="#2-package-json" class="headerlink" title="2. package.json"></a>2. package.json</h4><p>   应用程序文件，初始化时自动创建。</p>
<p>   可以查看hexo版本以及安装的扩展版本。</p>
<h4 id="3-deploy-git"><a href="#3-deploy-git" class="headerlink" title="3. .deploy_git"></a>3. .deploy_git</h4><p>   对应Github博客仓库中的内容 = 最近一次成功上传到Github上的public文件夹内容。在部署到Github后自动创建。</p>
<p>   可能出现的与public内容不一致，是由于重新生成但未上传所致。简而言之，public是<code>hexo g</code>生成静态页面的内容，而.deploy_git中是<code>hexo d</code>部署到Github中的内容。</p>
<h4 id="4-node-modules"><a href="#4-node-modules" class="headerlink" title="4. node_modules"></a>4. node_modules</h4><p>   存放安装的Hexo扩展，还有node .js的各种库。</p>
<h4 id="5-public"><a href="#5-public" class="headerlink" title="5. public"></a>5. public</h4><p>   执行<code>hexo g</code>命令后，Hexo会解析source文件夹和当前的Theme，从而生成静态网页，该内容存放于public文件夹中。</p>
<h4 id="6-scaffolds"><a href="#6-scaffolds" class="headerlink" title="6. scaffolds"></a>6. scaffolds</h4><p>   模板文件夹。<br>   内有页面模板<code>page.md</code>、文章模板<code>post.md</code>、草稿模板<code>draft.md</code>三个文件。<br>   具体可参考<a href="https://hexo.io/zh-cn/docs/templates.html">模板hexo</a>。</p>
<h4 id="7-source"><a href="#7-source" class="headerlink" title="7. source"></a>7. source</h4><p>   存放用户资源。</p>
<ol>
<li><p>_post</p>
<p>存放文章。<br>其中markdown文件(md)、html文件、org文件在<code>hexo g</code>生成页面过程中会被解析并拷贝到public文件夹中，最终通过<code>hexo d</code>发布。</p>
</li>
<li><p>其他_balabala文件夹/文件</p>
<p>生成时将会被忽略。</p>
<p>因此可以在此目录下创建_drafts文件夹来存放草稿。</p>
</li>
<li><p>其他非_balabala文件夹/文件</p>
<p>生成时会被拷贝到public中。</p>
<p>除了文章外，还会有图片、标签等用户资源，可以使用<code>hexo new page pilipili</code>来新建子目录<code>pilipili</code>。</p>
<p>比如我的目录下就有categories、pictures、tags等子目录。</p>
</li>
</ol>
<h4 id="themes"><a href="#themes" class="headerlink" title="themes"></a>themes</h4><p>   主题文件夹。<br>   我使用的是Ayer主题，因此在我的themes文件夹下，除了默认主题lanscape外，还有ayer文件夹。</p>
<h3 id="可以考虑的定制"><a href="#可以考虑的定制" class="headerlink" title="可以考虑的定制"></a>可以考虑的定制</h3><h4 id="1-对-config-yml进行修改"><a href="#1-对-config-yml进行修改" class="headerlink" title="1. 对_config.yml进行修改"></a>1. 对_config.yml进行修改</h4><p>   对blog目录下的站点配置文件_config.yml进行个性化修改，具体可以参考<a href="https://hexo.io/zh-cn/docs/configuration.html">配置hexo</a>。</p>
<h5 id="网站Site："><a href="#网站Site：" class="headerlink" title="网站Site："></a>网站Site：</h5><p>我们可以对文件中的site部分进行修改，比如：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction20210418121141.png"></p>
<h5 id="网址URL"><a href="#网址URL" class="headerlink" title="网址URL"></a>网址URL</h5><p>将url改成自己的网站域名，如：<code>http://apollomao.com</code>。</p>
<p>还可以对网址的永久链接格式进行修改，具体可参考<a href="https://hexo.io/zh-cn/docs/permalinks">永久链接</a></p>
<h4 id="2-对scaffolds文件夹中的模板文件进行修改"><a href="#2-对scaffolds文件夹中的模板文件进行修改" class="headerlink" title="2. 对scaffolds文件夹中的模板文件进行修改"></a>2. 对scaffolds文件夹中的模板文件进行修改</h4><p>所谓模板，即就是预定义，可以在每创建一篇文章后，预定义文章头部，省事。</p>
<p>比如，我的文章模板<code>post.md</code>为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210418230721.png"></p>
<p>那么我每次<code>hexo new post balabala</code>生成新的文章，该文章的头部就会是：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210418230825.png"></p>
<p>只需要按照需求补充一下tages、categories等值即可：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210418231042.png"></p>
<p>网页模板<code>page.md</code>和草稿模板<code>draft.md</code>也可参考<code>post.md</code>进行设置。创建命令分别是<code>hexo new page balabala</code>和<code>hexo new draft balabala</code>。</p>
<h4 id="3-其他"><a href="#3-其他" class="headerlink" title="3. 其他"></a>3. 其他</h4><p>还有一些常见的定制策略，比如加背景、加侧栏菜单、加背景音乐、加404界面等等，部分功能可能主题自带的，其他感兴趣的可以自行搜索~（开头的<a href="https://www.cnblogs.com/shwee/p/11421156.html">参考教程5</a>中包含一些，我没有尝试，仅供参考）</p>
<h2 id="图床"><a href="#图床" class="headerlink" title="图床"></a>图床</h2><p>图床是可以上传图片的服务器。常用的图床有很多，可以参考开头的<a href="https://www.jianshu.com/p/bff6638e450f">图床工具推荐</a>选择合适的图床和图床工具。</p>
<p>我个人用的是Github + PicGo的组合，因为省钱233</p>
<p>具体安装教程可以参考开头的<a href="https://sunhwee.com/posts/1788dd4a.html">PicGo+GitHub：你的最佳免费图床选择-洪卫</a>。</p>
<p>附上我的PicGo配置仅供参考：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419001244.png"></p>
<p>需要注意的是，与参考教程不同，在配置PicGo阶段，设定分支名需要与Github仓库中<code>Settings</code>–<code>Pages</code>–<code>Source</code>–<code>Branch</code>相对应，而不是统一设master。</p>
<ul>
<li>Note<ul>
<li>PicGo为了稳定期间最好别装测试版。</li>
<li>不要试图装watermark插件，别问我怎么知道的… orz</li>
</ul>
</li>
</ul>
<h1 id="牛刀小试"><a href="#牛刀小试" class="headerlink" title="牛刀小试"></a>牛刀小试</h1><p>现在我们来正经发一篇文章。</p>
<h2 id="Markdown基础"><a href="#Markdown基础" class="headerlink" title="Markdown基础"></a>Markdown基础</h2><p>博客文章主要用Markdown编写，有很多写作工具，我个人用的是VSCode。</p>
<p>零基础的同学可以参考<a href="https://zhuanlan.zhihu.com/p/56943330">使用vscode开始Markdown写作之旅</a>进行学习，亲测很好用~</p>
<p>附上其中的基础语法仅供参考：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419002530.png"></p>
<p>等你正式开始写文章的时候，具体一些用法还可以参考<a href="http://itmyhome.com/markdown/index.html">Learning-Markdown (Markdown 入门参考)</a></p>
<ul>
<li>最后再贴出来一个范例：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419003218.png"></p>
<p>这部分除了基础语法，我还用到了插入链接 <code>[title](网址)</code>，以及插入图片 <code>![title](图床地址or本地链接)</code>(直接从PicGo图床里复制就行)。</p>
<h2 id="正经发布一篇文章"><a href="#正经发布一篇文章" class="headerlink" title="正经发布一篇文章"></a>正经发布一篇文章</h2><p>get了最基础的markdown语法后，我们来走一遍完整流程。</p>
<ol>
<li><p>创建一篇文章”博客简介”</p>
<p>在本地blog目录下，右键进入Git Bash，输入：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">hexo new post "博客简介"</span><br></pre></td></tr></tbody></table></figure>
<p><strong>注意</strong>：</p>
<ul>
<li><p>post是默认值可以省略。</p>
</li>
<li><p>如果title有空格，必须要加双引号，否则双引号可以省略。</p>
</li>
</ul>
</li>
<li><p>编辑文章</p>
<p>在 blog\source_posts 路径下生成了<code>博客简介.md</code>文件，用vscode打开，编辑自己的文章。</p>
<p>比如我的博客简介是这么写的（仅供参考）：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419093855.png"></p>
<ul>
<li><p>可以在文章任意位置添加<code>&lt;!--more--&gt;</code>，首页只会显示more前的内容，more后的内容点击阅读全文后展示。</p>
</li>
<li><p>如果想给文章添加分类和标签，可以在文章头部补充，比如我是这么写的：<br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419093519.png"></p>
</li>
</ul>
</li>
<li><p>本地预览</p>
<p>写好文章后保存。</p>
<p><code>hexo s</code>启动本地服务器，在 <a href="http://localhost:4000/">http://localhost:4000</a> 进行预览测试。</p>
</li>
<li><p>发布文章<br>Finally!<br>输入以下命令，将写好的“博客简介”发布到网站上吧~</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></tbody></table></figure>
<p>此时，在浏览器访问自己的博客域名，就可以看到我们写的第一篇正经文章啦~ <span class="github-emoji"><span>☀</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2600.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>😼</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f63c.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419101933.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/Blogconstruction/Hexo20210419101753.png"></p>
<ul>
<li>附上<a href="https://sunhwee.com/posts/a927e90e.html#toc-heading-5">Markdown Emoji表情语法速查表</a></li>
</ul>
</li>
</ol>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>Anyway，终于搞完了~</p>
<p>4.12号开始搭，断断续续搞，也遇到了不少bug，4.14才搭的差不多，然后这篇教程又断断续续写了4天 orz</p>
<p>希望能对你有所帮助，欢迎批评指正，爱你ღ( ´･ᴗ･` )</p>
<hr>
<p>阿波罗猫</p>
<p>20210419</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM-READING</title>
    <url>/ORB-SLAM-reading/</url>
    <content><![CDATA[<h2 id="ORB-SLAM：一个多功能的、精确的单目SLAM系统"><a href="#ORB-SLAM：一个多功能的、精确的单目SLAM系统" class="headerlink" title="ORB-SLAM：一个多功能的、精确的单目SLAM系统"></a>ORB-SLAM：一个多功能的、精确的单目SLAM系统</h2><h1 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h1><ul>
<li>场景特征=地图点<h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h1>ORB-SLAM，一个基于特征点法的单目SLAM系统，可以实时运行。该系统包括前端跟踪、后端建图、回环检测、重定位，支持全自动初始化。<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><h2 id="1-1-motivation："><a href="#1-1-motivation：" class="headerlink" title="1.1 motivation："></a>1.1 motivation：</h2></li>
<li>BA和实时性的矛盾：<br> BA可以准确从从视觉图像中提炼出最优的相机位姿和特征点的位置，但是由于运算开销比较大，对于VSLAM这种需求实时性的应用而言，负担不起。</li>
</ul>
<p>BA算法如果想在实时SLAM里用，必须确保：</p>
<ol>
<li>选取部分帧作为关键帧，仅对关键帧做BA来估计特征点位置</li>
<li>考虑到复杂性，关键帧的数量需要合适</li>
<li>一组良好的关键帧中，特征点需要具有显著视差(parallax)，和大量回环匹配【？】</li>
<li>对关键帧对应的相机位姿和特征点位置进行初始化</li>
<li>具有可扩展性</li>
<li>针对回环检测的需求，需要能够快速实现全局优化（位姿图）【？】</li>
</ol>
<ul>
<li><p>前辈的工作</p>
<ul>
<li>PTAM[4]<ul>
<li>仅限于小规模场景</li>
</ul>
</li>
<li>重定位[5]</li>
<li>尺度感知的回环检测[6]</li>
<li>共视信息[7,8]</li>
</ul>
</li>
<li><p>我们的工作：</p>
<ul>
<li>将PTAM的多功能性扩展到对该系统而言困难的环境中。为了实现这一目标，从头开始设计了一个新颖的单目SLAM系统。</li>
</ul>
</li>
</ul>
<h2 id="1-2-Contribution"><a href="#1-2-Contribution" class="headerlink" title="1.2 Contribution:"></a>1.2 Contribution:</h2><ol>
<li>实现ORB-SLAM系统，<ol>
<li>该系统包含<code>跟踪</code>、<code>建图</code>、<code>重定位</code>和<code>回环检测</code>；</li>
<li>使用ORB特征[9]；</li>
<li>允许在无GPU情况下实时；</li>
<li>对视点(viewpoint)和关照具有良好的不变性</li>
</ol>
</li>
<li>在大规模场景下可以实现实时。这点得益于<code>共视图（covisibility graph）</code>的使用，可以在局部范围内跟踪和建图。</li>
<li>实时回环检测。这点得益于使用<code>essential graph</code>来初始化。</li>
<li>实时相机重定位。</li>
<li>基于<code>模型选择</code>的自动初始化程序，建立初始地图。</li>
<li>地图点和关键帧的选择：适者生存策略，宽进严出。</li>
</ol>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><h2 id="2-1-Place-Recognition"><a href="#2-1-Place-Recognition" class="headerlink" title="2.1 Place Recognition"></a>2.1 Place Recognition</h2><h2 id="2-2-地图初始化"><a href="#2-2-地图初始化" class="headerlink" title="2.2 地图初始化"></a>2.2 地图初始化</h2><h2 id="2-3-单目SLAM"><a href="#2-3-单目SLAM" class="headerlink" title="2.3 单目SLAM"></a>2.3 单目SLAM</h2><h1 id="3-系统总览"><a href="#3-系统总览" class="headerlink" title="3. 系统总览"></a>3. 系统总览</h1><h2 id="3-1-特征选择"><a href="#3-1-特征选择" class="headerlink" title="3.1 特征选择"></a>3.1 特征选择</h2><ol>
<li>用于跟踪和建图的特征 = 用于重定位和回环检测的特征</li>
<li>采用ORB特征 [9,11]</li>
</ol>
<h2 id="3-2-三线程：跟踪、局部建图、回环检测"><a href="#3-2-三线程：跟踪、局部建图、回环检测" class="headerlink" title="3.2 三线程：跟踪、局部建图、回环检测"></a>3.2 三线程：跟踪、局部建图、回环检测</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211013210934.png"></p>
<ul>
<li><p>跟踪：对每一帧的相机进行定位，并决定何时插入新的关键帧。（跟踪详情参考section 5，初始化详情参考section 4）</p>
<ol>
<li><p>首先进行位姿初始化。</p>
<ol>
<li>方法1：和前一帧做特征匹配，再使用<code>motion-only BA</code>来优化位姿。</li>
<li>方法2：如果由于遮挡、突然移动等因素导致跟丢，就通过<code>place recognition</code>模块来做全局重定位。</li>
</ol>
</li>
<li><p>track local map。</p>
<p>初始化对相机位姿和特征点【不确定，文章用的是‘特征匹配’】后，使用系统维护的关键帧的共视图来恢复局部可视化地图。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211014003632.png"></p>
<p>然后通过<code>重投影</code>来搜索局部特征点的匹配，再用所以的匹配来再次优化相机位姿。【？】</p>
</li>
<li><p>最后决定是否插入新的关键帧。</p>
</li>
</ol>
</li>
<li><p>局部建图：处理新的关键帧并使用<code>local BA</code>来实现相机位姿环境的最优化重建。（详见section 6）</p>
<p> 在共视图关联的关键帧中搜索新关键帧中未匹配的新ORB特征点，通过三角化来创建新点。</p>
<p> 在创建新点后，根据跟踪过程中获取的信息，使用一个<code>point culling policy</code>来剔除一些点，仅保留高质量的点。</p>
<p> 局部建图还需要剔除一些冗余的关键帧。</p>
</li>
<li><p>回环检测：对每个新的关键帧检测一次回环，再做回环修正。（详情见section 7）</p>
<p> 如果检测到回环，计算一个<code>相似性变换</code>，来获知回环中的漂移累积。</p>
<p> 然后做修正：对齐回环的两边，融合重复的点。</p>
<p> 最后，对<code>similarity constraint</code>[6]做一次位姿图优化，来实现全局一致性。【？】</p>
<p> 主要创意是优化<code>Essential Graph</code>。</p>
<ul>
<li>Essential Graph：共视图的一个稀疏子图。详情见section 3.3</li>
</ul>
</li>
<li><p>优化：使用<code>列文伯格-马夸尔特算法（麦夸特法）</code>+ g2o</p>
</li>
</ul>
<h2 id="3-3-地图点，关键帧，及其选择"><a href="#3-3-地图点，关键帧，及其选择" class="headerlink" title="3.3 地图点，关键帧，及其选择"></a>3.3 地图点，关键帧，及其选择</h2><ul>
<li><p>地图点<code>p_i</code>包含以下信息：</p>
<ul>
<li>世界坐标系下3d坐标<code>X_w,i</code></li>
<li>观测方向 <code>n_i</code>：所有观测方向（连接p_i和观测它的关键帧的光学中心的射线）的平均单位向量。</li>
<li>ORB描述子<code>D_i</code>：其汉明距离在所有观测该点的关键帧所关联的描述子中最小。【？】</li>
<li><code>d_max</code>, <code>d_min</code>：在ORB特征尺度不变性的限制下，可以观测到该点的最大和最小距离。</li>
</ul>
</li>
<li><p>关键帧<code>K_i</code>包含以下信息：</p>
<ul>
<li>相机位姿<code>T_iw</code>：世界坐标系中的点到相机坐标的刚体变化。</li>
<li>相机内参：包括focal length和principal point。</li>
<li>在这帧中提取到的所有ORB特征：这些提取到的特征无所谓是否与地图点相关联。（如果给出了畸变模型，这里的ORB特征坐标是undistorted）</li>
</ul>
</li>
</ul>
<p>地图点和关键帧的创建比较随意，但是剔除策略比较严格，剔除策略需要检测冗余的关键帧以及错误匹配或不可追踪的地图点。正因为有宽进严出的策略，地图可以在探索过程中灵活的扩展，并且在旋转、快速移动等困难条件下，还可以保持一定的跟踪稳定性。地图的大小在同一个环境不断重访的情况下受限，不会无限扩张，可以终生运行。</p>
<p>与PTAM相比，地图包含的点和异常值更更少。</p>
<p>详情请参考6.2和6.5。</p>
<h2 id="3-4-共视图和Essential-Graph"><a href="#3-4-共视图和Essential-Graph" class="headerlink" title="3.4 共视图和Essential Graph"></a>3.4 共视图和Essential Graph</h2><p>关键帧之间的共视信息很重要，这些信息以一无向加全图的形式呈现——共视图。</p>
<p>图中每个节点是一个关键帧，如果两个关键帧共享一定数量(&gt;=15)的地图点，则两帧之间存在一条边，边的权重θ为共享地图点的数量。</p>
<p>在回环校正的过程中，需要进行一次位姿图优化，来沿着图distribute the loop clossing error。由于共视图的所有边画在一起太过密集，我们建一个<code>Essential Graph</code>来保留所有关键帧节点和部分的边。Essential Graph是共视图的一个子图，但是仍是一个强大的网络，且结果依旧准确。</p>
<p>该系统从初始关键帧开始递增的建一棵<code>spanning tree</code>，这棵树表示共视图的连接子图，该子图拥有最少的边。当插入一个新关键帧，spanning tree中会加上这个新关键帧节点，并连接与该节点具有高共视度的关键帧节点。当剔除一个关键帧，系统更新所有受影响的连接。</p>
<p>Essential Graph中包括这个生成树、具有高共视度(θ&gt;=100)的共视图中边的子集、回环检测边【？】，因此形成了一个强大的相机网络。</p>
<p>例子（共视图、生成树、essential graph）：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211014020737.png"></p>
<p>实验见section 8.5。</p>
<h2 id="3-5-Bags-of-Words-Place-Recognition"><a href="#3-5-Bags-of-Words-Place-Recognition" class="headerlink" title="3.5 Bags of Words Place Recognition"></a>3.5 Bags of Words Place Recognition</h2><p>系统嵌入了<code>词袋位置重识别</code>模块，该模块基于词袋模型<code>DBoW2</code>[5]设计。此模块用于回环检测和重定位。</p>
<p>Visual words = visual vocabulary。</p>
<p>该词典是用从一堆图像中提取出的ORB描述子离线生成的。[11]以递增的方式构建了一个数据库，储存了词汇表中每个word在哪一帧中出现的索引。当关键帧被剔除时，数据库也会同步更新【不是离线生成的？】。</p>
<p>由于关键帧存在视觉上的重叠，查询数据库时，不会出现高分only一帧的情况。原始的<code>DBoW2</code>考虑到这个重叠，将时间相近的图像的分数叠加。但是这种做法具有局限性：不能涵盖时间不同但是地点相同的关键帧。鉴于此，我们的方法是：group 共视图中相连的关键帧。此外，我们的数据库返回的是所有分数高于best score【？】75%的关键帧matches。</p>
<p>词袋的另一个优点[5]：当我们需要计算两组ORB特征的关系时，可以将暴力匹配的过程限制在词典树的某一层上的相同节点的特征上【？】。在为新点的三角化测量做特征匹配、回环检测、重定位时，都可以使用这个trick。我们还使用<code>orientation consistency test</code>来细化这个关系[11]，这种方法去除了一些异常值，以确保所有对应关系具有旋转一致性。</p>
<h1 id="4-自动地图初始化"><a href="#4-自动地图初始化" class="headerlink" title="4. 自动地图初始化"></a>4. 自动地图初始化</h1><p>目标：计算两帧的相对位姿，通过三角化测量初始化一组初始地图点。</p>
<p>并行计算两个几何模型：平面场景的单应矩阵(homography) + 非平面场景的基础矩阵</p>
<p>用heuristic方法选择模型，再用算法X来计算相对位姿。</p>
<p>我们算法的前提是确定这两个帧是安全的，即提前检测低视差和二重平面模糊的情况，以避免初始化一个被破坏的地图。</p>
<p>算法步骤：</p>
<ol>
<li><p>找到初始对应关系：</p>
<p>在当前帧F_c的最优尺度下提取ORB特征，和参考帧F_r进行特征匹配，得到匹配的特征X_c——X_r。</p>
<p>如果没有足够的匹配特征，重置参考帧。</p>
</li>
<li><p>并行计算两个模型：</p>
<p>并行计算一个单应矩阵H_cr和一个基础矩阵F_cr。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211014111139.png"></p>
<p>分别采用RANSAC方案中的DLT算法和八点法来计算H和F。[2]</p>
<p>为了统一两个模型的计算步骤，预设好相同的迭代次数、以及每次迭代使用的点（F用8对点，H用4对点）。</p>
<p>在每次迭代中，为每个模型M（F/H）计算一个分数S_M：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211014111759.png"></p>
<ol>
<li>d^2是两帧间的<code>symmetric transfer error</code>[2]</li>
<li>假设标准差为1像素的情况下，阈值T_H=5.99，T_F=3.84。</li>
<li>大gamma = T_H</li>
</ol>
<p>H和F是得分最高的模型。</p>
<p>如果找不到模型了，就回步骤1。</p>
<p>【为啥要迭代？】</p>
</li>
<li><p>模型选择：</p>
<ul>
<li><p>选单应H：平面场景、近似平面场景、低视差场景。</p>
<p> 【检测到低视差（far away【只有far away这种情况吗？】）–&gt;拒绝初始化 or 选H？】</p>
</li>
<li><p>选基础F：具有足够视差的非平面场景。</p>
</li>
<li><p>使用启发式算法来选择模型，计算：</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211014114413.png"></p>
<p>R_H &gt;0.45选H，否则选F。</p>
</li>
<li><p>恢复 Motion and Structure from Motion：</p>
</li>
</ol>
<ul>
<li><p>单应矩阵H模型：采用[23]方法恢复8个(4对)运动假设(R,t)。</p>
<p> [23]中使用<code>cheriality test</code>来选择有效方案。但是该测试在低视点情况下会失效（点容易跑到相机前方或者后方）。</p>
<p> 我们之间对8个方案进行三角化测量，检查是否存在一个解决方案，使得大多数点能看到视差，且都在相机前，且具有低重投影误差。如果没有一个适合的解决方案，就不进行初始化了，直接返回步骤1重新做特征匹配。</p>
</li>
<li><p>基础矩阵F模型：</p>
<p> 将基础矩阵转换成本质矩阵E，其中K为内参(标定矩阵)，</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211015003223.png"></p>
<p> 再用<code>奇异值分解SVD</code>解出4个运动假设(R,t)。再对这4个方案进行三角化测量，选择其中一个来重建。</p>
</li>
</ul>
<p>相当于从H/E中恢复运动R,t，再用三角测量获得特征点的空间位置。</p>
<ol start="5">
<li>BA：<code>full BA</code>，来优化这个初始化重建。</li>
</ol>
<p>例子：</p>
<p>PTAM，LSD-SLAM，ORB-SALM</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211015004447.png"></p>
<h1 id="5-线程1-跟踪"><a href="#5-线程1-跟踪" class="headerlink" title="5. 线程1-跟踪"></a>5. 线程1-跟踪</h1><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211015175758.png"></p>
<p>对相机的每一帧执行该线程。</p>
<p>在线程1中的相机位姿优化仅包含<code>motion-only BA</code>。</p>
<h2 id="5-1-ORB提取"><a href="#5-1-ORB提取" class="headerlink" title="5.1 ORB提取"></a>5.1 ORB提取</h2><p>在8个尺度上(scale factor = 1.2)提取FAST角点。</p>
<p>对于分辨率在512x284——752x480像素的图片，提取1000个角点。</p>
<p>对于分辨率更高（如KITTI数据集上1241x376）的图片，提取2000个角点。</p>
<p>为了确保角点均匀分布，我们在每个尺度上按网格划分，试图在每个单元格内至少提取5个角点。然后我们在每个单元格内检测角点，如果没有检测到足够的角点就调整<code>detector threshold</code>。如果某些单元格内没有角点（无纹理/低对比度），也需要调整该阈值。</p>
<p>在保留的FAST角点上计算方向和ORB描述子。不同于PTAM中对<code>patch correlation</code>的检索，ORB描述子被用于所有特征匹配。</p>
<h2 id="5-2-从先前帧中初始化位姿估计"><a href="#5-2-从先前帧中初始化位姿估计" class="headerlink" title="5.2 从先前帧中初始化位姿估计"></a>5.2 从先前帧中初始化位姿估计</h2><p>如果成果跟踪到最后一帧，就使用一个恒速运动模型来预测相机位姿，并对最后一帧中观察到的地图点进行引导性搜索。如果找不到足够的匹配点（即运动模型明显不对），就在最后一帧中对地图点周围进行更广泛的搜索。根据找到的对应关系优化位姿。</p>
<h2 id="5-3-从全局重定位中初始化位姿估计"><a href="#5-3-从全局重定位中初始化位姿估计" class="headerlink" title="5.3 从全局重定位中初始化位姿估计"></a>5.3 从全局重定位中初始化位姿估计</h2><p>如果跟丢了，就将当前帧转换成词包，然后查找识别数据库中的候选关键帧用于全局重定位。计算与每个关键帧中的地图点相关的ORB的对应关系（3.5）。然后对于每个关键帧进行RANSAC迭代，使用PnP算法估计相机位姿[41]。</p>
<p>如果找到一个有足够<code>inliers</code>的相机位姿，就优化该位姿并对该候选关键帧的地图点进行引导性搜索，找到更多的匹配。最后，再次优化相机位姿，如果有足够的<code>inliers</code>，就继续跟踪。</p>
<h2 id="5-4-跟踪局部地图"><a href="#5-4-跟踪局部地图" class="headerlink" title="5.4 跟踪局部地图"></a>5.4 跟踪局部地图</h2><p>【没看太懂？】</p>
<p>一旦估计了相机位姿，且有了一组初始的匹配特征，就可以将地图投影到当前帧中，并搜索更多的地图点对应关系。为了约束大地图的复杂度，我们只投影局部地图。</p>
<p>局部地图包含一组关键帧<code>K1</code>，它们与当前帧共享地图点；以及一组关键帧<code>K2</code>（关键帧组K1在共视图中的邻居关键帧）。局部地图还包括参考关键帧<code>K_ref</code>（属于<code>K1</code>），参考关键帧与当前帧共享的地图点数量最多。现在，在当前帧中检索每一个K1、K2中可见的地图点：</p>
<ol>
<li>计算当前帧中的地图点投影x。如果超出图像边界，抛弃。</li>
<li>计算当前<code>viewing ray v</code>和地图点<code>mean viewing direction n</code>的夹角。如果v·n&lt;cos60°就抛弃。</li>
<li>计算相机中心到地图点的距离d。如果超过地图点的尺度不变范围[dmin,dmax]就抛弃。</li>
<li>在预测的尺度下，在x附近，比较地图点的描述子D和当前帧中仍未匹配的ORB特征，将地图点与最佳匹配点相连接。</li>
</ol>
<p>使用在帧中找到的所有地图点来优化相机位姿。</p>
<h2 id="5-5-决定新关键帧"><a href="#5-5-决定新关键帧" class="headerlink" title="5.5 决定新关键帧"></a>5.5 决定新关键帧</h2><p>这一步是判断是否要将当前帧作为一个新的关键帧。</p>
<p>局部建图环节有一个机制可以剔除多余的关键帧，为了能更稳健的跟踪复杂的相机运动（如旋转），在这个步骤中我们要尽可能快速的插入关键帧。插入新的关键帧需要满足以下条件：</p>
<ol>
<li>距离上一次全局重定位，至少已过了20帧。</li>
<li>线程2局部建图空闲，或者距离上一次插入关键帧已过了20帧。</li>
<li>当前帧至少跟踪到50个点。</li>
<li>当前帧跟踪到的点至少比参考帧K_ref少90%。</li>
</ol>
<p>不同于PTAM中使用与其他关键帧的距离作为评判标准，我们规定了一个最小的视觉变化（条件4）。条件1可以确保一个好的重定位，而条件3可以确保一个好的跟踪。如果在线程2忙碌时插入关键帧，需要传递一个暂停局部BA的信号【给谁？】，使得新的关键帧可以尽快被处理。</p>
<h1 id="6-线程2-局部建图"><a href="#6-线程2-局部建图" class="headerlink" title="6. 线程2-局部建图"></a>6. 线程2-局部建图</h1><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211015175824.png"></p>
<p>对每一个新的关键帧<code>K_i</code>执行该线程。地图由一个个地图点组成，地图的扩展来自于新地图点的创建。</p>
<h1 id="6-1-插入关键帧"><a href="#6-1-插入关键帧" class="headerlink" title="6.1 插入关键帧"></a>6.1 插入关键帧</h1><p>首先更新共视图：新节点<code>K_i</code>；更新边（根据与其他关键帧共享的地图点数量）。</p>
<p>然后更新生成树，链接<code>K_i</code>与具有most共享点的关键帧节点。【这里的most是最多还是大多？】</p>
<p>然后计算关键帧的词袋representation（有助于三角化新点时提供数据关联）。</p>
<h2 id="6-2-剔除最近的地图点"><a href="#6-2-剔除最近的地图点" class="headerlink" title="6.2 剔除最近的地图点"></a>6.2 剔除最近的地图点</h2><p>地图点如果想保留在地图上，必须在创建后的前三个关键帧中通过一个严格的测试，以确保地图点是可以跟踪的，且没有错误的三角化（由于数据关联错误）。这个地图点必须满足以下两个条件：</p>
<ol>
<li><code>跟踪</code>必须在25%以上的帧（这些帧中该点是预测可见的）中找到该点。</li>
<li>如果从地图点创建开始已经过了一个以上的关键帧，则必须在至少三个关键帧中观察到该点。【？】</li>
</ol>
<p>通过测试的地图点的移除：任何时候，观测到该点的关键帧少于3帧。这种情况会在关键帧剔除、BA抛弃离群观测(outlier observations)的情况下发生。</p>
<p>该策略减少了地图离群点。</p>
<h2 id="6-3-创建新地图点"><a href="#6-3-创建新地图点" class="headerlink" title="6.3 创建新地图点"></a>6.3 创建新地图点</h2><p>三角化共视图中相连关键帧<code>K_c</code>的ORB特征来创建新地图点。</p>
<p>对于每个K_i中未匹配的ORB特征，搜索其他关键帧中的未匹配点来做特征匹配。匹配过程如3.5所述，如果匹配不满足<code>对级约束(epipolar constraint)</code>则将其抛弃。</p>
<p>我们三角化了一对ORB特征，为了接受新点，还要检查两个相机的正向深度、视差、重投影误差以及尺度一致性。一开始，这个地图点只在这两个关键帧中被观测到，但是这个点也会与其他关键帧匹配，所以这个地图点被投影到其余的连接关键帧中，并且按照5.4来检索对应关系。</p>
<h2 id="6-4-局部BA"><a href="#6-4-局部BA" class="headerlink" title="6.4 局部BA"></a>6.4 局部BA</h2><p>局部BA优化</p>
<ol>
<li>当前处理的关键帧<code>K_i</code></li>
<li>共视图中与该帧相连的关键帧<code>K_c</code></li>
<li>这些关键帧可以看到全部的地图点</li>
</ol>
<p>其他可以看到这些地图点但不与该关键帧相连的关键帧也包括在优化范围内，但是固定不变。</p>
<p>被标记为<code>outliers</code>的观测点在优化的中间和最后被抛弃。（具体细节见附录）</p>
<h2 id="6-5-剔除局部关键帧"><a href="#6-5-剔除局部关键帧" class="headerlink" title="6.5 剔除局部关键帧"></a>6.5 剔除局部关键帧</h2><p>为了保持重建地图的紧凑性，局部建图步骤试图检测并删除冗余的关键帧。</p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>对BA有利，因为BA的复杂度随着关键帧数量增多而增大</li>
<li>使得在同一个环境下的<code>lifelong操作</code>具有可行性，因为关键帧的数量增长在相同环境下受限，只有改变场景才会继续增加。</li>
</ol>
<ul>
<li><p>抛弃<code>K_c</code>中的关键帧：</p>
<p> 这些关键帧中，至少有90%的地图点在相同尺度或更精细尺度下的其他三个（至少）关键帧中可见。</p>
<p> 这里设置尺度条件是为了确保地图点保留了最精准的关键帧。</p>
</li>
</ul>
<p>启发：[24]</p>
<h1 id="7-回环检测"><a href="#7-回环检测" class="headerlink" title="7. 回环检测"></a>7. 回环检测</h1><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211015175847.png"></p>
<h2 id="7-1-Loop-Candidates-Detection"><a href="#7-1-Loop-Candidates-Detection" class="headerlink" title="7.1 Loop Candidates Detection"></a>7.1 Loop Candidates Detection</h2><p>首先，计算K_i关键帧的词袋向量与共视图中K_i的邻居关键帧(θmin=30)的相似度得分，保留最低的得分<code>S_min</code>。</p>
<p>然后在<code>recognition database</code>中检索并抛弃所以得分小于<code>S_min</code>的关键帧。这是一个类似DBoW2中归一化分数的操作（增加鲁棒性），但是DBoW2是用先前图像来计算，而我们用的是共视信息。</p>
<p>另外，还有从结果中去掉所以与K_i直接相连的关键帧。</p>
<p>要接受一个回环候选帧，必须连续检测到三个一致的回环候选帧（这些关键帧在共视图中相连）。如果有多个地方看起来像K_i，就有多个回环候选帧。</p>
<h2 id="7-2-计算Similarity-Transformation"><a href="#7-2-计算Similarity-Transformation" class="headerlink" title="7.2 计算Similarity Transformation"></a>7.2 计算Similarity Transformation</h2><p>单目SLAM中，有7个自由度（3旋转、3平移、1比例因子）的地图会发生漂移[6]。为了闭环，我们需要计算从当前关键帧<code>K_i</code>到回环关键帧<code>K_l</code>的<code>相似转换</code>，该值表示回环中的误差累积。这个相似度的计算可以作为回环的几何验证。</p>
<p>首先，计算当前帧与回环候选帧的地图点相关联的ORB特征的对应关系，参见3.5。每个回环候选都有一个3D-3D的对应关系【？】。对于每个回环候选帧交替进行RANSAC迭代，使用<code>Horn方法</code>[42]来寻找<code>相似转换</code>。</p>
<p>如果找到了具有足够多<code>inliers</code>的相似度<code>S_il</code>，就用附录的方法优化它，并对更多的对应关系进行引导性搜索。如果<code>S_il</code>有足够多的inliers，再次优化它，并接受回环<code>K_l</code>。</p>
<h2 id="7-3-回环融合"><a href="#7-3-回环融合" class="headerlink" title="7.3 回环融合"></a>7.3 回环融合</h2><p>回环矫正的第一步就是融合重复的地图点并在共视图中插入新边来将回环闭合。</p>
<p>首先，用<code>相似转换S_il</code>来修正当前关键帧的位姿<code>T_iw</code>，这个修正会传播到K_i的所有邻居节点，串联<code>转换</code>，使得回环的两个边对齐。【？】</p>
<p>所有回环关键帧和邻居关键帧可见的地图点都投影到K_i和它的邻居上，再在投影周围的一个狭窄区域内搜索匹配(如5.4所述)。所有匹配的地图点和那些计算<code>S_il</code>时的inliers都被融合。【？】</p>
<p>所有参与融合的关键帧都将更新它们在共视图中的边，以此来有效的创建闭合回环的新边。</p>
<h2 id="7-4-Essential-Graph优化"><a href="#7-4-Essential-Graph优化" class="headerlink" title="7.4 Essential Graph优化"></a>7.4 Essential Graph优化</h2><p>为了更高效的闭合回环，我们在Essential Graph上执行了一次位姿图优化（3.4），该操作将回环检测误差分布在了图上。该优化是通过相似性变换来纠正尺度漂移的[6]，详见附录。</p>
<p>优化完成后，每个地图点都根据观察它的一个关键帧的修正来进行转换。</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper Reading--RadarSLAM: Radar based Large-Scale SLAM in All Weathers</title>
    <url>/Paper-Reading-RadarSLAM-Radar-based-Large-Scale-SLAM-in-All-Weathers/</url>
    <content><![CDATA[<p>RadarSLAM: Radar based Large-Scale SLAM in All Weathers</p>
<hr>
<p><a href="http://pro.hw.ac.uk/research/radarslam/">主页</a></p>
<p>数据集可下载，无代码；</p>
<p>本文对标orb-slam，实验部分和orb-slam2做的对比。</p>
<h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h1><p>本文提出的RadarSLAM是一个全雷达图形SLAM系统，该系统包含位姿跟踪、局部建图、回环检测、位姿图优化，并使用一个新颖的特征匹配算法、以及从雷达图像中生成的概率点云来提升系统性能。</p>
<p>测试数据集包括公开的雷达数据集以及多个self-collected的雷达序列。实验验证了该系统在各种恶劣天气条件下（如黑夜、浓雾、大雪）都具有SOTA的可靠性和定位精度。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><ul>
<li><p>Motivation：</p>
<ul>
<li>再过去的十年间，基于各种传感器（声呐sonar、相机、LIDAR）的SLAM技术蓬勃发展。但是室外的大规模SLAM仍受限于恶劣的天气条件。</li>
<li>调频连续波（FMCM）雷达传感器可以在多种天气环境下工作（该传感器多使用与自动驾驶和自动机器人领域）</li>
<li>我们想把FMCM雷达传感器应用在SLAM上</li>
</ul>
</li>
<li><p>已知：雷达的landmark extraction &amp; motion estimation：</p>
<ul>
<li>雷达路标路标提取和运动估计方法<ul>
<li>[1,2,3]</li>
<li>[4]，radar scan表示为点云形式，使用ICP算法估计相对运动</li>
</ul>
</li>
<li>雷达里程器(radar odometery)：Feature based geometric method[6,7,8]<ul>
<li>[6]，雷达路标提取算法：估计雷达功率信号</li>
<li>[7]，图匹配算法：实现不需要运动先验的雷达关键点数据关联</li>
</ul>
</li>
<li>雷达里程器：直接法[9]<ul>
<li>[9]，使用Fourier Mellin Transform 和局部图优化来估计相对位姿</li>
</ul>
</li>
<li>基于雷达的定位技术：[5,14,15,16,18,19]</li>
</ul>
</li>
<li><p>已知：基于深度学习的雷达特征提取和里程计方法</p>
<ul>
<li>[12,13,20]<ul>
<li>[13]，学习多个测量结果的coherence，来决定在读数中保留哪些信息；同时生成一个mask来过滤[12]中雷达读数的噪声</li>
<li>[20]，用于在Cartesian雷达图像中检测关键点的自监督框架，除了雷达里程计，该方法还可以用于后续的运动估计和回环检测。</li>
</ul>
</li>
</ul>
</li>
<li><p>本文：</p>
<ul>
<li>提出了一种基于<code>radar geometry</code> 和<code>graph SLAM</code>的<code>RadarSLAM</code>，该系统可以应用于大规模户外场景下的鲁棒性定位和建图。</li>
</ul>
</li>
<li><p>贡献：</p>
<ul>
<li>提出了一种特征匹配和位姿跟踪算法（使用雷达几何和图表征）</li>
<li>从雷达图像中生成概率点云（降低了<code>speckle noises</code>）</li>
<li>基于图优化的全雷达SLAM系统（恶劣天气条件下可工作）</li>
<li>在大规模场景环境进行了大量的实验，首次证明了算法可以在极端天气环境中工作(如浓雾和大雪)。</li>
</ul>
</li>
</ul>
<h1 id="2-雷达传感器"><a href="#2-雷达传感器" class="headerlink" title="2. 雷达传感器"></a>2. 雷达传感器</h1><p>FMCM雷达：不但可以测距，还可以测速（通过分析发射信号个接收信号的频率差来分析预测目标速度）[21]。</p>
<p>目前FMCM主要的问题是存在多种<code>噪声源</code>：范围误差、角误差、假阳性检测、假阴性检测等[22]</p>
<ul>
<li>例如，假阳性检测包括：(产生原因主要因为FMCM传感器对<code>surface reflection</code>和<code>reflector pose</code>高度敏感)<ul>
<li>杂波<code>clutter</code></li>
<li>旁波<code>sidelobes</code></li>
<li>多径反射<code>multi-path reflections</code><ul>
<li>会使得连续帧不一致，引起额外的噪声和异常值(outliers)</li>
</ul>
</li>
<li>接收器饱和<code>receiver saturation</code></li>
</ul>
</li>
</ul>
<p>鉴于此，相比相机和LIDAR数据，雷达读数更嘈杂，这个点造成雷达传感器在运动估计和SLAM中使用困难。</p>
<h2 id="2-1-Radar-Geometry"><a href="#2-1-Radar-Geometry" class="headerlink" title="2.1 Radar Geometry"></a>2.1 Radar Geometry</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021165145.png"></p>
<p>雷达图像常见噪声：</p>
<ul>
<li>斑点噪声speckle noise</li>
<li>接收机饱和receiver saturation</li>
<li>多路径反射multi-path reflection</li>
</ul>
<p>360°FMCM雷达连续扫过<code>Ns``方位角(azimuth angles)</code>（步长=2pi/Ns）。对于每个方位角，雷达发射波束，然后将返回的信号压缩成一个距离，这个过程不考虑高度信息。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021171146.png"></p>
<p>鉴于此，雷达图像可以提供<code>absolute metric information of range distance</code>（也就是相机图像缺失的深度信息）。</p>
<p>原始的<code>polar scan</code>可以转换到<code>笛卡尔空间(Cartesian space)</code>（灰度图像？）。</p>
<ul>
<li>在polar图像中给定点(a,r)，笛卡尔坐标P为：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021171332.png"></li>
</ul>
<p>其中θ=2pi*a/Ns（ranging angle），γ是<code>图像像素空间</code>和<code>世界度量空间</code>的尺度因子</p>
<ul>
<li>为了得到更好的分辨率，笛卡尔图像通常使用<code>bi-linear interpolation</code>来插值。</li>
<li>本文基于笛卡尔表示法。</li>
</ul>
<h1 id="3-基于雷达的SLAM"><a href="#3-基于雷达的SLAM" class="headerlink" title="3. 基于雷达的SLAM"></a>3. 基于雷达的SLAM</h1><p>RadarSLAM的目标是：给定一个radar scans序列，估计雷达位姿和全局一致的地图(使用graph SLAM)。</p>
<p>RadarSLAM系统包括了4个子系统：</p>
<ul>
<li>位姿跟踪</li>
<li>局部建图</li>
<li>回环检测</li>
<li>位姿图优化</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021172426.png"></p>
<h2 id="3-1-位姿跟踪"><a href="#3-1-位姿跟踪" class="headerlink" title="3.1 位姿跟踪"></a>3.1 位姿跟踪</h2><p>位姿跟踪：利用关键帧，对雷达位姿进行连续的online估计。</p>
<p>具体一点，为了在世界坐标系中追踪当前雷达帧t的位姿Ct，需要先算一个当前帧t到关键帧k（位姿为Ck）的转换<code>T_tk</code>。如果Ck已知，Ct就可计算：【用的是哪个关键帧k？是参考帧。】【参考帧怎么确定的？】</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021191056.png"></p>
<hr>
<p>示例：特征提取+特征匹配</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021200031.png"></p>
<p>利用笛卡尔雷达图像中的关键点的<code>geometry</code>来计算T。</p>
<ol>
<li><p>关键点特征提取</p>
<ul>
<li>SURF[23]算法，从当前帧t和关键帧k中分别提取两组关键点特征。</li>
</ul>
</li>
<li><p>根据这两组关键点特征的描述子进行特征匹配。</p>
<ul>
<li><p><strong>与视觉不同</strong>，雷达图像的特征匹配可以利用<code>radar geometry</code> 来实现。（<code>radar geometry</code>可以直接提供<code>metric ranging</code>信息）</p>
</li>
<li><p><code>减少雷达图像特征匹配错误</code>：</p>
<ul>
<li>机制1：引入<code>motion prior</code>（如最大速度）来限制在雷达局部坐标系中搜索关键点时的最大搜索半径。（减少错误匹配数量，降低特征匹配的计算开销）</li>
<li>机制2：<code>pairwise consistenct constraint</code>—成对的inliner关键点对应需要有相似的运动趋势，基于这一原则，进一步排除异常值(outliers)</li>
</ul>
<p> 当前帧t和关键帧k上的一对匹配的关键点需要满足以下<code>pairwise constraint</code>:</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021193627.png"></p>
<p> 其中，<code>||·||2</code>是欧氏距离，<code>P_t_i</code>和<code>P_k_i</code>是局部坐标系中关键点对i的笛卡尔坐标，<code>δ_c</code>是最小距离阈值。</p>
</li>
<li><p>G矩阵：表示满足<code>pariwise consistency</code>约束的所有匹配。</p>
<ul>
<li>寻找匹配一致的最大<code>inliner set</code>等价于求出G表示的图的<a href="https://www.jianshu.com/p/dabbc78471d7">最大团</a>，使用[26]可以有效地解决这个问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>获得最大<code>inliner set</code>后，可以利用匹配的关键点来计算对应的变换矩阵<code>T_tk</code></p>
<ul>
<li>计算方法：<code>SVD</code>[24]</li>
</ul>
</li>
<li><p>得到T后，可以用Eq.2来算当前位姿Ct。这里算出来的Ct是Eq.4优化的初始估计。</p>
</li>
</ol>
<hr>
<p>为了进一步限制<code>local drifts</code>，对Ct做进一步的优化（最小化当前帧t和关键帧k中成功匹配的点对的重投影误差），目标函数为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021200828.png"></p>
<p>其中<code>M_tk</code>是帧t和k的匹配点对集合，<code>P_w_i</code>是世界坐标下的对应地图点。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021201906.png"></p>
<hr>
<p>处理完当前帧t后（根据当前帧估计了Ct），当前帧可以转换成关键帧。</p>
<p>生成关键帧的方法：遵循ORB-SLAM中的关键帧生成方案[25]，该方案要考虑匹配关键点最小数量、当前帧和关键帧之间的平移和旋转。</p>
<h2 id="3-2-局部建图"><a href="#3-2-局部建图" class="headerlink" title="3.2 局部建图"></a>3.2 局部建图</h2><p>局部建图的目标：通过联合优化估计的位姿和局部地图来改善<code>位姿估计</code>和<code>局部地图的一致性</code>。</p>
<p>局部建图线程和位姿跟踪线程并行进行。</p>
<p>一旦生成了一个新的关键帧，该帧上的关键点就变成了地图点【？】。</p>
<p>然后，恢复附近的关键帧和这些关键帧可以观测到的地图点，执行<code>局部BA</code>[27]。即通过最小化损失函数（<code>加权的Squared Error之和</code>）来优化关键帧的位姿和地图点的位置，目标损失函数如下：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021205626.png"></p>
<p>其中<code>X</code>表示关键帧的位姿+地图点位姿，z-z(X)是预测值和观测值之间的<code>residual error</code>，<code>W</code>是正定对称权重矩阵【W哪来的？】。</p>
<p>使用<code>Levenberg-Marquardt梯度法</code>来解优化问题。</p>
<p>为了限制计算开销，最后一个关键帧创建的地图点如果没有被两个以上关键帧观测到就会被剔除。</p>
<h2 id="3-3-回环检测"><a href="#3-3-回环检测" class="headerlink" title="3.3 回环检测"></a>3.3 回环检测</h2><p>鲁棒的回环检测对于减少SLAM系统的漂移而言非常关键。</p>
<p>视觉SLAM在回环检测环节常用词袋模型，但是对于雷达SLAM而言行不通，原因如下：</p>
<ol>
<li>相比于光学图像，雷达图像的像素特征并不明显，这意味着雷达图像会出现大量相似的特征描述子。</li>
<li>雷达中的多路径反射问题会造成特征描述子模糊。</li>
<li>雷达传感器的微小旋转可能会使场景发生巨大的改变，导致描述子的<code>histogram distribution</code>发生较大扭曲。</li>
</ol>
<p>鉴于此，我们采用另一种技术：</p>
<ul>
<li>捕捉场景结构</li>
<li>利用雷达点云反射密度的空间特性。【？】</li>
</ul>
<hr>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021231525.png"></p>
<p>首先，将雷达图像(a)转换成点云。</p>
<p>(b): 局部最大值算法：在每个<code>azimuth</code>的读数中找到局部最大值来做峰值检测。<strong>但是</strong>，这个算法的问题是，由于<code>散斑噪声speckle noise</code>的存在，峰值会在整个雷达图像中随机分布，即使没有目标的地方也会出现峰值。</p>
<p>因此，我们提出了一种<code>使用概率模型的点云生成算法</code>（算法1）。</p>
<ul>
<li>假设每个<code>azimuth scan</code>的<code>peak power s</code>都服从正态分布：<ul>
<li><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021232233.png"></li>
</ul>
</li>
<li>选择<code>&gt;均值μ</code>且<code>&gt;标准差σ</code>的峰值（降低假阳性）</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211021232834.png"></p>
<hr>
<p>从雷达图像中生成点云后，使用M2DP[31]（3D点云中使用的一种具有旋转不变形的全局描述子）作为点云的描述子，来进行回环检测。</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_41398920/article/details/111661861">M2DP</a>：计算平面上点云的<code>density signature</code>，使用这些签名的<code>left and right singular vectors</code>作为点云的描述子。</li>
</ul>
<h2 id="3-4-位姿图优化"><a href="#3-4-位姿图优化" class="headerlink" title="3.4 位姿图优化"></a>3.4 位姿图优化</h2><p>位姿图随着雷达的移动而逐渐建立。</p>
<p>一旦检测到回环，就计算当前帧和检测到的关键帧之间的相对变换，然后将变换添加到位姿图中作为回环约束。</p>
<ul>
<li>计算方法：ICP[32] with RANSAC[33]</li>
</ul>
<p>如果ICP收敛了，就对所有关键帧执行<code>位姿图优化</code>。</p>
<ul>
<li>优化：使用g2o[34]库</li>
</ul>
<p>对关键帧的位姿进行优化后，更新全局地图上的所有地图点。</p>
<p>当全部序列完成时，执行一个全局BA来细化地图。</p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><p>实验包括定量实验和定性实验，用于评估大规模环境+多种天气情况下的RadarSLAM性能。</p>
<h2 id="4-1-Quantitative-Evaluation"><a href="#4-1-Quantitative-Evaluation" class="headerlink" title="4.1 Quantitative Evaluation"></a>4.1 Quantitative Evaluation</h2><h2 id="4-2-Qualitative-Evaluation"><a href="#4-2-Qualitative-Evaluation" class="headerlink" title="4.2 Qualitative Evaluation"></a>4.2 Qualitative Evaluation</h2><h2 id="4-3-Computation-Time"><a href="#4-3-Computation-Time" class="headerlink" title="4.3 Computation Time"></a>4.3 Computation Time</h2><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] J. W. Marck, A. Mohamoud, E. vd Houwen, and R. van Heijster, “Indoor radar slam a radar application for vision and gps denied environments,” in 2013 European Radar Conference. IEEE, 2013.</p>
<p>[2] D. Vivet, P. Checchin, and R. Chapuis, “Localization and mapping using only a rotating fmcw radar sensor,” Sensors, 2013.</p>
<p>[3] F. Schuster, C. G. Keller, M. Rapp, M. Haueis, and C. Curio, “Landmark based radar SLAM using graph optimization,” in IEEE International Conference on Intelligent Transportation Systems. IEEE, 2016.</p>
<p>[4] M. Holder, S. Hellwig, and H. Winner, “Real-time pose graph slam based on radar,” in IEEE Intelligent Vehicles Symposium. IEEE, 2019.</p>
<p>[5] Narula, L., Iannucci, P.A. and Humphreys, T.E., 2020, April. Automotive-radar-based 50-cm urban positioning. In 2020 IEEE/ION Position, Location and Navigation Symposium (PLANS) (pp. 856867). IEEE.</p>
<p>[6] S. Cen and P. Newman, “Precise ego-motion estimation with millimeter-wave radar under diverse and challenging conditions,” in IEEE International Conference on Robotics and Automation. IEEE, 2018, pp. 1–8.</p>
<p>[7] S. H. Cen and P. Newman, “Radar-only ego-motion estimation in difficult settings via graph matching,” in International Conference on Robotics and Automation. IEEE, 2019, pp. 298–304.</p>
<p>[8] R. Aldera, D. De Martini, M. Gadd, and P. Newman, “What could go wrong? introspective radar odometry in challenging environments,” in IEEE Intelligent Transportation Systems Conference. IEEE, 2019.</p>
<p>[9] Y. S. Park, Y.-S. Shin, and A. Kim, “Pharao: Direct radar odometry using phase correlation,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, May 2020.</p>
<p>[10] D. Barnes, M. Gadd, P. Murcutt, P. Newman, and I. Posner, “The oxford radar robotcar dataset: A radar extension to the oxford robotcar dataset,” in IEEE International Conference on Robotics and Automation, 2020.</p>
<p>[11] W. Maddern, G. Pascoe, C. Linegar and P. Newman, “1 Year, 1000km: The Oxford RobotCar Dataset,” The International Journal of Robotics Research (IJRR), 2016</p>
<p>[12] D. Barnes, R. Weston, and I. Posner, “Masking by moving: Learning distraction-free radar odometry from pose information,” arXiv preprint arXiv:1909.03752, 2019.</p>
<p>[13] R. Aldera, D. De Martini, M. Gadd, and P. Newman, “Fast radar motion estimation with a learnt focus of attention using weak supervision,” in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 1190–1196.</p>
<p>[14] S. Saftescu, M. Gadd, D. De Martini, D. Barnes and P. Newman. Kidnapped Radar: Topological Radar Localisation using RotationallyInvariant Metric Learning, in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Paris, France, 2020</p>
<p>[15] M. Gadd, D. De Martini, and P. Newman, Look Around You: Sequence-based Radar Place Recognition with Learned Rotational Invariance, in IEEE/ION Position, Location and Navigation Symposium (PLANS), (Portland, OR, USA), 2020.</p>
<p>[16] Tang, T.Y., De Martini, D., Barnes, D. and Newman, P., 2020. RSLNet: Localising in Satellite Images From a Radar on the Ground. IEEE Robotics and Automation Letters, 5(2), pp.1087-1094.</p>
<p>[17] Tang, T.Y., De Martini, D., Wu, S. and Newman, P., 2020. SelfSupervised Localisation between Range Sensors and Overhead Imagery. arXiv preprint arXiv:2006.02108.</p>
<p>[18] Giseop Kim, Yeong Sang Park, Younghun Cho, Jinyong Jeong, Ayoung Kim, MulRan: Multimodal Range Dataset for Urban Place Recognition. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Paris, May 2020</p>
<p>[19] Jinyong Jeong, Younggun Cho, Young-Sik Shin, Hyunchul Roh and Ayoung Kim, Complex Urban Dataset with Multi-level Sensors from Highly Diverse Urban Environments. International Journal of Robotics Research, 38(6):642-657, 2019</p>
<p>[20] D. Barnes and I. Posner, “Under the radar: Learning to predict robust keypoints for odometry estimation and metric localisation in radar,” in IEEE International Conference on Robotics and Automation, 2020.</p>
<p>[21] M. Skolnik, “Radar handbook,” 1970. [22] S. Cen, “Ego-motion estimation and localization with millimeter-wave scanning radar,” Master’s thesis, University of Oxford, 2019.</p>
<p>[23] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “Speeded-up robust features (surf),” Computer vision and image understanding, vol. 110, no. 3, pp. 346–359, 2008.</p>
<p>[24] J. H. Challis, “A procedure for determining rigid body transformation parameters,” Journal of Biomechanics, vol. 28, pp. 733–737, 1995.</p>
<p>[25] M. J. M. M. Mur-Artal, Ra´ul and J. D. Tard´os, “ORB-SLAM: a versatile and accurate monocular SLAM system,” IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147–1163, 2015.</p>
<p>[26] Konc, J. and Janezic, D., An improved branch and bound algorithm for the maximum clique problem. proteins, 4(5), 2007.</p>
<p>[27] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle adjustmenta modern synthesis,” in International Workshop on Vision Algorithms. Springer, 1999, pp. 298–372.</p>
<p>[28] W. Churchill, “Experience based navigation: Theory, practice and implementation,” 2012.</p>
<p>[29] J. Zhang and S. Singh, “Loam: Lidar odometry and mapping in realtime,” in Proceedings of Robotics: Science and Systems Conference, 2014.</p>
<p>[30] J. Behley and C. Stachniss, “Efficient surfel-based SLAM using 3D laser range data in urban environments,” in Robotics: Science and Systems, 2018.</p>
<p>[31] L. He, X. Wang, and H. Zhang, “M2dp: A novel 3d point cloud descriptor and its application in loop closure detection,” in IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2016, pp. 231–237.</p>
<p>[32] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,” in Sensor fusion IV: control paradigms and data structures, vol. 1611. International Society for Optics and Photonics, 1992, pp. 586–606.</p>
<p>[33] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography,” Communications of the ACM, 1981.</p>
<p>[34] R. K¨ummerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “g2o: A general framework for graph optimization,” in IEEE International Conference on Robotics and Automation. IEEE, 2011.</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>V-SLAM整理1</title>
    <url>/V-SLAM%E6%95%B4%E7%90%861/</url>
    <content><![CDATA[<p>VSLAM学习素材是高翔老师的《视觉SLAM十四讲第二版》。这是我首次接触SLAM，感觉还算有趣。学士学完了，但是还需要系统的梳理一遍知识体系。</p>
<p>内容比较多，放在两个博客里记述，第一部分是基础知识，第二部分是SLAM的前端，第三部分是SLAM的后端，第四部分是SLAM中的回环检测和建图。</p>
<p>好了，废话不多说，开始吧~</p>
<hr>
<h1 id="第1讲-预备知识"><a href="#第1讲-预备知识" class="headerlink" title="第1讲 预备知识"></a>第1讲 预备知识</h1><p>没啥内容，略过</p>
<hr>
<h1 id="第2将-初始SLAM"><a href="#第2将-初始SLAM" class="headerlink" title="第2将 初始SLAM"></a>第2将 初始SLAM</h1><h2 id="2-2-经典视觉SLAM框架"><a href="#2-2-经典视觉SLAM框架" class="headerlink" title="2.2 经典视觉SLAM框架"></a>2.2 经典视觉SLAM框架</h2><p>一图以蔽之：<br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928163420.png"></p>
<p>我们把相机传感器放到一个未知空间中，相机边移动边拍摄，我们的目的是通过这些帧图构建未知空间的地图。那么我们首先要先反向计算出相机是怎么移动的，再根据相机的移动轨迹来建图。<strong>【建图这里也许还要加上拍摄图片，记不清了，先这么理解，后面再说】</strong></p>
<p>我们通过相机传感器得到了数据，现在需要用堆数据来建图。相机在不停的移动，我们把某个时刻得到的帧图交给前端视觉里程器，视觉里程器可以通过相邻帧的数据（这里要调前面传进来的帧图）来估算相邻相机的运动，进而算出此刻相机的位姿，还要估算这部分数据对应的局部地图。前端会将估计的相机位姿和局部地图提供给后端。</p>
<p>这些数据除了要交给视觉里程计，还要同时给回环检测，回环检测会对比当前帧图和之前的帧图，来判断相机是否之前来过此地，一旦检测到回环，就会将回环信息提供给后端。搞回环主要是为了修正漂移。</p>
<p>后端接收前端和回环给的信息，将这些信息进行优化，得到全局一致的相机轨迹和地图。</p>
<p>建图环节会根据估计的轨迹和任务需求，来建立不同的地图。</p>
<h2 id="2-3-SLAM问题的数学表达"><a href="#2-3-SLAM问题的数学表达" class="headerlink" title="2.3 SLAM问题的数学表达"></a>2.3 SLAM问题的数学表达</h2><p>运动方程：k时刻相机位姿&lt;—k-1时刻位姿x + k时刻运动传感器的读数/输入u + k 时刻噪声w。</p>
<p>观测方程：j位置k时刻的观测数据z&lt;—j位置路标y + k时刻相机位姿x + 噪声v，相机对路标点拍摄后得到图像中的像素。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928170515.png"></p>
<p>在SLAM里面，通常会已知运动测量的读数u（输入数据）来估计位置x（定位），已知传感器读数z（观测数据）来估计y（建图）。</p>
<p><em>这俩方程其实挺不规整的，要估计的值一个在右边一个在左边…有毒。</em></p>
<h2 id="2-4-编程"><a href="#2-4-编程" class="headerlink" title="2.4 编程"></a>2.4 编程</h2><ol>
<li>linux 18.04</li>
<li>g++编译器</li>
<li>cmake编译器</li>
<li>IDE：KDevelop</li>
</ol>
<hr>
<h1 id="第3讲-三维空间刚体运动"><a href="#第3讲-三维空间刚体运动" class="headerlink" title="第3讲 三维空间刚体运动"></a>第3讲 三维空间刚体运动</h1><h2 id="3-1-旋转矩阵"><a href="#3-1-旋转矩阵" class="headerlink" title="3.1 旋转矩阵"></a>3.1 旋转矩阵</h2><h3 id="3-1-1-点、向量、坐标系"><a href="#3-1-1-点、向量、坐标系" class="headerlink" title="3.1.1 点、向量、坐标系"></a>3.1.1 点、向量、坐标系</h3><p>反对称符号^</p>
<h3 id="3-1-2-坐标系间的欧式变换"><a href="#3-1-2-坐标系间的欧式变换" class="headerlink" title="3.1.2 坐标系间的欧式变换"></a>3.1.2 坐标系间的欧式变换</h3><p>将坐标系A中的向量通过计算变换到坐标系B中，可以用<strong>欧式变换</strong>来实现。</p>
<p>欧式变换包括旋转和平移。其中旋转可以用一个<strong>旋转矩阵R</strong>来描述，旋转+平移用<strong>变换矩阵T</strong>来描述。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928175923.png"></p>
<p>需要注意的是：</p>
<ul>
<li>旋转矩阵是一个正交矩阵，其逆亦为其转置</li>
<li>旋转矩阵的逆表示一个相反的旋转</li>
<li>R12指坐标系2变换到坐标系1</li>
</ul>
<h3 id="3-1-3-变换矩阵与齐次坐标"><a href="#3-1-3-变换矩阵与齐次坐标" class="headerlink" title="3.1.3 变换矩阵与齐次坐标"></a>3.1.3 变换矩阵与齐次坐标</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928175642.png"></p>
<p>通过引入齐次坐标，两次变换的叠加便可以用两个变换矩阵相乘来表示，算是一个数学上的trick。</p>
<p>需要注意，旋转矩阵不引入齐次坐标。</p>
<h2 id="3-2-编程1"><a href="#3-2-编程1" class="headerlink" title="3.2 编程1"></a>3.2 编程1</h2><p>Eigen库：主要用来作矩阵运算</p>
<h2 id="3-3-旋转向量和欧拉角"><a href="#3-3-旋转向量和欧拉角" class="headerlink" title="3.3 旋转向量和欧拉角"></a>3.3 旋转向量和欧拉角</h2><h3 id="3-3-1-旋转向量"><a href="#3-3-1-旋转向量" class="headerlink" title="3.3.1 旋转向量"></a>3.3.1 旋转向量</h3><p>旋转矩阵缺陷：</p>
<ul>
<li>自由度冗余</li>
<li>矩阵自带约束</li>
</ul>
<p>旋转向量是另一种描述选择的方式，由旋转轴+旋转角组成。</p>
<p>（nθ）</p>
<p>旋转向量和旋转矩阵直接可以相互转换。</p>
<h3 id="3-3-2-欧拉角"><a href="#3-3-2-欧拉角" class="headerlink" title="3.3.2 欧拉角"></a>3.3.2 欧拉角</h3><p>首先，这个欧拉角和欧拉变换没啥直接关系，就是撞名了。</p>
<p>由于旋转矩阵和旋转向量都不够直观，这里引出足够直观的另一种描述旋转的方式：欧拉角。</p>
<p>（[r,p,y]^T）</p>
<p>欧拉角将一次旋转分解为3次绕不同轴的旋转：</p>
<ul>
<li>绕z，偏航角yaw</li>
<li>绕y，俯仰角pitch</li>
<li>绕x，滚转角roll</li>
</ul>
<p>例如，rpy角的旋转顺序是ZYX</p>
<p>欧拉角的缺陷：万向锁，即欧拉角存在奇异性。</p>
<h2 id="3-4-四元数"><a href="#3-4-四元数" class="headerlink" title="3.4 四元数"></a>3.4 四元数</h2><h3 id="3-4-1-四元数的定义"><a href="#3-4-1-四元数的定义" class="headerlink" title="3.4.1 四元数的定义"></a>3.4.1 四元数的定义</h3><ul>
<li>旋转矩阵缺点：不够直观，冗余</li>
<li>旋转向量缺点：不够直观，奇异性</li>
<li>欧拉角缺点：奇异性</li>
<li>四元数缺点：不够直观，运算复杂</li>
</ul>
<p>单位四元数可以描述一个三维旋转。</p>
<p>四元数和复数类有点像，一个实部+三个虚部。</p>
<p>q=q0+q1i+q2j+q3k;</p>
<p>q=[s,v]^T, s=q0, v=[q1,q2,q3]^T</p>
<h3 id="3-4-2-四元数的运算"><a href="#3-4-2-四元数的运算" class="headerlink" title="3.4.2 四元数的运算"></a>3.4.2 四元数的运算</h3><ol>
<li>加减</li>
<li>乘法</li>
<li>模长</li>
<li>共轭</li>
<li>逆</li>
<li>数乘</li>
</ol>
<h3 id="3-4-3-用四元数表示旋转"><a href="#3-4-3-用四元数表示旋转" class="headerlink" title="3.4.3 用四元数表示旋转"></a>3.4.3 用四元数表示旋转</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928204734.png">，</p>
<p>其中q为旋转四元数，等于把p转到四维空间再转回来。</p>
<h3 id="3-4-4-四元数到其他旋转表示的旋转"><a href="#3-4-4-四元数到其他旋转表示的旋转" class="headerlink" title="3.4.4 四元数到其他旋转表示的旋转"></a>3.4.4 四元数到其他旋转表示的旋转</h3><p>简单来说：</p>
<ul>
<li>四元数和旋转向量可以相互转换</li>
<li>旋转矩阵和旋转向量可以相互转换</li>
</ul>
<h2 id="3-5-相似、仿射、射影变换"><a href="#3-5-相似、仿射、射影变换" class="headerlink" title="3.5 相似、仿射、射影变换"></a>3.5 相似、仿射、射影变换</h2><p>这些变化都是前面的欧式变换的拓展，思路差不多：</p>
<ol>
<li>相似变换</li>
<li>仿射变换</li>
<li>射影变换</li>
</ol>
<h2 id="3-6-编程2"><a href="#3-6-编程2" class="headerlink" title="3.6 编程2"></a>3.6 编程2</h2><p>Eigen几何模块</p>
<h2 id="3-7-编程3"><a href="#3-7-编程3" class="headerlink" title="3.7 编程3"></a>3.7 编程3</h2><p>可视化演示</p>
<h3 id="3-7-1-显示运动轨迹"><a href="#3-7-1-显示运动轨迹" class="headerlink" title="3.7.1 显示运动轨迹"></a>3.7.1 显示运动轨迹</h3><p>Pangolin库：支持3D绘图</p>
<ul>
<li>轨迹文件每一行格式：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928205725.png">，其中t为平移。</p>
<ul>
<li>所谓轨迹，就是把一系列机器人坐标系原点在世界坐标系中的坐标连起来：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928210214.png"></p>
<h3 id="3-7-2-显示相机位姿"><a href="#3-7-2-显示相机位姿" class="headerlink" title="3.7.2 显示相机位姿"></a>3.7.2 显示相机位姿</h3><hr>
<h1 id="第4讲-李群与李代数"><a href="#第4讲-李群与李代数" class="headerlink" title="第4讲 李群与李代数"></a>第4讲 李群与李代数</h1><h2 id="4-1-李群与李代数基础"><a href="#4-1-李群与李代数基础" class="headerlink" title="4.1 李群与李代数基础"></a>4.1 李群与李代数基础</h2><p>旋转矩阵构成了特殊正交群SO(3)，变换矩阵构成了特殊欧式群SE(3)。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210928222743.png"></p>
<p>旋转矩阵和变换矩阵——只对乘法封闭，对加法不封闭。</p>
<p>群：只有一个封闭运算的集合</p>
<h3 id="4-1-1-群"><a href="#4-1-1-群" class="headerlink" title="4.1.1 群"></a>4.1.1 群</h3><p>更严谨的定义一下群：群是一种集合加上一种运算的代数结构。<br>群里的集合对于群的运算满足以下性质：</p>
<ul>
<li>封闭性</li>
<li>结合律</li>
<li>幺元</li>
<li>逆</li>
</ul>
<p>李群：具有连续性质的群。</p>
<ul>
<li>SO(3)、SE(3)</li>
</ul>
<p>但是由于SO(3)和SE(3)只有定义良好的乘法而没有加法，因此难以求导，故，引入李代数。</p>
<p>每一个李群都对应着一个李代数。</p>
<h3 id="4-1-2-李代数的引出"><a href="#4-1-2-李代数的引出" class="headerlink" title="4.1.2 李代数的引出"></a>4.1.2 李代数的引出</h3><p>对旋转矩阵求导引出的概念</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929091937.png"></p>
<h3 id="4-1-3-李代数的定义"><a href="#4-1-3-李代数的定义" class="headerlink" title="4.1.3 李代数的定义"></a>4.1.3 李代数的定义</h3><p>每个李群对应一个李代数。</p>
<p>李代数描述李群的局部性质，是单位元附近的正切空间。</p>
<p>李代数g：集合V + 数域F + 二元运算(李括号)[,]</p>
<h3 id="4-1-4-李代数so-3"><a href="#4-1-4-李代数so-3" class="headerlink" title="4.1.4 李代数so(3)"></a>4.1.4 李代数so(3)</h3><p>SO(3)对应的李代数so(3)里的元素记作Φ：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929091427.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929091737.png"></p>
<p>so(3)是一个由三维向量组成的集合，每个向量对应一个反对称矩阵，可以用于表达旋转矩阵的导数。</p>
<p>so(3)与SO(3)关系：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929092033.png"></p>
<h3 id="4-1-5-李代数se-3"><a href="#4-1-5-李代数se-3" class="headerlink" title="4.1.5 李代数se(3)"></a>4.1.5 李代数se(3)</h3><p>se(3)的每个元素是一个6维向量，前三维表示平移，后三维表示旋转，其后三维就是so(3)的元素Φ。因此，se(3)可以理解为一个三维平移加上一个so(3)元素。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929092633.png"></p>
<ul>
<li>∧：指代从向量到矩阵</li>
<li>∨：指代从矩阵到向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929092531.png"></p>
<h2 id="4-2-指数与对数映射"><a href="#4-2-指数与对数映射" class="headerlink" title="4.2 指数与对数映射"></a>4.2 指数与对数映射</h2><p>前面有提到so(3)和SO(3)之间可以用exp来转换，这章主要是说so(3)、se(3)和SO(3)、SE(3)之间的转换（也就是这个exp）究竟是如何计算的。</p>
<p>有趣的是，SO(3)对应旋转矩阵，而so(3)对应旋转向量。</p>
<h3 id="4-2-1-SO-3-上的指数映射"><a href="#4-2-1-SO-3-上的指数映射" class="headerlink" title="4.2.1 SO(3)上的指数映射"></a>4.2.1 SO(3)上的指数映射</h3><ul>
<li>so(3)—&gt;SO(3)：通过指数映射，可以将so(3)中的任意一个向量对应到SO(3)中的一个旋转矩阵上。</li>
</ul>
<p>R=exp(Φ^)=exp(θa^), 其中θ是模长，a是方向。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929094246.png"></p>
<ul>
<li>SO(3)—&gt;so(3)：对数映射</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929094947.png"></p>
<ul>
<li>需要注意的是，这种映射并不是一一对应的，一个SO(3)可能对应多个so(3)。</li>
</ul>
<h3 id="4-2-2-SE-3-上的指数映射"><a href="#4-2-2-SE-3-上的指数映射" class="headerlink" title="4.2.2 SE(3)上的指数映射"></a>4.2.2 SE(3)上的指数映射</h3><ul>
<li>se(3)—&gt;SE(3)：指数映射</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929095400.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929095524.png"></p>
<ul>
<li><p>SE(3)—&gt;se(3)：略</p>
</li>
<li><p><strong>总结一下转换关系：</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929095957.png"></p>
<h2 id="4-3-李代数求导与扰动模型"><a href="#4-3-李代数求导与扰动模型" class="headerlink" title="4.3 李代数求导与扰动模型"></a>4.3 李代数求导与扰动模型</h2><h3 id="4-3-1-BCH公式与近似模型"><a href="#4-3-1-BCH公式与近似模型" class="headerlink" title="4.3.1 BCH公式与近似模型"></a>4.3.1 BCH公式与近似模型</h3><p>探索一下李群乘法和李代数加法之间的关系。</p>
<ul>
<li><p>SO(3) &amp; so(3)：</p>
<ul>
<li><p>李群乘法：在李群上左乘小量，等于在李代数上加上一个左雅克比的逆乘小量</p>
<p>给旋转矩阵R左乘一个微小扰动—ΔR·R（左乘右乘区别不大，可以互推）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929101057.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929101149.png"></p>
</li>
<li><p>李代数加法：在李代数上加上小量，等于在李群上左乘一个左雅克比乘小量</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929101614.png"></p>
</li>
</ul>
</li>
<li><p>SE(3) &amp; se(3)：</p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929101716.png"></p>
</li>
</ul>
<h3 id="4-3-2-SO-3-上的李代数求导"><a href="#4-3-2-SO-3-上的李代数求导" class="headerlink" title="4.3.2 SO(3)上的李代数求导"></a>4.3.2 SO(3)上的李代数求导</h3><p>选择矩阵R的求导在SLAM上有很强的应用性：</p>
<p>观测方程：z=Tp+w，相机以T位姿观察到p点，在w噪声情况下产生观测数据z。</p>
<p>虽然有时候我们是用观测数据和通过运动方程得到的相机位姿来估计路标点p，但有时候并没有相机位姿反而有路标点p的数据，我们也可以利用路标点和观测来估计位姿。在这种情况下，相当于寻找一个最优的T，使得整体误差最小化：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929102802.png"></p>
<p>这是最小二乘估计，求解时目标函数J要对变换矩阵T求导。</p>
<p>总之就是SLAM里面经常出现”与位姿相关的函数对位姿求导”的情况，这时就需要对李群求导，但由于李群没有加法不太好搞，于是需要利用李代数来解决李群上的求导问题，思路包括：</p>
<ol>
<li>李代数求导模型：通过李群到李代数的转换，用李代数来表示位姿，然后对李代数求导</li>
<li>扰动模型：对李群左乘（右乘）微小扰动，对该扰动求导</li>
</ol>
<h3 id="4-3-3-李代数求导"><a href="#4-3-3-李代数求导" class="headerlink" title="4.3.3 李代数求导"></a>4.3.3 李代数求导</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929104458.png"> 等效于 <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929104519.png"></p>
<p>这个思路逻辑比较清晰，但是计算略麻烦，故通常并不使用这个求导思路。</p>
<h3 id="4-3-4-扰动模型（左乘）"><a href="#4-3-4-扰动模型（左乘）" class="headerlink" title="4.3.4 扰动模型（左乘）"></a>4.3.4 扰动模型（左乘）</h3><p>空间点p经过一次旋转R，对左扰动对应的李代数求导：设左扰动ΔR对应李代数φ</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929104832.png"></p>
<h3 id="4-3-5-SE-3-上的李代数求导"><a href="#4-3-5-SE-3-上的李代数求导" class="headerlink" title="4.3.5 SE(3)上的李代数求导"></a>4.3.5 SE(3)上的李代数求导</h3><p>扰动模型</p>
<p>空间点p经过一次变化T，对左扰动对应的李代数求导</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929105439.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929105514.png"></p>
<h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><h3 id="4-4-1-Sophus基本使用"><a href="#4-4-1-Sophus基本使用" class="headerlink" title="4.4.1 Sophus基本使用"></a>4.4.1 Sophus基本使用</h3><p>Sophus库：李群/李代数库，本书使用带模板的Sophus库</p>
<h3 id="4-4-2-评估轨迹误差"><a href="#4-4-2-评估轨迹误差" class="headerlink" title="4.4.2 评估轨迹误差"></a>4.4.2 评估轨迹误差</h3><p>误差指标：</p>
<ul>
<li>绝对轨迹误差ATE</li>
<li>绝对平移误差</li>
<li>相对位姿误差RPE</li>
<li>相对位姿平移误差</li>
</ul>
<h2 id="4-5-相似变换群与李代数"><a href="#4-5-相似变换群与李代数" class="headerlink" title="4.5 相似变换群与李代数"></a>4.5 相似变换群与李代数</h2><p>没看，略</p>
<hr>
<h1 id="第5讲-相机与图像"><a href="#第5讲-相机与图像" class="headerlink" title="第5讲 相机与图像"></a>第5讲 相机与图像</h1><h2 id="5-1-相机模型"><a href="#5-1-相机模型" class="headerlink" title="5.1 相机模型"></a>5.1 相机模型</h2><p>相机将三维世界坐标点（m）映射到二维图像平面（像素）的过程可以用一个几何模型来描述。</p>
<p>相机内参数：针孔模型 + 畸变模型，外部三维点投影到相机内部的成像平面。</p>
<h3 id="5-1-1-针孔相机模型"><a href="#5-1-1-针孔相机模型" class="headerlink" title="5.1.1 针孔相机模型"></a>5.1.1 针孔相机模型</h3><p>步骤：</p>
<ul>
<li>现实世界P[X,Y,Z]^T</li>
<li>成像平面P’[X’,Y’,Z’]^T<ul>
<li>真实成像平面</li>
<li>对称成像平面</li>
<li>归一化成像平面[x,y]^T </li>
</ul>
</li>
<li>像素平面[u,v]^T：在成像平面上对像进行采样和量化</li>
</ul>
<p><strong>内参</strong>：矩阵K为相机的内参数，有时生产厂商会告诉你，但有时也需要自己去标定。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929113717.png"></p>
<p><strong>外参</strong>：上面公式中的P实际上是<code>相机的世界坐标</code>根据<code>相机位姿</code>变换到<code>相机坐标系</code>下的结果。<code>相机位姿</code>，也就是这个变换矩阵就是<code>相机外参</code>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929114250.png"></p>
<p>这里的外参是SLAM中的待估计目标，代表着机器人的轨迹。</p>
<p>值得一提的是，这个过程中丢失了深度信息。</p>
<h3 id="5-1-2-畸变模型"><a href="#5-1-2-畸变模型" class="headerlink" title="5.1.2 畸变模型"></a>5.1.2 畸变模型</h3><p>径向畸变（失真）：透镜形状引起的畸变</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929114910.png"></p>
<p>切向畸变：透镜未和成像平面严格平行引起的畸变</p>
<p>畸变系数：k1, k2, k3, p1, p2（前3个为径向的，后2个为切向的）</p>
<p>畸变变换发生在<code>归一化成像平面[x,y]^T</code>到<code>像素平面[u,v]^T</code>的过程中：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929115410.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929115515.png"></p>
<ul>
<li>去畸变处理（畸变矫正）<ul>
<li>对整张图像去畸变，再用这个图像去对应空间位置</li>
<li>对图像上的每个像素点去畸变，对应其畸变前的空间点</li>
</ul>
</li>
</ul>
<p><strong>总的来说</strong>：</p>
<ul>
<li><code>世界坐标Pw</code> –+外参T–&gt;</li>
<li><code>相机坐标Pc</code> —&gt; </li>
<li><code>归一化坐标</code> –+畸变–+内参K(α,β,c,f)–&gt; </li>
<li><code>像素坐标Puv</code></li>
</ul>
<h3 id="5-1-3-双目相机模型"><a href="#5-1-3-双目相机模型" class="headerlink" title="5.1.3 双目相机模型"></a>5.1.3 双目相机模型</h3><p>双目相机的原理是同步采集左右相机图像，计算图像<code>视差</code>，计算每个像素点的深度。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929120525.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929120549.png">，b为基线</p>
<p>计算量比较大，要用GPU或FPGA来实现实时计算。</p>
<h3 id="5-1-4-RGB-D-相机模型"><a href="#5-1-4-RGB-D-相机模型" class="headerlink" title="5.1.4 RGB-D 相机模型"></a>5.1.4 RGB-D 相机模型</h3><p>RGB-D相机原理：主动测量每个像素深度</p>
<ul>
<li>通过<code>红外结构光</code>原理测量 深度</li>
<li>通过<code>飞行时间</code>原理测量深度</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929121048.png"></p>
<p>主要问题是，发射-接收测量方式使得使用范围比较受限。</p>
<h2 id="5-2-图像"><a href="#5-2-图像" class="headerlink" title="5.2 图像"></a>5.2 图像</h2><p>计算机中图像存储为数值矩阵。</p>
<p>灰度图：每个像素位置对应一个灰度值I</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">char</span> image[<span class="number">480</span>][<span class="number">640</span>]   <span class="comment">//一张宽640像素，高480像素分辨率的灰度图</span></span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">char</span> pixel = image[y][x]   <span class="comment">//访问图像像素，pixel=I(x,y)，注意下标</span></span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>深度图</li>
<li>彩色图：多通道<ul>
<li>opencv中通道顺序是B、G、R</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929121909.png"></p>
<h2 id="5-3-编程：计算机中的图像"><a href="#5-3-编程：计算机中的图像" class="headerlink" title="5.3 编程：计算机中的图像"></a>5.3 编程：计算机中的图像</h2><h3 id="5-3-1-opencv基本使用"><a href="#5-3-1-opencv基本使用" class="headerlink" title="5.3.1 opencv基本使用"></a>5.3.1 opencv基本使用</h3><p>opencv库：图像处理算法库</p>
<p>本书用的是opencv3</p>
<h3 id="5-3-2-图像去畸变"><a href="#5-3-2-图像去畸变" class="headerlink" title="5.3.2 图像去畸变"></a>5.3.2 图像去畸变</h3><figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">cv::<span class="built_in">Undistort</span>() <span class="comment">//去畸变函数</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="5-4-编程：3D视觉"><a href="#5-4-编程：3D视觉" class="headerlink" title="5.4 编程：3D视觉"></a>5.4 编程：3D视觉</h2><h3 id="5-4-1-双目视觉"><a href="#5-4-1-双目视觉" class="headerlink" title="5.4.1 双目视觉"></a>5.4.1 双目视觉</h3><p>从双目视觉的左右图像出发，计算图像对应的视差图，然后将像素点转换回相机坐标，构成物体的点云。</p>
<p>SGBM算法：计算视差</p>
<h3 id="5-4-2-RGB-D视觉"><a href="#5-4-2-RGB-D视觉" class="headerlink" title="5.4.2 RGB-D视觉"></a>5.4.2 RGB-D视觉</h3><p>位姿记录形式：[x,y,z,qx,qy,qz,qw]—平移向量+旋转四元数</p>
<ol>
<li>根据内参计算一对RGB-D图像对应的点云</li>
<li>根据各张图的相机位姿，把点云加起来，组成地图</li>
</ol>
<hr>
<h1 id="第6将-非线性优化"><a href="#第6将-非线性优化" class="headerlink" title="第6将 非线性优化"></a>第6将 非线性优化</h1><h2 id="6-1-状态估计问题"><a href="#6-1-状态估计问题" class="headerlink" title="6.1 状态估计问题"></a>6.1 状态估计问题</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929164600.png"></p>
<p>根据第5讲的内容，观测方程可以写成：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929163749.png"></p>
<p>SLAM的目标是通过带噪声的观测数据z和输入u，来推断位姿x和地图y，这就是个状态估计问题。</p>
<p>两种方案：</p>
<ul>
<li>滤波器/增量方法<ul>
<li>在当前时刻的估计状态下，来一个新数据，更新一次状态</li>
<li>只关心当前的xk</li>
</ul>
</li>
<li>批量方法<ul>
<li>攒一波数据，然后一并处理</li>
<li>目前的主流</li>
<li>为了达到实时性，可以固定一些历史轨迹，仅对当前时刻附近的一些轨迹进行优化（滑动窗口估计）</li>
</ul>
</li>
</ul>
<h3 id="6-1-1-批量状态估计与最大后验估计"><a href="#6-1-1-批量状态估计与最大后验估计" class="headerlink" title="6.1.1 批量状态估计与最大后验估计"></a>6.1.1 批量状态估计与最大后验估计</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929170517.png"></p>
<p>MAP估计：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929170545.png"></p>
<p>ML估计：在什么样的状态下(x,y)下，最可能产生当前观测到的数据</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929170719.png"></p>
<h3 id="6-1-2-最小二乘的引出"><a href="#6-1-2-最小二乘的引出" class="headerlink" title="6.1.2 最小二乘的引出"></a>6.1.2 最小二乘的引出</h3><p>先不考虑运动方程，且假设噪声是高斯分布。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929171226.png"></p>
<p>这个式子等同于在最小化噪声项，因此ML估计可以等价于最小化误差项(噪声)。</p>
<ul>
<li>上式的Q逆是信息矩阵</li>
<li>噪声项是一个二次型，称为马氏距离。</li>
</ul>
<hr>
<p>现在假设运动方程，考虑批量数据。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929171708.png">（因为独立）</p>
<p>上式的ML估计等同于最小化所有时刻的误差项：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929171832.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929171855.png"></p>
<p>这就是一个<strong>最小二乘问题</strong>了。</p>
<h3 id="6-1-3-例子：批量状态估计"><a href="#6-1-3-例子：批量状态估计" class="headerlink" title="6.1.3 例子：批量状态估计"></a>6.1.3 例子：批量状态估计</h3><p>略</p>
<h2 id="6-2-非线性最小二乘"><a href="#6-2-非线性最小二乘" class="headerlink" title="6.2 非线性最小二乘"></a>6.2 非线性最小二乘</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929173626.png"></p>
<p>这是一个比较正常的最小二乘的样子，一般用<code>梯度下降</code>来解</p>
<p>具体步骤：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174103.png"></p>
<h3 id="6-2-1-一阶和二阶梯度法"><a href="#6-2-1-一阶和二阶梯度法" class="headerlink" title="6.2.1 一阶和二阶梯度法"></a>6.2.1 一阶和二阶梯度法</h3><p>J：一阶导数，梯度、雅克比矩阵</p>
<p>H：二阶导数，海塞矩阵</p>
<ul>
<li><p>一阶梯度：最速下降法（并不是最小二乘）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174156.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174253.png"></p>
</li>
<li><p>二阶梯度：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174411.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174516.png"></p>
</li>
<li><p>缺点：</p>
<ul>
<li>最速过于贪心</li>
<li>二阶里的H过于难算</li>
</ul>
</li>
</ul>
<h3 id="6-2-2-高斯牛顿法"><a href="#6-2-2-高斯牛顿法" class="headerlink" title="6.2.2 高斯牛顿法"></a>6.2.2 高斯牛顿法</h3><p>对误差：f(x)，而不是目标函数：F(x)，做一阶泰勒展开：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174805.png"></p>
<p>构建最小二乘问题：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929174937.png"></p>
<p>得到<code>增量方程（高斯牛顿方程）</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929175144.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929175209.png"></p>
<p>可以看到，高斯牛顿法用JJ^T来做H的近似，避免了H的复杂计算。</p>
<ul>
<li><p>算法步骤：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929175400.png"></p>
</li>
<li><p>算法缺点：</p>
<ul>
<li>JJ^T不一定可逆，可能会导致无法收敛</li>
</ul>
</li>
<li><p>补救措施：</p>
<ul>
<li>线搜索法：加上步长，即先找到方向，再确定长度。</li>
<li>阻尼牛顿：引入信赖区域，认为近似只在区域内可靠</li>
</ul>
</li>
</ul>
<h3 id="6-2-3-列文伯格-马夸尔特方法"><a href="#6-2-3-列文伯格-马夸尔特方法" class="headerlink" title="6.2.3 列文伯格-马夸尔特方法"></a>6.2.3 列文伯格-马夸尔特方法</h3><p>又名阻尼牛顿法。</p>
<p>根据近似模型下降的值 如果能近似等于 函数实际下降的值，说明这个近似很nice，可以扩大近似的范围。（下式接近1）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929181536.png"></p>
<p>算法步骤：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929181856.png"></p>
<p>拉格朗日乘子来解，最后推导到求解：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929183816.png"></p>
<p>值得注意的是，上面的非线性优化都需要提供初始值，SLAM中，通常使用ICP、PnP等算法来提供初始值x0。</p>
<h2 id="6-3-编程：曲线拟合问题"><a href="#6-3-编程：曲线拟合问题" class="headerlink" title="6.3 编程：曲线拟合问题"></a>6.3 编程：曲线拟合问题</h2><h3 id="6-3-1-手写高斯牛顿算法"><a href="#6-3-1-手写高斯牛顿算法" class="headerlink" title="6.3.1 手写高斯牛顿算法"></a>6.3.1 手写高斯牛顿算法</h3><h3 id="6-3-2-使用Ceres进行曲线拟合"><a href="#6-3-2-使用Ceres进行曲线拟合" class="headerlink" title="6.3.2 使用Ceres进行曲线拟合"></a>6.3.2 使用Ceres进行曲线拟合</h3><p>Cere库：最小二乘问题求解库</p>
<p>Ceres求解的最小二乘问题的一般形式（带边界的核函数最小二乘）：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929185010.png"></p>
<h4 id="图优化理论"><a href="#图优化理论" class="headerlink" title="图优化理论"></a>图优化理论</h4><p>G2o库：图优化库，SLAM领域广泛使用。</p>
<p>图优化是一种将非线性优化和图论结合起来的理论，将优化问题用图（贝叶斯图）的形式呈现。</p>
<p>图：顶点+边</p>
<ul>
<li>顶点：表示优化变量</li>
<li>边：表示误差项</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929191415.png"></p>
<ul>
<li>使用g2o拟合曲线</li>
</ul>
<p>需要先将问题转化为图优化问题。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929191653.png"></p>
<p>曲线拟合问题中只有一个优化变量，而带噪声的数据构成了诸多误差项。</p>
<p>步骤：</p>
<ol>
<li>定义顶点和边的类型</li>
<li>构件图</li>
<li>选择优化算法</li>
<li>调用g2o进行优化，返回结果</li>
</ol>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>V-SLAM整理2</title>
    <url>/V-SLAM%E6%95%B4%E7%90%862/</url>
    <content><![CDATA[<h1 id="第7讲-视觉里程计1"><a href="#第7讲-视觉里程计1" class="headerlink" title="第7讲 视觉里程计1"></a>第7讲 视觉里程计1</h1><p>视觉里程计，也就是前端，核心是根据相邻图像的信息估计相机运动，给后端提供很好的估计值。</p>
<p>主要算法：</p>
<ul>
<li>特征点法</li>
<li>直接法</li>
</ul>
<h2 id="7-1-特征点法"><a href="#7-1-特征点法" class="headerlink" title="7.1 特征点法"></a>7.1 特征点法</h2><p>主流方法。</p>
<p>我们需要知道如何提取、匹配图像特征点，然后估计两帧之间的相机运动和场景结构，从而实现一个两帧间的视觉里程计。</p>
<h3 id="7-1-1-特征点"><a href="#7-1-1-特征点" class="headerlink" title="7.1.1 特征点"></a>7.1.1 特征点</h3><p>所谓特征，就是图像里比较有代表性的点，这些点在相机小量运动之后也能保持稳定。例如角点、边缘、区块。在SLAM中，特征=路标。</p>
<ul>
<li>角点特征（早期）<ul>
<li>Harris角点、FAST角点、GFTT角点等</li>
</ul>
</li>
<li>局部图像特征<ul>
<li>SIFT、SURF、<strong>ORB</strong>等</li>
</ul>
</li>
</ul>
<p>特征点：关键点 + 描述子（提取特征点=提取关键点并计算描述子）</p>
<ul>
<li>关键点：特征点在图像中的位置（大小、方向、评分）</li>
<li>描述子：描述关键点周围像素信息的向量</li>
</ul>
<h3 id="7-1-2-ORB特征"><a href="#7-1-2-ORB特征" class="headerlink" title="7.1.2 ORB特征"></a>7.1.2 ORB特征</h3><p>ORB特征：在平移、旋转、缩放变换下表现良好，同时高效</p>
<ul>
<li>Oriented FAST关键点提取：找出图像中的角点，计算特征点的主方向</li>
<li>改进后的BRIEF描述子：对前一步提取出的特征点的周围图像区域进行描述</li>
</ul>
<h4 id="1-FAST-关键点"><a href="#1-FAST-关键点" class="headerlink" title="1) FAST 关键点"></a>1) FAST 关键点</h4><ol>
<li>FAST角点主要检测局部像素灰度变化明显的地方。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929223949.png"></p>
<p>缺点：</p>
<ul>
<li>重复性（相同特征点可以在不同的图像中找到）不强</li>
<li>分布不均</li>
<li>不具有方向信息</li>
<li>不具有尺度信息</li>
</ul>
<hr>
<ol start="2">
<li>ORB改进版——Oriented FAST：加上尺度和旋转的描述。</li>
</ol>
<ul>
<li>尺度：图像金字塔，每层检测角点</li>
<li>旋转：灰度质心法—get关键点的方向信息——几何中心指向质心</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210929224335.png"></p>
<h4 id="2-BRIEF-描述子"><a href="#2-BRIEF-描述子" class="headerlink" title="2) BRIEF 描述子"></a>2) BRIEF 描述子</h4><p>BRIEF：一种二进制描述子，用0和1来编码特征点附近两个随机像素的大小关系。</p>
<p>Steer BRIEF：ORB改进后的BRIEF，利用关键点的方向信息，计算旋转之后的Steer BRIEF特征。这样一来描述子也能具有旋转不变形。</p>
<ul>
<li>描述子距离表示了两个特征的相似程度，对于BRIEF描述子，可以使用汉明距离。</li>
</ul>
<h3 id="7-1-3-特征匹配"><a href="#7-1-3-特征匹配" class="headerlink" title="7.1.3 特征匹配"></a>7.1.3 特征匹配</h3><p>确定当前图像路标与之前图像中的路标之间的对应关系。</p>
<p>主要问题：场景中的重复纹理导致误匹配。</p>
<p>方法：</p>
<ul>
<li>暴力匹配<ul>
<li>计算每个特征点与其他特征点的描述子距离，选取最近的作为匹配点</li>
</ul>
</li>
<li>快速近似最近邻（FLANN）</li>
</ul>
<h2 id="7-2-编程：特征提取和匹配"><a href="#7-2-编程：特征提取和匹配" class="headerlink" title="7.2 编程：特征提取和匹配"></a>7.2 编程：特征提取和匹配</h2><p>opencv</p>
<h3 id="7-2-1-OpenCV的ORB特征"><a href="#7-2-1-OpenCV的ORB特征" class="headerlink" title="7.2.1 OpenCV的ORB特征"></a>7.2.1 OpenCV的ORB特征</h3><p>提取并匹配两种图像的ORB特征：</p>
<ol>
<li>读取图像</li>
<li>初始化</li>
<li>检测Oriented FAST角点位置</li>
<li>根据角点位置计算BRIEF描述子</li>
<li>对两张图像中的BRIEF描述子进行匹配，使用汉明距离</li>
<li>匹配点对筛选：汉明距离小于最小距离的两倍，否则算是误匹配</li>
<li>绘制匹配结果<h3 id="7-2-2-手写ORB特征"><a href="#7-2-2-手写ORB特征" class="headerlink" title="7.2.2 手写ORB特征"></a>7.2.2 手写ORB特征</h3>编译该程序要求CPU支持SSE指令集</li>
</ol>
<h3 id="7-2-3-计算相机运动"><a href="#7-2-3-计算相机运动" class="headerlink" title="7.2.3 计算相机运动"></a>7.2.3 计算相机运动</h3><p>已有<code>匹配好的点对</code>，接下来要根据点对来<code>估计相机运动</code>：</p>
<ol>
<li>单目：对极几何</li>
<li>双目/RGBD：ICP</li>
<li>一组3d一组2d：PnP</li>
</ol>
<p>需要注意的是，这里的匹配点对是来自<strong>两帧</strong>的，估计的相机运动也是<strong>两帧之间</strong>的相机运动。</p>
<h2 id="7-3-2D-2D：对极几何"><a href="#7-3-2D-2D：对极几何" class="headerlink" title="7.3 2D-2D：对极几何"></a>7.3 2D-2D：对极几何</h2><p>已知匹配的投影点对位置，求相机的位姿（R, t）。</p>
<h3 id="7-3-1-对级约束"><a href="#7-3-1-对级约束" class="headerlink" title="7.3.1 对级约束"></a>7.3.1 对级约束</h3><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930002342.png"></p>
<ul>
<li>l1, l2：极线</li>
<li>e1, e2：极点</li>
<li>O1O2P：极平面</li>
<li>O1O2：基线</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930002827.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930002919.png"></p>
<p>推导得到<code>对级约束</code>：其中x1，x2是像素点的归一化平面坐标</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930003051.png"></p>
<p>对级约束的几何意义是O1，P，O2三者共面。</p>
<p>进一步化简对级约束：</p>
<ul>
<li><code>基础矩阵F</code></li>
<li><code>本质矩阵E</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930003316.png"></p>
<ul>
<li>对级约束给出了两个匹配点的空间位置关系</li>
</ul>
<p><strong>相机位姿估计问题</strong>：</p>
<ul>
<li>根据匹配点的像素位置（p1, p2）求出<code>E</code>或者<code>F</code> （内参K已知情况下，主要用E）</li>
<li>根据E或者F求出<code>R,t</code></li>
</ul>
<h3 id="7-3-2-本质矩阵E"><a href="#7-3-2-本质矩阵E" class="headerlink" title="7.3.2 本质矩阵E"></a>7.3.2 本质矩阵E</h3><p>E=t^R</p>
<p>E具有尺度等价性，实际上由5个自由度。</p>
<p><code>八代法</code>估计E：8对匹配点组成矩阵，满足秩为8的条件，解方程得到E</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930005101.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930004840.png"></p>
<hr>
<p>根据E求R,t：<code>奇异分解SVD</code></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930005337.png"></p>
<p>得到4个可能解，只有一个解对应的P在两个相机中都具有正的深度，是正确解</p>
<h3 id="7-3-3-单应矩阵"><a href="#7-3-3-单应矩阵" class="headerlink" title="7.3.3 单应矩阵"></a>7.3.3 单应矩阵</h3><p>单应矩阵H：描述了两个平面之间的映射关系，通常用于描述处于共同平面上的一些点在两张图像之间的变换关系。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930010300.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930113354.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930113417.png"></p>
<p>1对匹配点构造两项约束，<code>4对匹配点</code>可以得到8项约束，计算出8个自由度的单应矩阵：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930010838.png"></p>
<p>解得H，分解（数值法、解析法）后得到R,t。</p>
<ul>
<li>单应性在SLAM中的应用：if 特征点共面/相机发生<code>纯旋转</code>，基础矩阵的自由度下降，即<code>退化</code>，这种时候如果还用8点法求解基础矩阵，多余的自由度将会主要由噪声决定。这种时候我们可以同时估计基础矩阵F和单应矩阵H，选择重投影误差较小的作为最终的运动估计矩阵。</li>
</ul>
<h2 id="7-4-编程：对级约束求解相机运动"><a href="#7-4-编程：对级约束求解相机运动" class="headerlink" title="7.4 编程：对级约束求解相机运动"></a>7.4 编程：对级约束求解相机运动</h2><p>opencv</p>
<p>单目初始化不能只有纯旋转，必须有一定平移。</p>
<p><code>随机采样一致性RANSAC</code>：当数据带有错误匹配时可以用</p>
<h2 id="7-5-三角测量"><a href="#7-5-三角测量" class="headerlink" title="7.5 三角测量"></a>7.5 三角测量</h2><p>通过对极几何得到相机的运动R,t之后，需要用这个运动估计特征点的空间位置。</p>
<p>但是在单目SLAM里，单张图片无法获取像素的深度信息，需要用<code>三角测量法</code>来估计地图点的深度。</p>
<p>也就是说，目前已知R,t，想求解两个特征点的深度s1和s2.</p>
<p>三角测量：在不同位置对同一个路标点（特征点）进行观察，从观察到的位置推断路标点的距离。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930014541.png"></p>
<p>本来两条直线的交点P就是空间点了，但是由于存在噪声，这两条直线往往无法相交，可以通过最小二乘来求解。</p>
<h2 id="7-6-编程：三角测量"><a href="#7-6-编程：三角测量" class="headerlink" title="7.6 编程：三角测量"></a>7.6 编程：三角测量</h2><h3 id="7-6-1-三角测量代码"><a href="#7-6-1-三角测量代码" class="headerlink" title="7.6.1 三角测量代码"></a>7.6.1 三角测量代码</h3><p>之前对极几何求解的<code>相机位姿</code> + <code>三角化</code>，得到特征点的空间位置。</p>
<h3 id="7-6-2-讨论"><a href="#7-6-2-讨论" class="headerlink" title="7.6.2 讨论"></a>7.6.2 讨论</h3><p>纯旋转不能用三角测量，故如果相机原地旋转导致视差很小，可能出现追踪失败、尺度不正确等问题。</p>
<p>视差问题：三角化存在一个矛盾点，增大平移，可能导致匹配失效，平移太小，导致三角化精度不够。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930015330.png"></p>
<p>延迟三角化。</p>
<h2 id="7-7-3D-2D：PnP"><a href="#7-7-3D-2D：PnP" class="headerlink" title="7.7 3D-2D：PnP"></a>7.7 3D-2D：PnP</h2><p>2D-2D对极几何缺点：</p>
<ul>
<li>需要8+点对<ul>
<li>如果3D-2D，只需要2+1点对</li>
</ul>
</li>
<li>初始化问题</li>
<li>纯旋转问题</li>
<li>尺度问题</li>
</ul>
<p>因此，双目/RGBD直接用PnP估计相机运动，单目先初始化再用PnP。</p>
<p>PnP：</p>
<ul>
<li>已知3D点的空间位置和它在相机上的投影点，求相机的外参（R, t）【大多数情况】</li>
<li>已知相机坐标A中的3D点位置，和相机B中的2D点位置，求两个相机的相对运动，（和2D特征点的空间位置）</li>
</ul>
<p>PnP求解方法：</p>
<ul>
<li>代数法<ul>
<li>P3P</li>
<li>直接线性变换DLT</li>
<li>EPnP</li>
<li>UPnP</li>
</ul>
</li>
<li>优化法<ul>
<li>非线性优化——BA</li>
</ul>
</li>
</ul>
<h3 id="7-7-1-直接线性变换DLT"><a href="#7-7-1-直接线性变换DLT" class="headerlink" title="7.7.1 直接线性变换DLT"></a>7.7.1 直接线性变换DLT</h3><p>已知空间点P坐标和它的投影点x1，估计相机位姿R, t</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930113119.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930113456.png"></p>
<p>1个特征点（或者说1对3D-2d匹配点）可以提供2个关于t的线性约束，<code>6对匹配点</code>提供12个约束，可以求解12个未知t。</p>
<p>求出来T矩阵，但是左边3X3的矩阵块不一定满足旋转矩阵R的约束，因此要用<code>QR分解</code>找一个最好的旋转矩阵来对它进行近似：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930114031.png"></p>
<h3 id="7-7-2-P3P"><a href="#7-7-2-P3P" class="headerlink" title="7.7.2 P3P"></a>7.7.2 P3P</h3><p>上面的DLT用了6对匹配点，这里的P3P只需要用4对，3个来算，1个验证。</p>
<p>已知3个3D点ABC（已知其世界坐标系下的3D坐标），和匹配的3个2D点abc，估计相机外参。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930114515.png"></p>
<p><strong>思路是利用几何性质，将3D-2D转换成3D-3D问题再求解：</strong></p>
<ul>
<li>转换</li>
</ul>
<p>已知：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930115352.png"></p>
<p>未知：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930115319.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930115227.png"></p>
<p>可以用<code>吴氏消元法</code>+<code>验证点</code>解得a,b,c在世界坐标系下的3D坐标【这里书上写的是ABC在相机坐标下的3D坐标，感觉不太对】，然后就变成了3D-3D问题。</p>
<ul>
<li>求解3D-3D：<code>ICP</code></li>
</ul>
<h3 id="7-7-3-最小化重投影误差求解PnP"><a href="#7-7-3-最小化重投影误差求解PnP" class="headerlink" title="7.7.3 最小化重投影误差求解PnP"></a>7.7.3 最小化重投影误差求解PnP</h3><ul>
<li>线性方法：先求相机位姿，再求空间点位置（如2D-2D对极几何+三角化）<ul>
<li>线性方法利用多个匹配点来拉平噪声造成的误差？</li>
<li><strong>【疑问】：线性方法有没有考虑误差的影响？？？</strong></li>
</ul>
</li>
<li>非线性方法：将相机位姿和空间点放在一起来优化——Bundle Adjustment<ul>
<li>非线性方法直接对实际和估计的误差进行最小化</li>
</ul>
</li>
</ul>
<p>前面也说过PnP的2D-3D来源可以是两种：相机在某个位姿下将空间的3D点投影到2D平面，这种情况下我们只要估计相机位姿就行；另一种跟2D-2D有点像，两个匹配点分别来自两帧，只是一个是3D的一个是2D的，这种时候除了估计相机位姿，还要估计2D点的空间位置。</p>
<hr>
<ul>
<li><strong>优化位姿</strong></li>
</ul>
<p>下面讲解如何在PnP中构建一个BA问题来对<code>相机位姿</code>进行优化：</p>
<p>已知空间点P及其投影p，目标是估计相机位姿R，t。由于相机位姿未知且存在噪声，su=KTP存在误差，构建最小二乘问题，寻找最好的相机位姿来最小化误差。该误差项是3D点的投影位置和观测的投影位置作差，故被称为<code>重投影误差</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930142440.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930123534.png"></p>
<p>高斯牛顿/阻尼牛顿来求解，重点是J（重投影误差关于<code>相机位姿对应的李代数</code>的一阶导数）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930135757.png"></p>
<ul>
<li>P在世界坐标系下：[X,Y,Z]^T</li>
<li>P在相机坐标系下：P’=(TP)=[X’, Y’, Z’]^T</li>
<li>P’的投影(u,v)：su=KP’</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930140705.png"></p>
<p>J 的求解可以参考”如何使用扰动模型来求李代数的导数”：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930140204.png"></p>
<p>第一项是误差关于变换后点的导数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930140248.png"></p>
<p>第二项是变换后的点关于李代数的导数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930141755.png"></p>
<p>最终得到<code>雅克比矩阵J</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930141046.png"></p>
<hr>
<ul>
<li><strong>优化特征点的空间关系</strong>：</li>
</ul>
<p>这里J是重投影误差关于<code>空间点P</code>的一阶导数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930141610.png"></p>
<p>第一项如上：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930140248.png"></p>
<p>第二项：P’=RP+t，求导后只剩下R</p>
<p>最终得到<code>雅克比矩阵J</code>：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930141714.png"></p>
<h2 id="7-8-编程：求解PnP"><a href="#7-8-编程：求解PnP" class="headerlink" title="7.8 编程：求解PnP"></a>7.8 编程：求解PnP</h2><h3 id="7-8-1-使用EPnP求解位姿"><a href="#7-8-1-使用EPnP求解位姿" class="headerlink" title="7.8.1 使用EPnP求解位姿"></a>7.8.1 使用EPnP求解位姿</h3><p>线性优化</p>
<p>OpenCV中的EPnP</p>
<h3 id="7-8-2-手写位姿估计"><a href="#7-8-2-手写位姿估计" class="headerlink" title="7.8.2 手写位姿估计"></a>7.8.2 手写位姿估计</h3><p>非线性优化</p>
<p>手写高斯牛顿法的PnP</p>
<h3 id="7-8-3-使用g2o进行BA优化"><a href="#7-8-3-使用g2o进行BA优化" class="headerlink" title="7.8.3 使用g2o进行BA优化"></a>7.8.3 使用g2o进行BA优化</h3><p>非线性优化</p>
<p>调用g2o</p>
<p>第一个相机的位姿固定为0，根据1组3D点和第2个图像中的2D投影来估计第二个相机位姿。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930142416.png"></p>
<h2 id="7-9-3D-3D：ICP"><a href="#7-9-3D-3D：ICP" class="headerlink" title="7.9 3D-3D：ICP"></a>7.9 3D-3D：ICP</h2><p>已知一组匹配好的3D点P,P’，求解P’——&gt;P的欧式变换R,t。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930142846.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930142856.png"></p>
<p>值得注意的是，3D-3D的位姿估计问题中没有相机模型，因为两组3D点变换与相机没有关系。但是，如果将这两组点集分别看做两个帧图像的数据，那么这里的R,t就是两帧间的相机位姿了。</p>
<p>迭代最近点ICP：</p>
<ul>
<li>激光SLAM中的ICP，两个点集之间并无匹配关系，因为激光数据的特征不丰富，因此只能认为距离最近的两个点为同一个点</li>
<li>VSALM中的ICP，两个点集是匹配好的</li>
</ul>
<p>ICP求解：</p>
<ul>
<li>线性法：SVD</li>
<li>非线性优化：BA</li>
</ul>
<h3 id="7-9-1-SVD方法"><a href="#7-9-1-SVD方法" class="headerlink" title="7.9.1 SVD方法"></a>7.9.1 SVD方法</h3><p>构建最小二乘问题：最小化点集中各对点的误差项之和</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930143528.png"></p>
<p>定义质心后，简化后的最小二乘问题：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930143733.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930143936.png"></p>
<p>思路是最小化第一项，取t让第二项为0。</p>
<p>步骤：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144310.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144105.png"></p>
<p>再化简这个优化目标函数，最终相当于要优化：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144358.png"></p>
<p>通过<code>SVD</code>可以解出上述问题中最优的R：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144516.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144534.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144547.png"></p>
<p>再求t就行。</p>
<h3 id="7-9-2-非线性优化方法"><a href="#7-9-2-非线性优化方法" class="headerlink" title="7.9.2 非线性优化方法"></a>7.9.2 非线性优化方法</h3><p>BA法以迭代的方式最小化目标函数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930144812.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210930145336.png"></p>
<p>然后用高斯牛顿等方法不断迭代找到极小值就行。</p>
<p>点集匹配时求解ICP问题有个优势，是可以任意选定初始值（因为唯一解情况下极小值就是全局最优解）。</p>
<p>但是事实上，如果特征已经匹配好了，往往使用线性方法直接解（因为存在解析解），而不是迭代。也就是说，ICP更多使用在匹配未知的情况下。</p>
<h2 id="7-10-编程：求解ICP"><a href="#7-10-编程：求解ICP" class="headerlink" title="7.10 编程：求解ICP"></a>7.10 编程：求解ICP</h2><h3 id="7-10-1-编程：SVD方法"><a href="#7-10-1-编程：SVD方法" class="headerlink" title="7.10.1 编程：SVD方法"></a>7.10.1 编程：SVD方法</h3><p>Eigen调用SVD解出目标函数的解析解。</p>
<h3 id="7-10-2-编程：非线性优化方法"><a href="#7-10-2-编程：非线性优化方法" class="headerlink" title="7.10.2 编程：非线性优化方法"></a>7.10.2 编程：非线性优化方法</h3><hr>
<h1 id="第8讲-视觉里程计2"><a href="#第8讲-视觉里程计2" class="headerlink" title="第8讲 视觉里程计2"></a>第8讲 视觉里程计2</h1><p>特征点法缺点：</p>
<ul>
<li>关键点的提取与描述子计算非常耗时</li>
<li>只使用特征点会丢失大部分也许有用的信息</li>
<li>特征缺少的场景下匹配点不够计算相机运动</li>
</ul>
<h2 id="8-1-直接法的引出"><a href="#8-1-直接法的引出" class="headerlink" title="8.1 直接法的引出"></a>8.1 直接法的引出</h2><p>改良方案：</p>
<ul>
<li>光流法：保留特征点，只计算关键点，不计算描述子，将匹配描述子的步骤替换成<code>光流跟踪</code>，即使用<code>光流法</code>跟踪特征点的运动。<ul>
<li>降低计算开销</li>
<li>只改了特征匹配的方案（改为跟踪），但还是用对极几何、PnP或ICP来估计相机运动</li>
</ul>
</li>
<li>直接法：不再使用特征点，只计算关键点，不计算描述子。同时使用<code>直接法</code>计算特征点在下一时刻图像中的位置。<ul>
<li>直接法不要求提取到的点是角点，只需要根据图像的<code>像素灰度信息</code>来估计相机运动和点的投影。</li>
<li>进一步降低计算开销</li>
<li>全改，不需要匹配特征，直接<code>最小化光度误差</code></li>
</ul>
</li>
</ul>
<p>光流法也算是特征点法的一种。</p>
<p>直接法是从光流法演变而来的。</p>
<p>可用于的结构：</p>
<ul>
<li>直接法：<ul>
<li>稠密</li>
<li>半稠密</li>
<li>稀疏</li>
</ul>
</li>
<li>特征点法：<ul>
<li>稀疏</li>
</ul>
</li>
</ul>
<h2 id="8-2-2D光流"><a href="#8-2-2D光流" class="headerlink" title="8.2 2D光流"></a>8.2 2D光流</h2><h3 id="1-Lucas-Kanade光流"><a href="#1-Lucas-Kanade光流" class="headerlink" title="1) Lucas-Kanade光流"></a>1) Lucas-Kanade光流</h3><h2 id="8-3-编程：LK光流"><a href="#8-3-编程：LK光流" class="headerlink" title="8.3 编程：LK光流"></a>8.3 编程：LK光流</h2><h3 id="8-3-1-使用LK光流"><a href="#8-3-1-使用LK光流" class="headerlink" title="8.3.1 使用LK光流"></a>8.3.1 使用LK光流</h3><h3 id="8-3-2-用高斯牛顿法实现光流"><a href="#8-3-2-用高斯牛顿法实现光流" class="headerlink" title="8.3.2 用高斯牛顿法实现光流"></a>8.3.2 用高斯牛顿法实现光流</h3><h3 id="8-3-3-光流实践小结"><a href="#8-3-3-光流实践小结" class="headerlink" title="8.3.3 光流实践小结"></a>8.3.3 光流实践小结</h3><h2 id="8-4-直接法"><a href="#8-4-直接法" class="headerlink" title="8.4 直接法"></a>8.4 直接法</h2><h3 id="8-4-1-直接法的推导"><a href="#8-4-1-直接法的推导" class="headerlink" title="8.4.1 直接法的推导"></a>8.4.1 直接法的推导</h3><h3 id="8-4-2-直接法的讨论"><a href="#8-4-2-直接法的讨论" class="headerlink" title="8.4.2 直接法的讨论"></a>8.4.2 直接法的讨论</h3><h2 id="8-5-编程：直接法"><a href="#8-5-编程：直接法" class="headerlink" title="8.5 编程：直接法"></a>8.5 编程：直接法</h2><h3 id="8-5-1-单层直接法"><a href="#8-5-1-单层直接法" class="headerlink" title="8.5.1 单层直接法"></a>8.5.1 单层直接法</h3><h3 id="8-5-2-多层直接法"><a href="#8-5-2-多层直接法" class="headerlink" title="8.5.2 多层直接法"></a>8.5.2 多层直接法</h3><h3 id="8-5-3-结果讨论"><a href="#8-5-3-结果讨论" class="headerlink" title="8.5.3 结果讨论"></a>8.5.3 结果讨论</h3><h3 id="8-5-4-直接法优缺点总结"><a href="#8-5-4-直接法优缺点总结" class="headerlink" title="8.5.4 直接法优缺点总结"></a>8.5.4 直接法优缺点总结</h3>]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>VSCode+Clang+MinGW+OpenGl配置教程</title>
    <url>/VSCode-Clang-MinGW-OpenGl%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这几天帮别人跑一个天空盒代码，需要用到OpenGl环境，配置过程掉坑比较多略为艰难，因此写个教程记录一下，免得以后换电脑配环境的时候又掉坑233。</p>
<p>顺便吐槽一下，这位仁兄电脑是联想核显，据说因为木有GPU，跑不了OpenGl程序，多好的反面教材<span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>~ 这里提醒诸君，搞计算机视觉的话，电脑还是独显比较好。</p>
<p>其实很久以前笔者在CodeBlocks上配置过OpenGl环境，当时用的是袁学长提供的傻瓜教程，私以为so easy……In fact，光是环境配置就要了老命，果然配置教程这种东西要及时记录！</p>
<p><strong>掉坑的核心：64位和32位一定要区分清楚，所有配置必须all32或all64，否则程序会出现<code>skipping imcompatible</code>的问题。</strong></p>
<p><strong>——以下配置都是64bit！</strong></p>
<p>老规矩，附上参考链接，向前辈致敬：</p>
<table>
<thead>
<tr>
<th>链接</th>
</tr>
</thead>
<tbody><tr>
<td>1. <a href="https://zhuanlan.zhihu.com/p/87864677">VSCode配置C/C++环境</a></td>
</tr>
<tr>
<td>2. <a href="https://anyway521.github.io/post/fb46ea0d.html">VSCode配置C/C++环境完整版(附OpenGL配置)</a></td>
</tr>
<tr>
<td>3. <a href="https://blog.csdn.net/qq_41523096/article/details/104628484">20秒 一键配置 VSCode (Visual Studio Code) C/C++开发环境</a></td>
</tr>
<tr>
<td>4. <a href="https://my.oschina.net/u/4277479/blog/3525117">VS Code + MinGW + Clang + OpenGL (vscode 配置 opengl环境)</a></td>
</tr>
</tbody></table>
<h1 id="VSCode配置C-C-环境"><a href="#VSCode配置C-C-环境" class="headerlink" title="VSCode配置C/C++环境"></a>VSCode配置C/C++环境</h1><p>首先，如果你只需要一个在VSCode上的C/C++环境，其他类似OpenGL、OpenCV等等都不需求，那么强烈推荐<a href="https://blog.csdn.net/qq_41523096/article/details/104628484">C参考链接3</a>，全自动傻瓜配置，非常nice。</p>
<p>另外，如果你还没有装VSCode或者想要卸载，推荐<a href="https://zhuanlan.zhihu.com/p/87864677">参考链接1</a>，里面有VSCode的安装教程，在此就不累述了。</p>
<p>那么，正文开始。</p>
<h2 id="1-安装Clang"><a href="#1-安装Clang" class="headerlink" title="1. 安装Clang"></a>1. 安装Clang</h2><p>根据维基百科的定义，Clang是一个编译器前端，采用LLVM（一款用来开发编译器前后端的软件）作为其后端。Clang作为GNU编译器套装（<code>GCC</code>）的替代品，支持GNU编译器的大多数编译设定。Clang支持C、C++、Objective-C和Objective-C++。简单来说，我们安装LLVM等同于安装配置Clang。</p>
<ul>
<li>下载地址：<a href="https://releases.llvm.org/download.html">LLVM</a></li>
</ul>
<p>以LLVM 11.1.0为例：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210427120902.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210427121320.png"></p>
<p>笔者的安装位置：<code>D:\Programfiles\LLVM</code>，安装过程中注意勾选：<code>添加环境变量</code></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210427121625.png"></p>
<h2 id="2-安装MinGM-W64"><a href="#2-安装MinGM-W64" class="headerlink" title="2. 安装MinGM-W64"></a>2. 安装MinGM-W64</h2><p>首先我们需要大概了解一下MinGM是个啥。前述的Clang作为GCC的替代品，和GCC一样也是主要用在Unix系统下的，我们想在Windows系统中使用Clang，便需要借助MinGM (Minimalist GNU for Windows)的力量。</p>
<p>MinGM简单来讲，就是Window上的GNU开发平台，等于说有了MinGM你不用装linux也可以在Windows上玩GNU。</p>
<ul>
<li>版本说明：<ul>
<li><p>MinGM（mingm32）—32位；</p>
</li>
<li><p>MinGM-w64—32+64位【我用的是这个】</p>
<p>下载地址：<a href="https://sourceforge.net/projects/mingw-w64/files/">MinGW-w64 - for 32 and 64 bit Window</a></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428111555.png"></p>
</li>
</ul>
</li>
<li><strong>注意：</strong> MinGW是在线安装，但是由于某种不可抗力，安装过程缓慢到不能忍受，因此我最后用的是离线安装的方法。</li>
</ul>
<h3 id="离线安装方法"><a href="#离线安装方法" class="headerlink" title="离线安装方法"></a>离线安装方法</h3><ol>
<li><p>下载X86_64-win32-seh：<br>下载地址：<a href="https://sourceforge.net/projects/mingw-w64/files/">MinGW-w64 - for 32 and 64 bit Windows</a></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428111823.png"></p>
</li>
<li><p>解压、合并文件：将解压后<code>mingw64</code>文件夹中的所有文件直接复制到之前的LLVM安装目录<code>D:\Programfiles\LLVM</code>，系统会自动整合同名文件夹中的文件。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428112733.png"></p>
</li>
<li><p>输入<code>clang -v</code>和<code>gcc -v</code>进行测试，出现版本号则安装成功。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428130609.png"></p>
<p>当前系统的环境变量为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428131225.png"></p>
</li>
</ol>
<h2 id="3-安装VSCode插件"><a href="#3-安装VSCode插件" class="headerlink" title="3. 安装VSCode插件"></a>3. 安装VSCode插件</h2><p>在VSCode左侧的<code>扩展</code>中搜索安装以下插件：</p>
<p><em>ps，在此之前，建议通过重定向将插件储存位置从C盘改出来:<br><code>"D:\Program Files (x86)\Microsoft VS Code\Code.exe" --extensions-dir "D:\Program Files (x86)\Microsoft VS Code\extensions"</code></em></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428132711.png"></p>
<ul>
<li><p>插件1：C/C++</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428132109.png"></p>
</li>
<li><p>插件2：C/C++ Clang Command Adapter</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428132325.png"></p>
</li>
<li><p>插件3：Code Runner</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210428132842.png"></p>
</li>
</ul>
<h1 id="VSCode配置Gult环境"><a href="#VSCode配置Gult环境" class="headerlink" title="VSCode配置Gult环境"></a>VSCode配置Gult环境</h1><p>Gult是OpenGl需要的相关依赖，包括<code>glut.h</code>，<code>glut32.lib</code>，<code>glut32.dll</code>文件。</p>
<p>值得注意的是，由于之前的C++环境是64位的，gult版本必须与之对应，即64位。</p>
<ol>
<li><p>下载<code>glut.h</code>，<code>glut32.lib</code>，<code>glut32.dll</code>：</p>
<p>下载地址：<a href="https://pan.baidu.com/s/1PN_0SxxKGJ-D_AZRDa4LDA">https://pan.baidu.com/s/1PN_0SxxKGJ-D_AZRDa4LDA</a> 提取码：sf1z </p>
</li>
<li><p>将gult文件放到相应目录下（LLVM安装路径）：</p>
</li>
</ol>
<ul>
<li><code>gult.h</code>放到目录：<code>D:\Programfiles\LLVM\include</code></li>
<li><code>gult32.dll</code>放到目录：<code>D:\Programfiles\LLVM\bin</code></li>
<li><code>gult32.lib</code>放到目录：<code>D:\Programfiles\LLVM\lib</code></li>
</ul>
<h1 id="VSCode配置文件"><a href="#VSCode配置文件" class="headerlink" title="VSCode配置文件"></a>VSCode配置文件</h1><p>在文件夹里新建<code>.vscode</code>，<code>include</code>和<code>lib</code>文件夹，<code>include</code>和<code>lib</code>现在先不用管，先专注<code>.vscode</code>文件夹，做VSCode下C/C++环境的基本配置。</p>
<p>在<code>.vscode</code>文件夹里新建<code>c_cpp_properties.json</code>，<code>launch.json</code>，<code>settings.json</code>以及<code>tasks.json</code>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210430102555.png"></p>
<h2 id="1-配置编译器：c-cpp-properties-json"><a href="#1-配置编译器：c-cpp-properties-json" class="headerlink" title="1. 配置编译器：c_cpp_properties.json"></a>1. 配置编译器：c_cpp_properties.json</h2><p>首先配置编译器路径。</p>
<h3 id="方法-1："><a href="#方法-1：" class="headerlink" title="方法 1："></a>方法 1：</h3><figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">    <span class="string">"configurations"</span>: [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"MinGW"</span>,</span><br><span class="line">            <span class="string">"intelliSenseMode"</span>: <span class="string">"clang-x64"</span>,</span><br><span class="line">            <span class="string">"compilerPath"</span>: <span class="string">"D:/Programfiles/LLVM/bin/g++.exe"</span>, <span class="comment">//之前安装的编译器的路径</span></span><br><span class="line">            <span class="string">"includePath"</span>: [</span><br><span class="line">                <span class="string">"${workspaceFolder}/**"</span>,</span><br><span class="line">                <span class="string">"${workspaceFolder}/include/*"</span>, <span class="comment">//当前文件夹下的include</span></span><br><span class="line">                <span class="string">"D:/Programfiles/LLVM/include/*"</span>, <span class="comment">//LLVM的include</span></span><br><span class="line">                <span class="string">"D:/Programfiles/LLVM/lib/gcc/x86_64-w64-mingw32/8.1.0/include/*"</span> <span class="comment">//mingw的include</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="string">"defines"</span>: [],</span><br><span class="line">            <span class="string">"browse"</span>: {</span><br><span class="line">                <span class="string">"path"</span>: [</span><br><span class="line">                    <span class="string">"${workspaceFolder}"</span></span><br><span class="line">                ],</span><br><span class="line">                <span class="string">"limitSymbolsToIncludedHeaders"</span>: <span class="literal">true</span>,</span><br><span class="line">                <span class="string">"databaseFilename"</span>: <span class="string">""</span></span><br><span class="line">            },</span><br><span class="line">            <span class="string">"cStandard"</span>: <span class="string">"c11"</span>,</span><br><span class="line">            <span class="string">"cppStandard"</span>: <span class="string">"c++17"</span></span><br><span class="line">        }</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"version"</span>: <span class="number">4</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="方法-2："><a href="#方法-2：" class="headerlink" title="方法 2："></a>方法 2：</h3><p><a href="https://zhuanlan.zhihu.com/p/87864677">参考链接1</a>中<code>配置编译器</code>部分给出了另一种图形化的配置方式。</p>
<p>简单来说就是不需要手动新建<code>.vscode</code>文件夹和<code>c_cpp_properties.json</code>文件，直接用<code>Ctrl+Shift+P</code>调出控制面板，输入<code>C/C++</code>选择<code>Edit Configurations(UI)</code>进入配置。</p>
<ul>
<li><p>修改编译器路径：D:/Programfiles/LLVM/bin/g++.exe</p>
</li>
<li><p>修改intelliSense：clang-x64</p>
</li>
</ul>
<p>配置完成后，自动新建了<code>.vscode</code>文件夹和<code>c_cpp_properties.json</code>文件，再在<code>c_cpp_properties</code>文件中修改include路径即可。</p>
<h2 id="2-配置构建任务tasks-json"><a href="#2-配置构建任务tasks-json" class="headerlink" title="2. 配置构建任务tasks.json"></a>2. 配置构建任务tasks.json</h2><p><code>tasks.json</code>可以告诉VSCode如何编译程序。</p>
<h3 id="方法-1：-1"><a href="#方法-1：-1" class="headerlink" title="方法 1："></a>方法 1：</h3><figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">{</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"2.0.0"</span>,</span><br><span class="line">    <span class="string">"tasks"</span>: [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"label"</span>: <span class="string">"Compile"</span>, <span class="comment">// 任务名称，与launch.json的preLaunchTask相对应</span></span><br><span class="line">            <span class="string">"command"</span>: <span class="string">"clang++"</span>,</span><br><span class="line">            <span class="string">"args"</span>: [ <span class="comment">//编译时的参数</span></span><br><span class="line">                <span class="string">"${file}"</span>,</span><br><span class="line">                <span class="string">"-o"</span>, <span class="comment">//指定输出文件名</span></span><br><span class="line">                <span class="string">"${fileDirname}/${fileBasenameNoExtension}.exe"</span>,</span><br><span class="line">                <span class="string">"-g"</span>, <span class="comment">//添加gdb调试选项</span></span><br><span class="line">                <span class="string">"-Wall"</span>,  <span class="comment">//开启额外警告</span></span><br><span class="line">                <span class="string">"-static-libgcc"</span>, <span class="comment">//静态链接libgcc</span></span><br><span class="line">                <span class="string">"--target=x86_64-w64-mingw"</span>,  <span class="comment">//clang编译器需要加上这条，因为它默认的target是msvc；如果用gcc或者linux要注释掉</span></span><br><span class="line">                <span class="string">"-std=c++17"</span>,</span><br><span class="line">                <span class="string">"-I${workspaceFolder}/include"</span>, <span class="comment">//添加工作路径下的include</span></span><br><span class="line">                <span class="string">"-L${workspaceFolder}/lib"</span>, <span class="comment">//添加工作路径下的lib</span></span><br><span class="line">                <span class="string">"-lglut32"</span>, <span class="comment">//使用glut</span></span><br><span class="line">                <span class="string">"-lglu32"</span>,  <span class="comment">//使用glut</span></span><br><span class="line">                <span class="string">"-lopengl32"</span>, <span class="comment">//使用opengl</span></span><br><span class="line">                <span class="string">"-lglad"</span>, <span class="comment">//使用glad+glfw，这里可以先注释掉</span></span><br><span class="line">                <span class="string">"-lglfw3"</span>,  <span class="comment">//使用glad+glfw，这里可以先注释掉 </span></span><br><span class="line">                <span class="string">"-lglfw3dll"</span>, <span class="comment">//使用glad+glfw，这里可以先注释掉</span></span><br><span class="line">                <span class="string">"-lgdi32"</span>,  <span class="comment">//使用glad+glfw，这里可以先注释掉</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"shell"</span>,</span><br><span class="line">            <span class="string">"group"</span>: {</span><br><span class="line">                <span class="string">"kind"</span>: <span class="string">"build"</span>,</span><br><span class="line">                <span class="string">"isDefault"</span>: <span class="literal">true</span> <span class="comment">//表示快捷键Ctrl+Shift+B可以运行该任务</span></span><br><span class="line">            },</span><br><span class="line">            <span class="string">"presentation"</span>: {</span><br><span class="line">                <span class="string">"echo"</span>: <span class="literal">true</span>,</span><br><span class="line">                <span class="string">"reveal"</span>: <span class="string">"always"</span>, <span class="comment">// 执行任务时是否跳转到终端面板</span></span><br><span class="line">                <span class="string">"focus"</span>: <span class="literal">false</span>,</span><br><span class="line">                <span class="string">"panel"</span>: <span class="string">"shared"</span> <span class="comment">// 不同的文件的编译信息共享一个终端面板</span></span><br><span class="line">            },</span><br><span class="line">            <span class="string">"problemMatcher"</span>: []</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h3 id="方法-2：-1"><a href="#方法-2：-1" class="headerlink" title="方法 2："></a>方法 2：</h3><p><a href="https://zhuanlan.zhihu.com/p/87864677">参考链接1</a>中<code>配置构建任务</code>部分给出了另一种图形化的配置方式。</p>
<p>不需要手动新建<code>tasks.json</code>文件，直接用<code>Ctrl+Shift+P</code>调出控制面板，输入<code>tasks</code>选择<code>Tasks:Configure Default Build Task</code>，自动生成tasks.json文件，主要修改其中的<code>"args"</code>参数部分。</p>
<h2 id="配置调试设置launch-json"><a href="#配置调试设置launch-json" class="headerlink" title="配置调试设置launch.json"></a>配置调试设置launch.json</h2><p><code>launch.json</code>用来配置调试的相关信息。</p>
<h3 id="方法-1：-2"><a href="#方法-1：-2" class="headerlink" title="方法 1："></a>方法 1：</h3><figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">    <span class="string">"version"</span>: <span class="string">"0.2.0"</span>,</span><br><span class="line">    <span class="string">"configurations"</span>: [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"(gdb) Launch"</span>, <span class="comment">// 配置名称，将会在启动配置的下拉菜单中显示</span></span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"cppdbg"</span>, <span class="comment">// 配置类型，cppdbg对应cpptools提供的调试功能；可以认为此处只能是cppdbg</span></span><br><span class="line">            <span class="string">"request"</span>: <span class="string">"launch"</span>, <span class="comment">// 请求配置类型，可以为launch（启动）或attach（附加）</span></span><br><span class="line">            <span class="string">"program"</span>: <span class="string">"${fileDirname}/${fileBasenameNoExtension}.exe"</span>, <span class="comment">// 将要进行调试的程序的路径</span></span><br><span class="line">            <span class="string">"args"</span>: [], <span class="comment">// 程序调试时传递给程序的命令行参数，一般设为空即可</span></span><br><span class="line">            <span class="string">"stopAtEntry"</span>: <span class="literal">false</span>, <span class="comment">// 设为true时程序将暂停在程序入口处，相当于在main上打断点</span></span><br><span class="line">            <span class="string">"cwd"</span>: <span class="string">"${workspaceFolder}"</span>, <span class="comment">// 调试程序时的工作目录，此为工作区文件夹；改成${fileDirname}可变为文件所在目录</span></span><br><span class="line">            <span class="string">"environment"</span>: [], <span class="comment">// 环境变量</span></span><br><span class="line">            <span class="string">"externalConsole"</span>: <span class="literal">false</span>, <span class="comment">// 为true时使用单独的cmd窗口，与其它IDE一致；18年10月后设为false可调用VSC内置终端</span></span><br><span class="line">            <span class="string">"internalConsoleOptions"</span>: <span class="string">"neverOpen"</span>, <span class="comment">// 如果不设为neverOpen，调试时会跳到“调试控制台”选项卡，你应该不需要对gdb手动输命令吧？</span></span><br><span class="line">            <span class="string">"MIMode"</span>: <span class="string">"gdb"</span>, <span class="comment">// 指定连接的调试器，可以为gdb或lldb。但我没试过lldb</span></span><br><span class="line">            <span class="string">"miDebuggerPath"</span>: <span class="string">"gdb.exe"</span>, <span class="comment">// 调试器路径，Windows下后缀不能省略，Linux下则不要</span></span><br><span class="line">            <span class="string">"setupCommands"</span>: [</span><br><span class="line">                { <span class="comment">// 模板自带，好像可以更好地显示STL容器的内容，具体作用自行Google</span></span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"Enable pretty-printing for gdb"</span>,</span><br><span class="line">                    <span class="string">"text"</span>: <span class="string">"-enable-pretty-printing"</span>,</span><br><span class="line">                    <span class="string">"ignoreFailures"</span>: <span class="literal">false</span></span><br><span class="line">                }</span><br><span class="line">            ],</span><br><span class="line">            <span class="string">"preLaunchTask"</span>: <span class="string">"Compile"</span> <span class="comment">// 调试会话开始前执行的任务，一般为编译程序。与tasks.json的label相对应</span></span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h3 id="方法-2：-2"><a href="#方法-2：-2" class="headerlink" title="方法 2："></a>方法 2：</h3><p>菜单栏<code>运行</code>-<code>启动调试</code>-<code>C++(GDB/LLDB)</code>，自动生成一个<code>launch.json</code>文件，再根据需要修改相关配置。</p>
<p>ps，不推荐该方法，原博主也遇到了没解决的问题，笔者也没深究，还是建议方法1。</p>
<h2 id="配置用户设置settings-json"><a href="#配置用户设置settings-json" class="headerlink" title="配置用户设置settings.json"></a>配置用户设置settings.json</h2><figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"files.defaultLanguage"</span>: <span class="string">"cpp"</span>, <span class="comment">// ctrl+N新建文件后默认的语言</span></span><br><span class="line">  <span class="string">"editor.formatOnType"</span>: <span class="literal">true</span>, <span class="comment">// 输入时就进行格式化，默认触发字符较少，分号可以触发</span></span><br><span class="line">  <span class="string">"editor.snippetSuggestions"</span>: <span class="string">"top"</span>, <span class="comment">// snippets代码优先显示补全</span></span><br><span class="line">  <span class="string">"code-runner.runInTerminal"</span>: <span class="literal">true</span>, <span class="comment">// 设置成false会在“输出”中输出，无法输入</span></span><br><span class="line">  <span class="string">"code-runner.executorMap"</span>: {</span><br><span class="line">      <span class="string">"c"</span>: <span class="string">"cd $dir &amp;&amp; clang $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -std=c11 &amp;&amp; $dir$fileNameWithoutExt"</span>,</span><br><span class="line">      <span class="string">"cpp"</span>: <span class="string">"cd $dir &amp;&amp; clang++ $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -lglut32 -lglu32 -lopengl32 -std=c++17 &amp;&amp; $dir$fileNameWithoutExt"</span> </span><br><span class="line">  }, <span class="comment">// 设置code runner的命令行</span></span><br><span class="line">  <span class="string">"code-runner.saveFileBeforeRun"</span>: <span class="literal">true</span>, <span class="comment">// run code前保存</span></span><br><span class="line">  <span class="string">"code-runner.preserveFocus"</span>: <span class="literal">true</span>, <span class="comment">// 若为false，run code后光标会聚焦到终端上。如果需要频繁输入数据可设为false</span></span><br><span class="line">  <span class="string">"code-runner.clearPreviousOutput"</span>: <span class="literal">false</span>, <span class="comment">// 每次run code前清空属于code runner的终端消息</span></span><br><span class="line"></span><br><span class="line">  <span class="string">"C_Cpp.clang_format_sortIncludes"</span>: <span class="literal">true</span>, <span class="comment">// 格式化时调整include的顺序（按字母排序）</span></span><br><span class="line">  <span class="string">"C_Cpp.intelliSenseEngine"</span>: <span class="string">"Default"</span>, <span class="comment">// 可以为Default或Tag Parser，后者较老，功能较简单。具体差别参考cpptools扩展文档</span></span><br><span class="line">  <span class="string">"C_Cpp.errorSquiggles"</span>: <span class="string">"Disabled"</span>, <span class="comment">// 因为有clang的lint，所以关掉</span></span><br><span class="line">  <span class="string">"C_Cpp.autocomplete"</span>: <span class="string">"Disabled"</span>, <span class="comment">// 因为有clang的补全，所以关掉</span></span><br><span class="line"></span><br><span class="line">  <span class="string">"clang.cflags"</span>: [ <span class="comment">// 控制c语言静态检测的参数</span></span><br><span class="line">      <span class="string">"--target=x86_64-w64-mingw"</span>,</span><br><span class="line">      <span class="string">"-std=c11"</span>,</span><br><span class="line">      <span class="string">"-Wall"</span>,</span><br><span class="line">      <span class="string">"-I./include"</span>,</span><br><span class="line">      <span class="string">"-L./lib"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"clang.cxxflags"</span>: [ <span class="comment">// 控制c++静态检测时的参数</span></span><br><span class="line">      <span class="string">"--target=x86_64-w64-mingw"</span>,</span><br><span class="line">      <span class="string">"-std=c++17"</span>,</span><br><span class="line">      <span class="string">"-Wall"</span>,</span><br><span class="line">      <span class="string">"-I./include"</span>,</span><br><span class="line">      <span class="string">"-L./lib"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"clang.completion.enable"</span>:<span class="literal">true</span>, <span class="comment">// 效果效果比cpptools要好</span></span><br><span class="line">  <span class="string">"clang.diagnostic.delay"</span>:<span class="number">300</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p><strong>值得注意的是</strong>，之前我run code一直报错找不到一些glut函数定义，只有debug能跑。后来发现问题出在<code>code-runner.executorMap</code>这里。</p>
<p>之前<code>tasks.json</code>中调试参数部分为：<br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210430124111.png"></p>
<p>而<code>settings.json</code>的<code>code-runner.executorMap</code>部分需要与之对应：<br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210430124608.png"></p>
<p>加上红框部分就能run code了。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><p>至此VSCode配置结束，之后跑新的C++项目只需要将<code>.vscode</code>文件夹复制过去就行，如果要用到其他库，只需要对其中一些参数稍作修改。</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>最后，我们建个main.cpp来测试一下：</p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;glut.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">display</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span>{</span><br><span class="line">    <span class="built_in">glutInit</span>(&amp;argc, argv);</span><br><span class="line">    <span class="built_in">glutInitDisplayMode</span>(GLUT_RGB | GLUT_SINGLE);</span><br><span class="line">    <span class="built_in">glutInitWindowPosition</span>(<span class="number">800</span>, <span class="number">150</span>);</span><br><span class="line">    <span class="built_in">glutInitWindowSize</span>(<span class="number">600</span>, <span class="number">400</span>);</span><br><span class="line">    <span class="built_in">glutCreateWindow</span>(<span class="string">"OpenGL 3D View"</span>);</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    <span class="built_in">glutDisplayFunc</span>(display);</span><br><span class="line">    <span class="built_in">glutMainLoop</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>{</span><br><span class="line">    <span class="built_in">glClearColor</span>(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    <span class="built_in">glMatrixMode</span>(GL_PROJECTION);</span><br><span class="line">    <span class="built_in">glOrtho</span>(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">-5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">15</span>);</span><br><span class="line">    <span class="built_in">glMatrixMode</span>(GL_MODELVIEW);</span><br><span class="line">    <span class="built_in">gluLookAt</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">display</span><span class="params">()</span></span>{</span><br><span class="line">    <span class="built_in">glClear</span>(GL_COLOR_BUFFER_BIT);</span><br><span class="line">    <span class="built_in">glColor3f</span>(<span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">glutWireTeapot</span>(<span class="number">3</span>);</span><br><span class="line">    <span class="built_in">glFlush</span>();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>run code：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210430121933.png"></p>
<p>大功告成！<span class="github-emoji"><span>☀</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2600.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>🐱</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f431.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>~</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>emm，没啥好说的啦，干的不错喵喵喵~</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>OpenGl</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：ORB-SALM2</title>
    <url>/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AORB-SALM2/</url>
    <content><![CDATA[<h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h1><p>ORB-SLAM2：一个适用于单目、双目、RGB-D相机的完整SLAM系统。</p>
<ul>
<li>系统包括地图重利用、回环检测和重定位功能</li>
<li>该系统在标准CPUs上实时工作，适用于多种环境：室内小型手持设备、工业环境中的无人机、城市中行驶的车辆。</li>
<li>该系统后端采用基于单目和双目观测的BA，允许metric尺度下对轨迹的精确估计。</li>
<li>该系统包括一个轻量级的定位模式，该模式可以在零点漂移(zero-drift)的情况下，利用视觉里程计来追踪未建图的区域并匹配地图点。</li>
</ul>
<p>实验方面：<br>在29个公开序列上对ORB-SLAM2进行评估，该系统实现了SOTA的准确性，并且在多数情况下是最准确的SLAM方案。</p>
<p>代码已公开。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>在过去20年间，CV和机器人领域中，SLAM算是热门方向，近年更是吸引了不少高科技公司的关注。<br>SLAM技术旨在建立一个未知环境的地图，并且在地图中地位传感器，其重点在于实时操作。<br>在众多传感器中，相机相对廉价且能提供丰富的环境信息。相机传感器允许鲁邦且准确的位置识别(place recoginition)。因此，在VSLAM中，主流方案还是使用相机传感器。<br>位置识别是SLAM系统中回环检测部分的关键模块，此外，位置识别对于跟踪失败（由于遮挡<code>occlusion</code>或者剧烈运动或者系统重新初始化）后相机的重定位也很关键。</p>
<p>VSLAM可以只用单目相机来完成，这是一种最廉价且最小型的传感器设置。然而，单目存在一些问题：</p>
<ol>
<li>由于仅用一台相机不能观测深度，地图和估计轨迹的尺度是未知的。</li>
<li>由于不能从第一帧中进行三角化测量，单目SLAM系统的启动(bootstrapping)需要多视角或<code>滤波技术</code>【？】才能生成初始地图。</li>
<li>单目SLAM可能出现<a href="https://www.zhihu.com/question/301977205">尺度漂移</a></li>
<li>如果探索过程中出现纯旋转可能会失败。</li>
</ol>
<p>使用双目/RGBD相机可以解决上述问题。</p>
<hr>
<p>在这篇论文中，我们基于单目ORB-SLAM[1]提出ORB-SLAM2，主要贡献如下：</p>
<ul>
<li>首个关于单目、双目和RGB-D相机的开源SLAM系统，系统包括回环检测、重定位和地图重用。</li>
<li>RGB-D结果显示，BA比ICP或者<code>光度(photometric)和深度误差最小化</code>更准确。</li>
<li>ORB-SLAM2使用远近双目点和单目观测，双目结果比SOTA中直接使用双目的SLAM系统更准确。【？】</li>
<li>在无法建图的情况下，ORB-SLAM2提供一个轻量级的定位模式，实现地图的高效重用。</li>
</ul>
<hr>
<p>下图分别是输入为双目和RGB-D时，ORB-SLAM2的输出结果：</p>
<ul>
<li><p>双目：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211023100214.png"></p>
<p>这是<code>KITTI数据集</code>[2]序列<code>00</code>的最终轨迹和稀疏重建。该序列是一个有多个回环的城市数据集，从结果看，ORB-SLAM2成功检测到了回环。</p>
</li>
<li><p>RGB-D：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211023100327.png"></p>
<p>这是<code>TUM RGB-D数据集</code>[3]序列<code>fr1_room</code>的关键帧位姿估计和一个稠密点云图，该稠密点云图是通过从估计的关键帧位姿反映射(backproject)到传感器深度地图来构建的。【？】</p>
<p>ORB-SLAM2没有做KinectFusion[4]一类的数据融合处理【？】，但是关键帧位姿的估计还是很准确的。</p>
</li>
<li><p>其他范例可以参考附录视频</p>
</li>
</ul>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><h2 id="2-1-Stereo-SLAM"><a href="#2-1-Stereo-SLAM" class="headerlink" title="2.1 Stereo SLAM"></a>2.1 Stereo SLAM</h2><h2 id="2-2-RGB-D-SLAM"><a href="#2-2-RGB-D-SLAM" class="headerlink" title="2.2 RGB-D SLAM"></a>2.2 RGB-D SLAM</h2><h1 id="3-ORB-SLAM2"><a href="#3-ORB-SLAM2" class="headerlink" title="3. ORB-SLAM2"></a>3. ORB-SLAM2</h1><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211024205807.png"></p>
<ul>
<li><p>系统包括三个主线程和一个小线程：</p>
<ul>
<li>跟踪：通过与局部地图进行特征匹配和用<code>motion-only BA</code>来最小化重投影误差，来跟踪每一帧对应的相机位置</li>
<li>局部建图：通过执行<code>局部BA</code>，来建立和优化局部地图</li>
<li>回环检测：通过执行<code>位姿图优化</code>，来检测大回环并且矫正累积漂移</li>
<li><code>Full BA</code>（小线程）:在位姿图优化后进入第四个线程，该线程执行<code>full BA</code>，来计算出最优的结构和运动。</li>
</ul>
</li>
<li><p>系统还嵌入了一个<code>Place Recognition模块</code></p>
<ul>
<li><code>Place Recognition模块</code>基于DBoW2[16]</li>
<li>用途：<ul>
<li>在跟踪失败(例如遮挡<code>occlusion</code>)情况下重定位</li>
<li>在已建图场景中重新初始化</li>
<li>回环检测</li>
</ul>
</li>
</ul>
</li>
<li><p>该系统维护一个<code>共视图</code>(covisibility graph)[8]和一个<code>最小生成树</code>(minimum spanning tree)</p>
<ul>
<li>共视图：连接任意两个共同观测点的关键帧</li>
<li>最小生成树：连接所有关键帧</li>
<li>这些图结构能够得到一个关键帧的局部窗口，便于在局部范围内跟踪和建立局部地图。图结构使系统可以工作在大规模环境中，并在回环检测部分作为图优化的一种结构。</li>
</ul>
</li>
<li><p>该系统在<code>跟踪</code>、<code>建图</code>和<code>place recognition</code>中使用<code>ORB特征</code>[17]</p>
<ul>
<li>ORB特征优点：<ul>
<li>对于旋转和尺度变换具有鲁棒性</li>
<li>对于相机<code>自增益auto-gain</code>、<code>自曝光auto-exposure</code>、<code>光度变化</code>具有良好的稳定性</li>
<li>能快速进行特征提取和特征匹配<ul>
<li>有助于实时性操作</li>
<li>在<code>词袋place recognition</code>中表现出良好的精确率和召回率[18]</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3-1-单目、近双目、远双目关键点"><a href="#3-1-单目、近双目、远双目关键点" class="headerlink" title="3.1 单目、近双目、远双目关键点"></a>3.1 单目、近双目、远双目关键点</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211024221656.png"></p>
<p>ORB-SLAM2是一种基于特征的方法，对输入进行预处理后，再提取显著关键点位置特征。</p>
<p>然后，丢弃输入图像，之后所有的系统操作都是基于这些提取的特征，因此，ORB-SLAM2系统独立于传感器（stereo、RGB-D）。</p>
<p>系统对单目和双目的关键点进行处理，这些关键点分为近点和远点两类。</p>
<p><a href="http://zhaoxuhui.top/blog/2018/12/14/SLAMCamera.html">SLAM相机成像原理</a></p>
<h3 id="3-1-1-双目关键点"><a href="#3-1-1-双目关键点" class="headerlink" title="3.1.1 双目关键点"></a>3.1.1 双目关键点</h3><p>双目关键点定义为$X_s=(u_L,v_L,u_R)$，其中$(u_L,v_L)$是<code>左图</code>的坐标，$u_R$是<code>右图</code>的<em>水平</em>坐标。</p>
<ul>
<li><p>对于双目相机：我们对每对图像提取ORB特征，在右图中寻找左图的ORB匹配。这对于双目图像矫正(stereo rectified images)【？】很有效，因此极线(epipolar lines)是<em>水平</em>的。</p>
<p>然后，使用 左ORB的坐标 和 右匹配的水平坐标 生成双目关键点，通过<code>修补相关性(patch correlation)</code>来细化亚像素<code>subpixel</code>。【？】</p>
</li>
<li><p>对于RGB-D相机：我们在RGB图像上提取ORB特征，如[8]所述，对于每个坐标为$(u_L,v_L)$的特征，我们将深度d转换成虚右坐标(virtual right coordinate)（对标双目里的$u_R$）：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025013701.png"></p>
<p>其中$f_x$是水平焦距，b是结构光投影仪(structured light projector)和红外摄像机之间的基线。<code>Kinect</code>和<code>Asus Xtion</code>的基线大约8cm。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025021252.png"></p>
<p>深度传感器的不确定性可以用虚右坐标的不确定性来表示。这样一来，双目的输入特征和RGB-D的输入特征在系统其余部分都可以等效处理。</p>
</li>
</ul>
<p>双目关键点分为近点和远点两类：</p>
<ul>
<li>近点：深度小于40倍的双目/RGB-D基线[5]<ul>
<li>这是因为近点的深度估计是准确的，且提供了尺度、平移和旋转信息。</li>
<li>因此，可以从<strong>单独一帧</strong>中安全的三角化。</li>
</ul>
</li>
<li>远点：其他<ul>
<li>远点提供了准确的旋转信息，但是尺度信息和平移信息比较弱。</li>
<li>只有提供<strong>多视角</strong>的情况下，才能对远点进行三角化。</li>
</ul>
</li>
</ul>
<h3 id="3-1-2-单目关键点"><a href="#3-1-2-单目关键点" class="headerlink" title="3.1.2 单目关键点"></a>3.1.2 单目关键点</h3><p>单目关键点定义为$x_m=(u_L,v_L)$，对应找不到双目匹配、以及RGB-D深度无效的情况。</p>
<ul>
<li>这些关键点没有尺度信息</li>
<li>只能从<strong>多视角</strong>中三角化</li>
<li>有助于旋转估计和平移估计【？】</li>
</ul>
<h2 id="3-2-系统启动"><a href="#3-2-系统启动" class="headerlink" title="3.2 系统启动"></a>3.2 系统启动</h2><p>使用双目/RGBD相机的一个最主要的好处是可以从单独一帧获取深度信息，不需要像单目那样从运动初始化中获得特定的结构。</p>
<p>在系统启动时，我们将第一帧创建为关键帧，将其位姿设置为原点，从所有双目关键点中创建初始化地图。</p>
<h2 id="3-3-单目和双目约束的BA"><a href="#3-3-单目和双目约束的BA" class="headerlink" title="3.3 单目和双目约束的BA"></a>3.3 单目和双目约束的BA</h2><ul>
<li>在跟踪线程中，系统执行<code>motion-only BA</code>来优化相机位姿。</li>
<li>在局部建图线程中，系统执行<code>local BA</code>来优化关键帧和点的局部窗口。</li>
<li>在回环检测后的第四线程中，系统执行<code>full BA</code>来优化所有关键帧和点。</li>
</ul>
<p>以上所有BA都使用<code>Levenberg-Marquardt梯度法</code>，该方法基于<code>g2o</code>库[19]实现。</p>
<h3 id="3-3-1-Motion-only-BA"><a href="#3-3-1-Motion-only-BA" class="headerlink" title="3.3.1 Motion-only BA"></a>3.3.1 Motion-only BA</h3><p>优化相机位姿，即优化相机方向$R（SO(3)）$和位置$t（R^3）$。</p>
<p>方法：最小化匹配的3D点$X^i（R^3）$的世界坐标和关键点$x_(.)^i$之间的重投影误差：</p>
<p>（单目：$x_m^i（R^2）$；双目：$x_s^i（R^3）$；i属于匹配集）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025102348.png"></p>
<h3 id="3-3-2-Local-BA"><a href="#3-3-2-Local-BA" class="headerlink" title="3.3.2 Local BA"></a>3.3.2 Local BA</h3><p>优化一组共视关键帧$K_L$以及这些关键帧观测到的所有点$P_L$。</p>
<p>其他观测到这些点但是不在待优化关键帧组$K_L$中的关键帧$K_F$，在优化过程中固定不变（即这些关键帧不被优化），但是参与优化方程。</p>
<p>$X_k$表示<code>关键帧k中的关键点</code>与<code>点集$P_L$中的点</code>相匹配的点的集合。</p>
<p>目标函数：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025103106.png"></p>
<h3 id="3-3-3-Full-BA"><a href="#3-3-3-Full-BA" class="headerlink" title="3.3.3 Full BA"></a>3.3.3 Full BA</h3><p><code>Full BA</code>优化地图中所有的关键帧和点。</p>
<ul>
<li><code>Full BA</code>实际上只是<code>local BA</code>的一种特殊情况。</li>
<li>原始关键帧固定（不被优化），这是为了消除<code>gauge freedom</code>【？】</li>
</ul>
<h2 id="3-4-回环检测和全BA"><a href="#3-4-回环检测和全BA" class="headerlink" title="3.4 回环检测和全BA"></a>3.4 回环检测和全BA</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025103823.png"></p>
<p>回环检测包括两个步骤：</p>
<ol>
<li>检测并验证回环</li>
<li>通过优化位姿图来矫正回环</li>
</ol>
<hr>
<p>ORB-SLAM2不用管单目ORB-SLAM中的<code>尺度漂移</code>[20]：</p>
<ul>
<li>这是因为双目/深度信息可以直接获得，这使得尺度是可以被观测的</li>
<li>ORB-SLAM2中，<code>几何验证</code>和<code>位姿图优化</code>时不再需要处理尺度漂移【？】<ul>
<li>ORB-SLAM中，这两个部分是基于相似性的</li>
<li>ORB-SLAM2中，这两个部分是基于刚体变换的</li>
</ul>
</li>
</ul>
<hr>
<p>在ORB-SLAM2中，我们在位姿图优化后加了一个<code>full BA优化</code>来实现方案最优化。这个优化过程开销比较大，我们为它开辟了一个单独的线程，因此系统可以继续创建地图、检测回环。</p>
<p>但这也带来了一个挑战：如何将BA的输出与当前地图相融合。</p>
<ul>
<li><p>如果检测到新的回环，而full BA优化正在进行中，就中断优化先执行闭环，然后再启动优化。</p>
</li>
<li><p>当full BA优化结束后，需要将<code>BA优化后的更新版关键帧和点</code>与<code>优化允许时插入的未更新版关键帧和点</code>相融合。</p>
<ul>
<li>通过生成树，我们将<code>更新版关键帧</code>的修正（即未优化位姿到优化位姿的转换）传播给<code>未更新版关键帧</code>。</li>
<li>根据对参考关键帧的修正来转换<code>未更新版点</code>。【？】</li>
</ul>
</li>
</ul>
<h2 id="3-5-关键帧插入"><a href="#3-5-关键帧插入" class="headerlink" title="3.5 关键帧插入"></a>3.5 关键帧插入</h2><p>ORB-SLAM2延续了ORB-SLAM中的策略：频繁插入关键帧，严格剔除冗余关键帧。</p>
<p>双目远点和近点的区别允许我们在关键帧插入环节引入新的条件。在<code>大部分场景距离双目传感器较远（远点较多）</code>的情况下，该条件起到关键作用。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211025112744.png"></p>
<p>在该场景下，我们需要足够数量的近点来准确估计位姿转移，因此，如果跟踪到的近点数量在$\tau_t$以下，且该帧能够创建至少$\tau_c$个新的近点，系统就会插入新的关键帧。</p>
<ul>
<li>实验值：$\tau_t=100$，$\tau_c=70$</li>
</ul>
<h2 id="3-6-定位模式"><a href="#3-6-定位模式" class="headerlink" title="3.6 定位模式"></a>3.6 定位模式</h2><p>系统包括一个<code>定位模式</code>，该模块有助于在已有的良好建图区域中进行长期的轻量级定位，前提是环境没有发生太大变化。</p>
<p>在该模式下：</p>
<ul>
<li>局部建图线程和回环检测线程停用，而相机始终在通过跟踪进行重定位（如果需要）</li>
<li>跟踪线程利用<code>visual odometry matches（VO匹配）</code>来匹配地图点。<ul>
<li><code>VO匹配</code>将当前帧的ORB特征与从历史帧中根据双目/深度信息创建的3D点（地图点？）相匹配</li>
<li>这些匹配使得定位在未建图区域中具有鲁棒性，但是依然存在<code>累积漂移</code>【？】</li>
<li>地图点匹配使得系统可以在已有地图上进行零漂移定位</li>
</ul>
</li>
</ul>
<p>随附该模式的演示视频。</p>
<h1 id="4-评估"><a href="#4-评估" class="headerlink" title="4. 评估"></a>4. 评估</h1><p>环境：Intel Core i7-4790 + 16G RAM</p>
<p>鉴于多线程系统存在不确定性，每个数据序列允许5次，取中值作为估计轨迹的精度。</p>
<h2 id="4-1-KITTI数据集"><a href="#4-1-KITTI数据集" class="headerlink" title="4.1 KITTI数据集"></a>4.1 KITTI数据集</h2><p>KITTI数据集[2]：</p>
<ul>
<li>城市和公路环境中汽车采集的双目序列。</li>
<li>00, 02, 05, 06, 07, 09 序列中存在回环</li>
<li>ORB-SLAM2未成功检测到序列09的回环，原因是回环位于序列末尾很少的帧中</li>
</ul>
<p>双目传感器：</p>
<ul>
<li>基线：~54cm</li>
<li>工作频率：10Hz</li>
<li>修正后的分辨率：1240x376</li>
</ul>
<hr>
<ul>
<li><p>表1显示11个训练序列的结果，这些序列都有公开的<code>ground-truth</code>；与SOTA的<code>双目LSD-SLAM</code>[11]进行比较：</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026192600.png"></p>
<ul>
<li>度量(metrics)：<ul>
<li><code>绝对 translation RMSE</code> $t_abs$[3]</li>
<li><code>平均相对 translation误差</code> $t_rel$[2]</li>
<li><code>平均相对 rotation误差</code> $r_rel$[2]</li>
</ul>
</li>
</ul>
<p>我们的系统在大多数序列中的表现都超过了<code>双目LSD-SLAM</code>，总体上相对误差低于1%。</p>
</li>
<li><p>序列01：训练集中唯一的公路序列，translation误差略差。</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026193256.png"></p>
<p>由于高速度和低帧率，该序列中能跟踪到的近点比较少，因此<code>translation</code>很难估计。</p>
<p>不过，许多远点可以长时间被跟踪到，因此<code>orientation</code>能够被准确估计，每100m误差0.21度。</p>
</li>
<li><p>轨迹估计范例：</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026193953.png"></p>
</li>
</ul>
<hr>
<p>与ORB-SALM的结果相比，双目系统能够处理单目系统处理不了的序列01。【？KITTI不是双目序列吗】</p>
<p>在该序列中，近点仅在少数帧中可见，双目系统可以从单独一个双目关键帧中创建地图点，而单目需要在两个关键帧中寻找匹配，故存在初始化延迟。这点是不跟丢的关键。</p>
<p>此外，双目系统以<code>metric scale</code>估计地图和轨迹，能够估计出地图和轨迹的真实尺度，没有<code>尺度漂移</code>现象。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026194827.png"></p>
<h2 id="4-2-EuRoC数据集"><a href="#4-2-EuRoC数据集" class="headerlink" title="4.2 EuRoC数据集"></a>4.2 EuRoC数据集</h2><p>EuRoC数据集[21]：</p>
<ul>
<li>包含11个双目序列，数据来源是在两个不同房间和一个大型工业环境中飞行的微型飞行器(MAV)。</li>
<li>序列分类：easy, meduim, difficult（取决于MAV的速度、光照、场景纹理）</li>
</ul>
<p>双目传感器：</p>
<ul>
<li>基线：~11cm</li>
<li>频率：20Hz</li>
<li>分辨率：WVGA（854×480或800×480）</li>
</ul>
<hr>
<p>跑序列等于MAV重新访问环境，ORB-SLAM2能够实现地图重用、并在必要时闭环。</p>
<ul>
<li><p>表2：</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026195708.png"></p>
<ul>
<li><p>ORB-SLAM2的定位精度为几厘米，比双目LSD-SLAM要高。</p>
</li>
<li><p>由于存在严重的<code>motion blur</code>，系统在<code>V2_03_difficult</code>序列中的部分区域跟丢了。根据[22]所述，该序列可以使用<code>IMU信息</code>进行处理。</p>
</li>
</ul>
</li>
<li><p>图6：轨迹估计范例</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026200246.png"></p>
</li>
</ul>
<h2 id="4-3-TUM-RGB-D数据集"><a href="#4-3-TUM-RGB-D数据集" class="headerlink" title="4.3 TUM RGB-D数据集"></a>4.3 TUM RGB-D数据集</h2><p>TUM RGB-D数据集[3]：</p>
<ul>
<li>包含RGB-D传感器采集的室内序列。</li>
<li>序列分为几种类别，用于评估不同纹理、光照、结构条件下，目标重建和SLAM/视觉里程计方法。</li>
</ul>
<hr>
<p>选择了部分RGB-D方法常用的序列来进行评估。</p>
<ul>
<li><p>表3：对比了ElasticFusion [15], Kintinuous [12], DVO-SLAM [14] and RGB-D SLAM [13]。</p>
<ul>
<li>我们的ORB-SLAM2是唯一一个基于BA的方法（其他都是基于直接法/ICP），且在大多数序列中表现最佳。</li>
</ul>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026200930.png"></p>
<p>我们在ORB-SLAM[1]已经注意到：<code>freiburg2序列</code>存在4%的尺度偏差(scale bias)，该误差可能来源于校准错误(miscalibration)。我们在系统运行中对该偏差进行了补偿。【how？】</p>
</li>
<li><p>图7：从4个序列估计的关键帧位姿中反投影出传感器深度图得到的点云</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026201636.png"></p>
<p>桌子和海报轮廓清晰笔直，证明了ORB-SLAM2的高精度定位。</p>
</li>
</ul>
<h2 id="4-4-Timing-Results"><a href="#4-4-Timing-Results" class="headerlink" title="4.4 Timing Results"></a>4.4 Timing Results</h2><ul>
<li><p>表4：不同图像分辨率和不同传感器的三个序列的计时结果，显示了每个线程的平均值和两个标准差范围。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/img20211026202149.png"></p>
<p>由于这些序列都是单环的，<code>Full BA</code>线程以及<code>回环检测线程</code>中的一些任务只执行一次，因此只计算一次运行时间。</p>
<ul>
<li><p>跟踪</p>
<p><strong>平均每帧的跟踪时间小于每个序列的相机帧率的倒数，这意味着我们的系统能够实时工作。</strong></p>
<p>  由于双目图像中ORB特征提取是并行进行的，可以看到从V2_02序列的双目WVGA图像中提取1000个ORB特征约等于从fr3_office序列的single VGA image channel中提取1000个特征。</p>
</li>
<li><p>回环检测</p>
<p>回环中关键帧的数量可以作为回环检测用时的参考：</p>
<ul>
<li><p>虽然KITTI 07序列中包含更多的关键帧，但是室内fr3_office序列的共视图更稠密，因此该序列的回环融合、姿态图优化、full BA任务都更耗时。</p>
</li>
<li><p>共视图越稠密，局部地图包含的关键帧和点的数量就越多，因此局部图跟踪和局部BA也会更加耗时。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><h1 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h1><p>[1] R. Mur-Artal, J. M. M. Montiel, and J. D. Tard´os, “ORB-SLAM: a versatile and accurate monocular SLAM system,” IEEE Trans. Robot., vol. 31, no. 5, pp. 1147–1163, 2015.</p>
<p>[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics: The KITTI dataset,” Int. J. Robot. Res., vol. 32, no. 11, pp. 1231–1237, 2013.</p>
<p>[3] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark for the evaluation of RGB-D SLAM systems,” in IEEE/RSJ Int. Conf. Intell. Robots and Syst. (IROS), 2012, pp. 573–580.</p>
<p>[4] R. A. Newcombe, A. J. Davison, S. Izadi, P. Kohli, O. Hilliges, J. Shotton, D. Molyneaux, S. Hodges, D. Kim, and A. Fitzgibbon, “KinectFusion: Real-time dense surface mapping and tracking,” in IEEE Int. Symp. on Mixed and Augmented Reality (ISMAR), 2011.</p>
<p>[5] L. M. Paz, P. Pini´es, J. D. Tard´os, and J. Neira, “Large-scale 6-DOF SLAM with stereo-in-hand,” IEEE Trans. Robot., vol. 24, no. 5, pp. 946–957, 2008.</p>
<p>[6] J. Civera, A. J. Davison, and J. M. M. Montiel, “Inverse depth parametrization for monocular SLAM,” IEEE Trans. Robot., vol. 24, no. 5, pp. 932–945, 2008.</p>
<p>[7] H. Strasdat, J. M. M. Montiel, and A. J. Davison, “Visual SLAM: Why filter?” Image and Vision Computing, vol. 30, no. 2, pp. 65–77, 2012.</p>
<p>[8] H. Strasdat, A. J. Davison, J. M. M. Montiel, and K. Konolige, “Double window optimisation for constant time visual SLAM,” in IEEE Int. Conf. Comput. Vision (ICCV), 2011, pp. 2352–2359.</p>
<p>[9] C. Mei, G. Sibley, M. Cummins, P. Newman, and I. Reid, “RSLAM: A system for large-scale mapping in constant-time using stereo,” Int. J. Comput. Vision, vol. 94, no. 2, pp. 198–214, 2011.</p>
<p>[10] T. Pire, T. Fischer, J. Civera, P. De Crist´oforis, and J. J. Berlles, “Stereo parallel tracking and mapping for robot localization,” in IEEE/RSJ Int. Conf. Intell. Robots and Syst. (IROS), 2015, pp. 1373–1378.</p>
<p>[11] J. Engel, J. Stueckler, and D. Cremers, “Large-scale direct SLAM with stereo cameras,” in IEEE/RSJ Int. Conf. Intell. Robots and Syst. (IROS), 2015.</p>
<p>[12] T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. McDonald, “Real-time large-scale dense RGB-D SLAM with volumetric fusion,” Int. J. Robot. Res., vol. 34, no. 4-5, pp. 598–626, 2015.</p>
<p>[13] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard, “3-D mapping with an RGB-D camera,” IEEE Trans. Robot., vol. 30, no. 1, pp. 177– 187, 2014.</p>
<p>[14] C. Kerl, J. Sturm, and D. Cremers, “Dense visual SLAM for RGB-D cameras,” in IEEE/RSJ Int. Conf. Intell. Robots and Syst. (IROS), 2013.</p>
<p>[15] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison, and S. Leutenegger, “ElasticFusion: Real-time dense SLAM and light source estimation,” Int. J. Robot. Res., vol. 35, no. 14, pp. 1697–1716, 2016.</p>
<p>[16] D. G´alvez-L´opez and J. D. Tard´os, “Bags of binary words for fast place recognition in image sequences,” IEEE Trans. Robot., vol. 28, no. 5, pp. 1188–1197, 2012.</p>
<p>[17] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: an efficient alternative to SIFT or SURF,” in IEEE Int. Conf. Comput. Vision (ICCV), 2011, pp. 2564–2571.</p>
<p>[18] R. Mur-Artal and J. D. Tard´os, “Fast relocalisation and loop closing in keyframe-based SLAM,” in IEEE Int. Conf. on Robot. and Autom. (ICRA), 2014, pp. 846–853.</p>
<p>[19] R. Kuemmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “g2o: A general framework for graph optimization,” in IEEE Int. Conf. on Robot. and Autom. (ICRA), 2011, pp. 3607–3613.</p>
<p>[20] H. Strasdat, J. M. M. Montiel, and A. J. Davison, “Scale drift-aware large scale monocular SLAM.” in Robot.: Sci. and Syst. (RSS), 2010.</p>
<p>[21] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The EuRoC micro aerial vehicle datasets,” Int. J. Robot. Res., vol. 35, no. 10, pp. 1157–1163, 2016.</p>
<p>[22] R. Mur-Artal and J. D. Tard´os, “Visual-inertial monocular SLAM with map reuse,” IEEE Robot. and Autom. Lett., vol. 2, no. 2, pp. 796 – 803, 2017.</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>SLAM</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文收集</title>
    <url>/%E8%AE%BA%E6%96%87%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<h1 id="一、智慧交通"><a href="#一、智慧交通" class="headerlink" title="一、智慧交通"></a>一、智慧交通</h1><h2 id="1-1"><a href="#1-1" class="headerlink" title="1-1"></a>1-1</h2><p>   Real-Time Adaptive Traffic Control System For Smart Cities <a href="https://ieeexplore.ieee.org/document/9402597">智慧城市实时自适应交通控制系统</a></p>
<ul>
<li><strong>作者</strong>：Shyam Shankaran R; Logesh Rajendran</li>
<li><strong>会议</strong>: 2021 International Conference on Computer Communication and Informatics (ICCCI)</li>
<li><strong>引用</strong>：0，浏览24</li>
<li><strong>摘要</strong>：In-country like India, billions of people start and end each working day stuck in traffic or commuting on congested trains and buses. It is vital to the quality of life to enhance the everyday commute. By 2025, cities that implement smart mobility systems on average, reduce commuting cycles by 15-20 percent, with some individuals experiencing even greater reductions. Depending on each city’s density, current transit facilities, and commuting habits, the capacity associated with each application is highly variable. Slowed synchronization of traffic signals leads to traffic congestion and delays. The pre-programmed, regular signal timing patterns are employed in traditional signal systems. To overcome the problems of traditional traffic control systems, there is a shift in adaption to an Adaptive traffic control system. The Adaptive Traffic Control System (ATCS) is a traffic management technique that modifies or adapts the timing of traffic signals based on the real demand for traffic and achieved using a control system that includes both hardware and software, where hardware is the sensor used for real-time traffic density estimation and software is designed using captured data analysis of the city’s current traffic flow. This paper depicts a model of camera-based traffic monitoring and processing system which reduces the cycle time and possesses special provisions for emergency vehicles.</li>
<li><strong>点评</strong>：使用opencv，光学传感器，偏向图像识别。</li>
</ul>
<h2 id="1-2"><a href="#1-2" class="headerlink" title="1-2"></a>1-2</h2><p>   Big Traffic Data Analytics For Smart Urban Intelligent Traffic System Using Machine Learning Techniques <a href="https://ieeexplore.ieee.org/document/9291790">使用机器学习技术的智慧城市智能交通系统大交通数据分析</a></p>
<ul>
<li><strong>作者</strong>：Su Su Hlaing; Mie Mie Tin; Mie Mie Khin</li>
<li><strong>会议</strong>：2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)</li>
<li><strong>引用</strong>：1，浏览138</li>
<li><strong>摘要</strong>：Due to huge number of private and public vehicles in last two decades, the traffic load and congestion have increased significantly, which is major problem in transportation system. An intelligent traffic management system becomes an important part of the transportation system to manage the traffic properly in smart cities. In this paper, we have proposed intelligent traffic management system to build smart platform in Mandalay, Myanmar. The main aim of the paper is to reduce traffic congestion, road crash accidents, fuel consumption and save travel time. To provide safe, comfortable, and less frustrating travel, this paper uses big data technology and machine learning technique for analysis of the volume of traffic data and predicts optimal road traffic using machine learning.</li>
<li><strong>点评</strong>：论文很短，比起具体方法更像是框架构想。</li>
</ul>
<h2 id="1-3"><a href="#1-3" class="headerlink" title="1-3"></a>1-3</h2><p>   Internet of things — smart traffic management system for smart cities using big data analytics <a href="https://ieeexplore.ieee.org/document/8301496">物联网——使用大数据分析的智慧城市智能交通管理系统</a></p>
<ul>
<li><strong>作者</strong>：Abida Sharif; Jianping Li; Mudassir Khalil</li>
<li><strong>会议</strong>：2017 14th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)</li>
<li><strong>引用</strong>：10，浏览2600</li>
<li><strong>摘要</strong>：Smart Traffic System (STS) is a one of the important aspect for future smart city. STS is more expensive and highly configurable to provide better quality of service for public traffic management. This paper proposes a low cost future STS to provide better service by deploying traffic update instantly. Low cost vehicle detecting sensors are fixed in the middle of road for every 500 meters. Internet of Things (IoT) is being used to attain public traffic data quickly and send it for data processing. The Real time streaming data is sent for Big Data analytics. There are several analytical scriptures to analyze the traffic density and provide solution through predictive analytics.</li>
<li><strong>点评</strong>：比较宏观，缺少细节</li>
</ul>
<h2 id="1-4"><a href="#1-4" class="headerlink" title="1-4"></a>1-4</h2><p>   Towards An Optimized Smart Traffic for Congestion Avoidance with Multi Layered (ST-CA) Framework<br>   <a href="https://ieeexplore.ieee.org/document/8538401">使用多层 (ST-CA) 框架实现优化的智能交通拥塞避免</a></p>
<ul>
<li><strong>作者</strong>：Nada M. Alhakkak; Ban Salman; Najeeb Abbas Al-Sammarraie</li>
<li><strong>会议</strong>：2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE)</li>
<li><strong>引用</strong>：1，浏览127</li>
<li><strong>摘要</strong>：In many cities around the world, traffic congestion is considered a major problem that should be managed properly. Managing traffic flow is not simply applied by constructing new road or maintaining old ones; it is not just a matter of physical infrastructure. For managing traffic, there should be ‘’smart’’ ways to be taken by the use of available technologies’ facilities. Countries should think and act smart to manage today’s traffic in their cities. The use of controlled smart traffic systems is gaining a noticeable interest for the past few years and various solutions have been proposed, adopted and even implemented in several countries. In this paper, we propose Smart Traffic for Congestion Avoidance with Multi Layered (ST-CA) Framework, for solving the congestion problem. The proposed framework is constructed in multilayer approach for easy maintenance in the future. The STCA framework should be more flexible, in sending/ receiving messages, than others and depend in its decision making on expert systems.</li>
<li><strong>点评</strong>：交通控制框架，信号灯算法，针对交通堵塞</li>
</ul>
<h2 id="1-5"><a href="#1-5" class="headerlink" title="1-5"></a>1-5</h2><p>   Foundations for Smarter Cities <a href="https://ieeexplore.ieee.org/document/5512826">智慧城市的基础</a></p>
<ul>
<li><strong>作者</strong>：C. Harrison; B. Eckman; R. Hamilton</li>
<li><strong>年份</strong>：2010</li>
<li><strong>发表</strong>：IBM Journal of Research and Development</li>
<li><strong>引用</strong>：499，浏览1357</li>
<li><strong>摘要</strong>：This paper describes the information technology (IT) foundation and principles for Smarter Cities™. Smarter Cities are urban areas that exploit operational data, such as that arising from traffic congestion, power consumption statistics, and public safety events, to optimize the operation of city services. The foundational concepts are instrumented, interconnected, and intelligent. Instrumented refers to sources of near-real-time real-world data from both physical and virtual sensors. Interconnected means the integration of those data into an enterprise computing platform and the communication of such information among the various city services. Intelligent refers to the inclusion of complex analytics, modeling, optimization, and visualization in the operational business processes to make better operational decisions. This approach enables the adaptation of city services to the behavior of the inhabitants, which permits the optimal use of the available physical infrastructure and resources, for example, in sensing and controlling consumption of energy and water, managing waste processing and transportation systems, and applying optimization to achieve new efficiencies among these resources. Additional roles exist in intelligent interaction between the city and its inhabitants and further contribute to operational efficiency while maintaining or enhancing quality of life.</li>
<li><strong>关键词</strong>：Information technology, Urban areas, Smart buildings, Decision making, Intelligent structures, Structural engineering</li>
<li><strong>点评</strong>：宏观框架，需要一读</li>
</ul>
<h2 id="1-6"><a href="#1-6" class="headerlink" title="1-6"></a>1-6</h2><p>   Three Decades of Driver Assistance Systems: Review and Future Perspectives <a href="https://ieeexplore.ieee.org/document/6936444">驾驶辅助系统的三个十年：回顾和未来展望</a></p>
<ul>
<li><strong>作者</strong>：Klaus Bengler; Klaus Dietmayer; Berthold Farbe</li>
<li><strong>年份</strong>：2014</li>
<li><strong>发表</strong>：IEEE Intelligent Transportation Systems Magazine</li>
<li><strong>引用</strong>：414，,浏览10237</li>
<li><strong>摘要</strong>：This contribution provides a review of fundamental goals, development and future perspectives of driver assistance systems. Mobility is a fundamental desire of mankind. Virtually any society strives for safe and efficient mobility at low ecological and economic costs. Nevertheless, its technical implementation significantly differs among societies, depending on their culture and their degree of industrialization. A potential evolutionary roadmap for driver assistance systems is discussed. Emerging from systems based on proprioceptive sensors, such as ABS or ESC, we review the progress incented by the use of exteroceptive sensors such as radar, video, or lidar. While the ultimate goal of automated and cooperative traffic still remains a vision of the future, intermediate steps towards that aim can be realized through systems that mitigate or avoid collisions in selected driving situations. Research extends the state-of-the-art in automated driving in urban traffic and in cooperative driving, the latter addressing communication and collaboration between different vehicles, as well as cooperative vehicle operation by its driver and its machine intelligence. These steps are considered important for the interim period, until reliable unsupervised automated driving for all conceivable traffic situations becomes available. The prospective evolution of driver assistance systems will be stimulated by several technological, societal and market trends. The paper closes with a view on current research fields.</li>
<li><strong>关键词</strong>：Vehicle safety, Sensor systems, Laser radar, Advanced driver assistance systems, Vehicle dynamics, Automatic control, Research and development, Technology forecasting</li>
<li><strong>点评</strong>：综述类型，需要一读。</li>
</ul>
<h1 id="二、雷达-AI-无人驾驶"><a href="#二、雷达-AI-无人驾驶" class="headerlink" title="二、雷达+AI+无人驾驶"></a>二、雷达+AI+无人驾驶</h1><h2 id="2-1"><a href="#2-1" class="headerlink" title="2-1"></a>2-1</h2><p>   On-Road Vehicle Detection and Tracking Using MMW Radar and Monovision Fusion <a href="https://ieeexplore.ieee.org/document/7463071">使用毫米波雷达和 Monovision Fusion 进行道路车辆检测和跟踪</a></p>
<ul>
<li><strong>作者</strong>：Xiao Wang; Linhai Xu; Hongbin Sun; Jingmin Xin; Nanning Zheng</li>
<li><strong>年份</strong>：2016</li>
<li><strong>发表</strong>：IEEE Transactions on Intelligent Transportation Systems</li>
<li><strong>引用</strong>：80，浏览4229</li>
<li><strong>摘要</strong>：With the potential to increase road safety and provide economic benefits, intelligent vehicles have elicited a significant amount of interest from both academics and industry. A robust and reliable vehicle detection and tracking system is one of the key modules for intelligent vehicles to perceive the surrounding environment. The millimeter-wave radar and the monocular camera are two vehicular sensors commonly used for vehicle detection and tracking. Despite their advantages, the drawbacks of these two sensors make them insufficient when used separately. Thus, the fusion of these two sensors is considered as an efficient way to address the challenge. This paper presents a collaborative fusion approach to achieve the optimal balance between vehicle detection accuracy and computational efficiency. The proposed vehicle detection and tracking design is extensively evaluated with a real-world data set collected by the developed intelligent vehicle. Experimental results show that the proposed system can detect on-road vehicles with 92.36% detection rate and 0% false alarm rate, and it only takes ten frames (0.16 s) for the detection and tracking of each vehicle. This system is installed on Kuafu-II intelligent vehicle for the fourth and fifth autonomous vehicle competitions, which is called “Intelligent Vehicle Future Challenge” in China.</li>
<li><strong>关键词</strong>：Radar tracking, Radar detection, Vehicle detection, Vehicles, Cameras, Sensors</li>
<li><strong>点评</strong>：SVM，协同融合毫米波雷达和单目摄像头，可以关注一下“智能汽车未来挑战赛”，需要一读。</li>
</ul>
<h2 id="2-2"><a href="#2-2" class="headerlink" title="2-2"></a>2-2</h2><p>   Architecture Design and Implementation of an Autonomous Vehicle <a href="https://ieeexplore.ieee.org/document/8340798">自动驾驶汽车的架构设计与实现</a></p>
<ul>
<li><strong>作者</strong>：Wenhao Zong; Changzhu Zhang; Zhuping Wang</li>
<li><strong>年份</strong>：2018</li>
<li><strong>发表</strong>：IEEE Access </li>
<li><strong>引用</strong>：12，浏览11325</li>
<li><strong>摘要</strong>：Architecture design is one of the most important problems for an intelligent system. In this paper, a practical framework of hardware and software is proposed to reveal the external configuration and internal mechanism of an autonomous vehicle-a typical intelligent system. The main contributions of this paper are as follows. First, we compare the advantages and disadvantages of three typical sensor plans and introduce a general autopilot for a vehicle. Second, we introduce a software architecture for an autonomous vehicle. The perception and planning performances are improved with the help of two inner loops of simultaneous localization and mapping. An algorithm to enlarge the detection range of the sensors is proposed by adding an inner loop to the perception system. A practical feedback to restrain mutations of two adjacent planning periods is also realized by the other inner loop. Third, a cross-platform virtual server (named project cocktail) for data transmission and exchange is presented in detail. Through comparisons with the robot operating system, the performance of project cocktail is proven to be considerably better in terms of transmission delay and throughput. Finally, a report on an autonomous driving test implemented using the proposed architecture is presented, which shows the effectiveness, flexibility, stability, and low-cost of the overall autonomous driving system.</li>
<li><strong>关键词</strong>：Robot sensing systems, Autonomous vehicles, Cameras, Millimeter wave radar, Laser radar, Wheels</li>
<li><strong>点评</strong>：论文里有三种传感器方案的比较，车的自动驾驶操作系统，硬软件都有，可以大致读一下</li>
</ul>
<h2 id="2-3"><a href="#2-3" class="headerlink" title="2-3"></a>2-3</h2><p>   Deep Learning-based Object Classification on Automotive Radar Spectra <a href="https://ieeexplore.ieee.org/document/8835775">基于深度学习的汽车雷达光谱对象分类</a></p>
<ul>
<li><strong>作者</strong>：Kanil Patel; Kilian Rambach; Tristan Visentin</li>
<li><strong>年份</strong>：2019</li>
<li><strong>发表</strong>：2019 IEEE Radar Conference (RadarConf)</li>
<li><strong>引用</strong>：17，浏览1528</li>
<li><strong>摘要</strong>：Scene understanding for automated driving requires accurate detection and classification of objects and other traffic participants. Automotive radar has shown great potential as a sensor for driver assistance systems due to its robustness to weather and light conditions, but reliable classification of object types in real time has proved to be very challenging. Here we propose a novel concept for radar-based classification, which utilizes the power of modern Deep Learning methods to learn favorable data representations and thereby replaces large parts of the traditional radar signal processing chain. We propose to apply deep Convolutional Neural Networks (CNNs) directly to regions-of-interest (ROI) in the radar spectrum and thereby achieve an accurate classification of different objects in a scene. Experiments on a real-world dataset demonstrate the ability to distinguish relevant objects from different viewpoints. We identify deep learning challenges that are specific to radar classification and introduce a set of novel mechanisms that lead to significant improvements in object classification performance compared to simpler classifiers. Our results demonstrate that Deep Learning methods can greatly augment the classification capabilities of automotive radar sensors.</li>
<li><strong>关键词</strong>：Deep learning, Automotive engineering, Azimuth, Radar imaging, Radar tracking, Training</li>
<li><strong>点评</strong>：CNN应用于分析雷达光谱，优化传统雷达信号处理方法，可用的现实世界训练数据有限情况下。</li>
</ul>
<h2 id="2-4"><a href="#2-4" class="headerlink" title="2-4"></a>2-4</h2><p>   Robust vehicle localization in urban environments using probabilistic maps <a href="https://ieeexplore.ieee.org/document/5509700">使用概率图在城市环境中进行稳健的车辆定位</a></p>
<ul>
<li><strong>作者</strong>：Jesse Levinson; Sebastian Thrun</li>
<li><strong>年份</strong>：2010</li>
<li><strong>发表</strong>：2010 IEEE International Conference on Robotics and Automation</li>
<li><strong>引用</strong>：285，浏览4931</li>
<li><strong>摘要</strong>：Autonomous vehicle navigation in dynamic urban environments requires localization accuracy exceeding that available from GPS-based inertial guidance systems. We have shown previously that GPS, IMU, and LIDAR data can be used to generate a high-resolution infrared remittance ground map that can be subsequently used for localization. We now propose an extension to this approach that yields substantial improvements over previous work in vehicle localization, including higher precision, the ability to learn and improve maps over time, and increased robustness to environment changes and dynamic obstacles. Specifically, we model the environment, instead of as a spatial grid of fixed infrared remittance values, as a probabilistic grid whereby every cell is represented as its own gaussian distribution over remittance values. Subsequently, Bayesian inference is able to preferentially weight parts of the map most likely to be stationary and of consistent angular reflectivity, thereby reducing uncertainty and catastrophic errors. Furthermore, by using offline SLAM to align multiple passes of the same environment, possibly separated in time by days or even months, it is possible to build an increasingly robust understanding of the world that can be then exploited for localization. We validate the effectiveness of our approach by using these algorithms to localize our vehicle against probabilistic maps in various dynamic environments, achieving RMS accuracy in the 10cm-range and thus outperforming previous work. Importantly, this approach has enabled us to autonomously drive our vehicle for hundreds of miles in dense traffic on narrow urban roads which were formerly unnavigable with previous localization methods.</li>
<li><strong>关键词</strong>：Robustness, Remotely operated vehicles, Vehicle dynamics, Mobile robots, Navigation, Global Positioning System, Laser radar, Gaussian distribution, Bayesian methods, Reflectivity</li>
<li><strong>点评</strong>：在动态环境中根据概率图做车辆定位，地图导航向，有点意思。</li>
</ul>
<h2 id="2-5"><a href="#2-5" class="headerlink" title="2-5"></a>2-5</h2><p>   MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map <a href="https://ieeexplore.ieee.org/document/8936918">MapLite：无需详细事先地图的自主交叉路口导航</a></p>
<ul>
<li><strong>作者</strong>：Teddy Ort; Krishna Murthy; Rohan Banerjee</li>
<li><strong>年份</strong>：2019</li>
<li><strong>发表</strong>：IEEE Robotics and Automation Letters</li>
<li><strong>引用</strong>：1，浏览2964</li>
<li><strong>摘要</strong>：In this work, we present MapLite: a one-click autonomous navigation system capable of piloting a vehicle to an arbitrary desired destination point given only a sparse publicly available topometric map (from OpenStreetMap). The onboard sensors are used to segment the road region and register the topometric map in order to fuse the high-level navigation goals with a variational path planner in the vehicle frame. This enables the system to plan trajectories that correctly navigate road intersections without the use of an external localization system such as GPS or a detailed prior map. Since the topometric maps already exist for the vast majority of roads, this solution greatly increases the geographical scope for autonomous mobility solutions. We implement MapLite on a full-scale autonomous vehicle and exhaustively test it on over 15 km of road including over 100 autonomous intersection traversals. We further extend these results through simulated testing to validate the system on complex road junction topologies such as traffic circles.</li>
<li><strong>关键词</strong>：Roads, Navigation, Sensors, Autonomous vehicles, Laser radar, Support vector machines, Surface texture</li>
<li><strong>点评</strong>：地图导航向，激光雷达，无需详细地图的自动驾驶。</li>
</ul>
<h2 id="2-6"><a href="#2-6" class="headerlink" title="2-6"></a>2-6</h2><p>   Object Classification and Recognition From Mobile Laser Scanning Point Clouds in a Road Environment <a href="https://ieeexplore.ieee.org/document/7287763">基于移动激光扫描点云的道路环境目标分类与识别</a></p>
<ul>
<li><strong>作者</strong>：Matti Lehtomäki; Anttoni Jaakkola; Juha Hyyppä</li>
<li><strong>年份</strong>：2015</li>
<li><strong>发表</strong>：IEEE Transactions on Geoscience and Remote Sensing</li>
<li><strong>引用</strong>：51，浏览2410</li>
<li><strong>摘要</strong>：Automatic methods are needed to efficiently process the large point clouds collected using a mobile laser scanning (MLS) system for surveying applications. Machine-learning-based object recognition from MLS point clouds in a road and street environment was studied in order to create maps from the road environment infrastructure. The developed automatic processing workflow included the following phases: the removal of the ground and buildings, segmentation, segment classification, and object location estimation. Several novel geometry-based features, which were previously applied in autonomous driving and general point cloud processing, were applied for the segment classification of MLS point clouds. The features were divided into three sets, i.e., local descriptor histograms (LDHs), spin images, and general shape and point distribution features, respectively. These were used in the classification of the following roadside objects: trees, lamp posts, traffic signs, cars, pedestrians, and hoardings. The accuracy of the object recognition workflow was evaluated using a data set that contained more than 400 objects. LDHs and spin images were applied for the first time for machine-learning-based object classification in MLS point clouds in the surveying applications of the road and street environment. The use of these features improved the classification accuracy by 9.6% (resulting in 87.9% accuracy) compared with the accuracy obtained using 17 general shape and point distribution features that represent the current state of the art in the field of MLS; therefore, significant improvement in the classification accuracy was achieved. Connected component segmentation and ground extraction were the cause of most of the errors and should be thus improved in the future.</li>
<li><strong>关键词</strong>：Three-dimensional displays, Histograms, Roads, Image segmentation, Buildings, Object recognition, Accuracy</li>
<li><strong>点评</strong>：目标识别+移动激光扫描系统收集的大型点云，关系不大，仅供参考。</li>
</ul>
<h2 id="2-7"><a href="#2-7" class="headerlink" title="2-7"></a>2-7</h2><p>   Machine Vision Based Traffic Sign Detection Methods: Review, Analyses and Perspectives <a href="https://ieeexplore.ieee.org/document/8746141">基于机器视觉的交通标志检测方法：回顾、分析和展望</a></p>
<ul>
<li><strong>作者</strong>：Chunsheng Liu; Shuang Li; Faliang Chang</li>
<li><strong>年份</strong>：2019</li>
<li><strong>发表</strong>：IEEE Access</li>
<li><strong>引用</strong>：18，浏览2431</li>
<li><strong>摘要</strong>：Traffic signs recognition (TSR) is an important part of some advanced driver-assistance systems (ADASs) and auto driving systems (ADSs). As the first key step of TSR, traffic sign detection (TSD) is a challenging problem because of different types, small sizes, complex driving scenes, and occlusions. In recent years, there have been a large number of TSD algorithms based on machine vision and pattern recognition. In this paper, a comprehensive review of the literature on TSD is presented. We divide the reviewed detection methods into five main categories: color-based methods, shape-based methods, color- and shape-based methods, machine-learning-based methods, and LIDAR-based methods. The methods in each category are also classified into different subcategories for understanding and summarizing the mechanisms of different methods. For some reviewed methods that lack comparisons on public datasets, we reimplemented part of these methods for comparison. The experimental comparisons and analyses are presented on the reported performance and the performance of our reimplemented methods. Furthermore, future directions and recommendations of the TSD research are given to promote the development of the TSD.</li>
<li><strong>关键词</strong>：Cameras, Shape, Laser radar, Color, Support vector machines, Machine vision, Roads</li>
<li><strong>点评</strong>：综述类型，针对交通标志识别，激光雷达，道路场景</li>
</ul>
<h2 id="2-8"><a href="#2-8" class="headerlink" title="2-8"></a>2-8</h2><p>   Unexpected Collision Avoidance Driving Strategy Using Deep Reinforcement Learning <a href="https://ieeexplore.ieee.org/document/8961990">使用深度强化学习的意外碰撞避免驾驶策略</a></p>
<ul>
<li><strong>作者</strong>：Myounghoe Kim; Seongwon Lee; Jaehyun Lim</li>
<li><strong>年份</strong>：2020</li>
<li><strong>发表</strong>：IEEE Access</li>
<li><strong>引用</strong>：4，浏览1380</li>
<li><strong>摘要</strong>：In this paper, we generated intelligent self-driving policies that minimize the injury severity in unexpected traffic signal violation scenarios at an intersection using the deep reinforcement learning. We provided guidance on reward engineering in terms of the multiplicity of objective function. We used a deep deterministic policy gradient method in the simulated environment to train self-driving agents. We designed two agents, one with a single-objective reward function of collision avoidance and the other with a multi-objective reward function of both collision avoidance and goal-approaching. We evaluated their performances by comparing the percentages of collision avoidance and the average injury severity against those of human drivers and an autonomous emergency braking (AEB) system. The percentage of collision avoidance of our agents were 78.89% higher than human drivers and 84.70% higher than the AEB system. The average injury severity score of our agents were only 8.92% of human drivers and 6.25% of the AEB system.</li>
<li><strong>关键词</strong>：Reinforcement learning, Collision avoidance, Injuries, Vehicles, Gradient methods, Wheels, Laser radar</li>
<li><strong>点评</strong>：意外场景下避免碰撞的自动驾驶策略，仅使用单通道lidar，有点意思。</li>
</ul>
<h1 id="三、-多传感器融合"><a href="#三、-多传感器融合" class="headerlink" title="三、 多传感器融合"></a>三、 多传感器融合</h1><h2 id="2-1-1"><a href="#2-1-1" class="headerlink" title="2-1"></a>2-1</h2><h2 id="3-1"><a href="#3-1" class="headerlink" title="3-1"></a>3-1</h2><p>   An introduction to multisensor data fusion <a href="https://ieeexplore.ieee.org/document/554205">多传感器数据融合简介</a></p>
<ul>
<li><strong>作者</strong>：D.L. Hall; J. Llinas</li>
<li><strong>年份</strong>：1997</li>
<li><strong>发表</strong>：Proceedings of the IEEE</li>
<li><strong>引用</strong>：1403，浏览15349</li>
<li><strong>摘要</strong>：Multisensor data fusion is an emerging technology applied to Department of Defense (DoD) areas such as automated target recognition, battlefield surveillance, and guidance and control of autonomous vehicles, and to non-DoD applications such as monitoring of complex machinery, medical diagnosis, and smart buildings. Techniques for multisensor data fusion are drawn from a wide range of areas including artificial intelligence, pattern recognition, statistical estimation and other areas. This paper provides a tutorial on data fusion, introducing data fusion applications, process models, and identification of applicable techniques. Comments are made on the state-of-the-art in data fusion.</li>
<li><strong>关键词</strong>：Target recognition, Surveillance, Navigation, Automatic control, Intelligent vehicles, Remotely operated vehicles, Mobile robots, Biomedical monitoring, Computerized monitoring, Condition monitoring</li>
<li><strong>点评</strong>：综述类型，需要一读</li>
</ul>
<h1 id="四、5G定位-雷达"><a href="#四、5G定位-雷达" class="headerlink" title="四、5G定位+雷达"></a>四、5G定位+雷达</h1>]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Mutual Learning-reading</title>
    <url>/Deep-Mutual-Learning-reading/</url>
    <content><![CDATA[<h2 id="论文笔记：Deep-Mutual-Learning"><a href="#论文笔记：Deep-Mutual-Learning" class="headerlink" title="论文笔记：Deep Mutual Learning"></a>论文笔记：Deep Mutual Learning</h2><h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><h2 id="1-logit"><a href="#1-logit" class="headerlink" title="1. logit"></a>1. logit</h2><p>logit原本是sigmoid函数的反函数。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004022202.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004021607.png"></p>
<p>而在深度学习中，logits表示最终的全连接层（网络最后一层）的输出，logit经过sigmoid或者softmax进行激活后，得到概率p。在神经网络中，大部分情况并不需要用到logit函数公式。</p>
<p>在知识蒸馏中，在二分类情况下需要用到logit函数，因为需要从神经网络黑盒输出的概率值反推logits，再用logits作为输入调节温度后重新计算概率：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004022240.png"></p>
<p>因此对于逻辑回归问题而言，logit是输出函数（p）经过logit函数的结果。</p>
<p>然而对于多分类问题使用的激活函数softmax而言，其反函数其实不是logit函数，而是log函数，继续称作logit只是习惯罢了。</p>
<p>softmax：<br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004023129.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004022736.png"></p>
<p>总之，神经网络中的术语logits实际上跟logit函数关系不大，就是定义神经网络的一层输出结果，在该输出后一般会接一个softmax或者sigmoid激活层输出正则化后的概率。</p>
<p><a href="https://www.zhihu.com/question/60751553">参考</a></p>
<h2 id="2-KD散度"><a href="#2-KD散度" class="headerlink" title="2. KD散度"></a>2. KD散度</h2><p>KL散度是两个概率分布P和Q差别的非对称性的度量（P-Q不等于Q-P）。</p>
<p>KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004025020.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004025059.png"></p>
<h2 id="3-梯度下降"><a href="#3-梯度下降" class="headerlink" title="3. 梯度下降"></a>3. 梯度下降</h2><p><a href="https://www.zhihu.com/question/264189719">参考</a></p>
<h3 id="1-批量梯度下降"><a href="#1-批量梯度下降" class="headerlink" title="1). 批量梯度下降"></a>1). 批量梯度下降</h3><p>每次更新时使用所有样本。也就是说在第i次梯度下降过程中，对于θi的更新，所有样本都有贡献。</p>
<p>这个方法计算得到的是一个标准梯度，对于最优化问题、凸问题，肯定可以达到一个全局最优。</p>
<p>但是每一次更新的幅度都是比较大的。样本不多的情况下，收敛速度会很快，但是如果样本很多，每一次更新都会耗时很久。</p>
<h3 id="2-随机梯度下降"><a href="#2-随机梯度下降" class="headerlink" title="2). 随机梯度下降"></a>2). 随机梯度下降</h3><p>在每次更新时只用一个样本来更新θ。思路是用一个样本来近似所有样本。【这个样本是不是随机取的？最后是不是要用完所有样本？】</p>
<p>随机梯度下降计算的不是一个准确梯度，对于最优化问题、凸问题，虽然不是每次迭代都是向着全局最优方向，但是大方向是向着全局最优解的，最后的结果也往往在全局最优解附近。</p>
<p>这种方法速度更快，虽然不是全局最优，但有些情况下也是可以接受的。</p>
<h3 id="3-mini-batch梯度下降"><a href="#3-mini-batch梯度下降" class="headerlink" title="3). mini-batch梯度下降"></a>3). mini-batch梯度下降</h3><p>批量和随机的一种折中，每次更新时用b个样本，即使用一些样本来近似全部样本。【本文用的应该就是这个吧？】</p>
<h2 id="4-epoch，iteration，batchsize"><a href="#4-epoch，iteration，batchsize" class="headerlink" title="4. epoch，iteration，batchsize"></a>4. epoch，iteration，batchsize</h2><p>深度学习中经常看到epoch、iteration和batchsize，下面按照自己的理解说说这三个区别：</p>
<ul>
<li>batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</li>
<li>iteration：1个iteration等于使用batchsize个样本训练一次；</li>
<li>epoch：1个epoch等于使用训练集中的全部样本训练一次；</li>
</ul>
<p>举个例子，训练集有1000个样本，batchsize=10，那么：<br>训练完整个样本集需要：<br>100次iteration，1次epoch。</p>
<h2 id="5-generalization"><a href="#5-generalization" class="headerlink" title="5. generalization"></a>5. generalization</h2><p>泛化 (generalization) 是指神经网络对未在训练 (学习) 过程中遇到的数据可以得到合理的输出。</p>
<h2 id="6-机器学习常用的评测指标"><a href="#6-机器学习常用的评测指标" class="headerlink" title="6. 机器学习常用的评测指标"></a>6. 机器学习常用的评测指标</h2><ul>
<li>Rank-n</li>
<li>Precision &amp; Recall</li>
<li>F-score</li>
<li>Map </li>
<li>CMC</li>
<li>ROC Single shot</li>
<li>Muti shot<br><a href="https://blog.csdn.net/sinat_42239797/article/details/93651594">详细</a></li>
</ul>
<h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h1><p><code>模型蒸馏</code>是一种将知识从教师网络传递到学生网络的技术。这项技术最典型的应用是强大的大型或集合网络向小型网络的转移，以便于低内存或快速执行的需求。</p>
<p>在本文中，我们提出了<code>Deep Mutual Learning</code>，在这个策略中，不再是静态的预设的教师网络向学生网络的单向传递，而是学生集合在训练过程中协作学习、互相教导。</p>
<p>实验表明，多种网络结构都可以从互学习策略中收益，实验在<code>CIFAR-100识别数据集</code>和<code>Market-1501 person re-identification 基线</code>上取得了很好的结果。</p>
<p>令人惊讶的是，我们发现：强大的教师网络不是必需的。简单学生网络间的互学习，比从强大的静态的教师网络中学习的效果更好。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>DNN在许多问题上有最先进的表现，但是DNN的网络结构通常都非常深或者宽，并且包含大量的参数。这就引出了DNN的一个<code>缺点</code>：执行慢或者需要大量memory来存储，这个缺点限制了它在有低内存/快执行要求的app或者platform上的应用。于是，有关更小更快的model的研究开始增多。实现一个紧凑且准确的模型可以用这些方法：</p>
<ol>
<li>explicit frugal architecture design[8]</li>
<li>model compression[20]</li>
<li>pruning[13]</li>
<li>binarisation[18]</li>
<li>模型蒸馏[7]</li>
</ol>
<p>基于蒸馏的模型压缩与这样的observation[2,3]相关：1. 小网络和大网络有相同的<code>representation capacity</code>，2. 但是相比于大型网络，小网络只是更难训练和找到实现所需功能的正确参数。也就是说，限制点主要在于优化的难度而不是网络的大小[2]。为了更好的学习一个小网络，蒸馏方法起始于一个强大的（深/宽）的教师网络（或者网络集合），然后训练一个更小的学生网络去模拟教师网络[7,2,16,3]。模仿教师网络的类别概率[7]and/or 传递常规的监督学习目标外的额外信息的<code>feature representation</code>[2,19]。学习如何模仿教师网络的优化问题比直接学习目标函数更容易，小得多的学生网络可以匹配甚至超越更大的教师网络。</p>
<p>在本文中，我们探索了一种不同于模型蒸馏但是与之相关的思想——<code>mutual learning</code>。蒸馏起始于一个强大的大规模且预训练过的教师网络，然后向一个小的、未训练的学生网络单向传递知识。相反，在互学习中，我们开始于一群未训练的学生网络，这些网络同时学习如何一起解决任务。具体来说，每一个学生网络用两个损失来训练：1. 传统的监督学习损失，2. 模仿损失（就是将每个学生网络的class posterior与其他学生的类别概率相匹配）。以这样的方式训练，每个学生网络在基于peer-teaching的场景下的学习效果 比 在传统监督学习场景下单独学习的效果要好。此外，通过互学习训练的学生网络，比通过传统的从大型预训练教师网络中蒸馏训练来的学生网络的效果要好。而且，虽然传统的蒸馏要求教师网络比目标学生网络更大更强，但是事实证明，在很多情况下，几个大型网络的互学习与独立学习相比，性能也能获得提高。</p>
<p>我们所提出的互学习为何能有如此效果，其原因或许并不明显。当学习开始于所有小型且未训练的学生网络时，额外的知识来自哪里？为什么会收敛到一个好的结果，而不是像“the blind lead the blind”那样被群体思维所束缚？关于这些问题的直觉可以通过考虑以下情况而获得：每个学生主要由传统的监督学习损失来指导，这意味着他们的表现普遍提高，且他们不能像cohort一样任意漂移到群体思维中。通过监督学习，所以网络很快能对每个训练实例预测出相同的标签（正确的），但是由于每个网络的初始条件不同，他们对下一个样本最可能标签的概率估计是不同的。正是这些<code>secondary quantities</code>为蒸馏[7]和互学习提供了额外的信息。在互学习中，student cohort有效汇集了学生对下一个样本最可能类别的集体估计。根据peers来找出并匹配每个训练实例的其他最可能类别，增加了每个学生网络的<code>posterior entropy</code>[4,17]，这有助于他们收敛都一个更稳定(flatter)的最小值，对测试数据有更好的generalization。这与最近深度学习中高<code>posterior entropy</code>方案（网络参数设置）的鲁棒性研究[4,17]有关，但与盲目的正则化熵相比，对备选方案的选择更明智些。【4.8.2】</p>
<p>总之，互学习提供了一种简单有效的方法，通过与其他网络群组的合作训练来提高网络的泛化能力。我们在物体类别识别（CIFAR100[12]上的图像分类）和实例识别问题（Market1501[33]上的人物重识别）上都进行了广泛的实验。结果表明，与预先训练好的静态大网络的蒸馏相比，小的同辈网络之间的合作学习可以实现更好的性能。特别是在人的重新识别任务上，与最新的竞争者相比，使用一个用相互学习训练的更小的网络就可以获得最先进的结果。</p>
<p>此外我们还观察到：</p>
<ol>
<li>互学习可以应用于多种网络结构，以及由大小网络混合组成的异构群组(heterogeneous cohorts)；【会不会拉低大网络的训练效果？？？等下再看一下具体的实验数据】</li>
<li>随着群组中网络数量的增加，效果也会提高——这是一个很好的特性，因为可以只对小网络进行训练，多的网络可以适应给定的GPU资源，从而进行更有效的相互学习【？】；</li>
<li>它还有利于半监督学习，在有标签和无标签的数据上都激活模仿损失。</li>
</ol>
<p>最后，我们注意到，尽管我们的重点是获得一个单独的有效网络，但是整个群组也可以作为一个高效的集合模型来使用。</p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><h3 id="Model-Distillation"><a href="#Model-Distillation" class="headerlink" title="Model Distillation"></a>Model Distillation</h3><p>基于蒸馏的模型压缩方法在十年前就提出来了[3]，但是因为[7]的原因最近又重新流行了起来，[7]提出了一些为何这种方法能有效果的额外直觉——由于对higher entropy soft-targets的额外监督和正则化【？】。起初，一个常见的应用是将一个强大的模型/集合的教授网络的近似函数蒸馏到一个单独的学生神经网络中[3,7]。但是后来，这个思路被应用在将强大的但是易于训练的大型网络蒸馏到小的但是很难训练的学生网络里[19]，小网络的性能甚至能超过他们的老师。最近，蒸馏已经更系统的连接到信息学习理论[15]和SVM+[22]上——一个聪明的教师网络为学生网络提供privileged信息。<em>Zhang等人[29]在动作识别中利用了这种利用特权信息学习的模型提炼的想法：更昂贵的光流场被视为特权信息，光流CNN被用来教导运动矢量CNN。就从教师那里提炼的知识的表示而言，现有的模型通常使用教师的类概率[8]和/或特征表示[1, 19]。最近，Yim等人[27]利用了层之间的流动，计算为层之间特征图的内积</em>。在这里，我们讨论的是完全抛弃教师网络，让一群学生网络在互相蒸馏中互相教导。</p>
<h3 id="Collaborative-Learning"><a href="#Collaborative-Learning" class="headerlink" title="Collaborative Learning"></a>Collaborative Learning</h3><p>另一个相关的思想是<code>Dual Learning</code>[5]，两个跨语言翻译(cross-lingual translation)的模型进行交互式互相教学。但这只适用于这种特殊的翻译问题，在这个的问题中，一个无条件的语言内模型(unconditional within-language model)可以用于评估预测的质量，并最终提供驱动学习过程的监督。相比之下，我们的互学习方法适用于一般的分类问题。虽然关于集合的传统亮点是优先考虑多样性[12]，而我们互学习的方法减少了多样性，即所有学生通过学习互相模仿对方而变得更加相似。但是，我们的目标不是必需产生一个多样化的集合，而是要让网络能够找到一个对测试数据具有良好generalization的稳健的解决方案，这个解决方案很难通过传统的监督学习找到。<em>此外，在dual learning中，不同的模型有不同的学习任务，而在互学习中，这些任务是相同的。最近，合作学习 [Cooperative learning with visual attributes] 被提出来，为同一任务但在不同的领域联合学习多个模型。例如，识别同一组物体类别，但一个模型输入的是RGB图像，另一个输入的是深度图像。这些模型通过对象属性进行交流，这些属性是领域不变的。这也不同于相互学习，在相互学习中，所有的模型都处理相同的任务和领域。</em>。</p>
<h1 id="3-Deep-Mutual-Learning"><a href="#3-Deep-Mutual-Learning" class="headerlink" title="3. Deep Mutual Learning"></a>3. Deep Mutual Learning</h1><h2 id="3-1-Formulation"><a href="#3-1-Formulation" class="headerlink" title="3.1 Formulation"></a>3.1 Formulation</h2><p>如图是在两个网络群组上的DML，至于如何扩展到更多的网络，请参考2.3节</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004021001.png"></p>
<p>从M个类别中给定N个样本X = {xi}1:N，将对应的标签表示为Y={yi}1:N。给定神经网络θ1时，样本xi属于类别m的概率是：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004023313.png"></p>
<p>其中logit zm是神经网络θ1中softmax层的输出。【？不是输出是输入吧】</p>
<p>对于多类别的分类问题，训练网络θ1时的目标函数（损失函数）被定义为预测值和正确标签间的交叉熵错误：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004024024.png"></p>
<p>其中函数I是<code>指标函数indicator function</code>，定义如下：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004024115.png"></p>
<p>传统的监督性损失训练网络来预测训练实例的正确标签。为了提高θ1在测试实例上的<code>generalization performance</code>，我们用另外的同辈网络θ2以后验概率p2的形式来提供自己的训练经验。为了测量两个网络的预测结果p1和p2的匹配度，我们采用<code>Kullback Leibler Divergence</code>。</p>
<p>p1和p2的KL距离计算为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004024642.png"></p>
<p>网络θ1的整体损失函数Lθ1定义为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004024820.png"></p>
<p>类似的，θ2的目标损失函数Lθ2可以计算为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004025200.png"></p>
<p><em>通过这种方式，每个网络既要学习正确预测训练实例的真实标签（监督损失Lc），又要学习与其同伴的概率估计相匹配（minicry loss KL）。</em></p>
<p>我们基于KL散度的模仿损失(minicry loss)是不对称的，因此两个网络是不同的。我们可以改用对称的<code>Jensen-Shannon Divergence loss</code>:</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005120821.png"></p>
<p><em>然而，我们根据实验发现，无论是使用对称还是非对称的KL损失，并没有什么区别。【？】</em></p>
<h2 id="3-2-Optimization"><a href="#3-2-Optimization" class="headerlink" title="3.2 Optimization"></a>3.2 Optimization</h2><p><em>模型蒸馏和DML最关键的区别是：在DML中，两个模型是共同合作进行优化的，两个模型的优化过程互相密切干预。</em></p>
<p>互学习策略嵌入每个基于mini-batch的模型的更新步骤中，贯穿整个训练过程。<em>这些模型使用相同的mini-batch进行学习</em>。在每个迭代中，我们计算两个模型的预测值，并根据对方的预测值更新两个网络的参数。θ1和θ2的优化是迭代进行的，直到收敛。算法1中总结了优化的细节：<br><em>如果在单独的GPU上运行，它包括这4个连续的步骤；当有两个GPU时，可以分布式训练，即在一个GPU上运行1、2，在另一个GPU上运行3、4</em></p>
<h3 id="算法1-DML"><a href="#算法1-DML" class="headerlink" title="算法1: DML"></a>算法1: DML</h3><ul>
<li><p>输入：训练集X，标签集Y，学习率γ1,t和γ2,t</p>
</li>
<li><p>初始化：不同初始条件下的模型θ1和θ2</p>
</li>
<li><p>重复：</p>
<ul>
<li><p>t=t+1</p>
</li>
<li><p>从X中随机采样数据x</p>
<ul>
<li><p>对于当前mini-batch，根据公式1更新x的预测值p1和p2【这里的x是一个还是mini-batch梯度下降中的一组—是一组】</p>
</li>
<li><p>计算<code>stochastic gradient</code>，并更新θ1：</p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004031611.png"></p>
</li>
<li><p>对于当前mini-batch，根据公式1更新x的预测值p1和p2</p>
</li>
<li><p>计算<code>stochastic gradient</code>，并更新θ2：</p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004031913.png"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>直到：收敛</p>
</li>
</ul>
<p>【θ1和2的更新顺序是否有讲究？有，会一定程度上影响公平，因此后续出现了更加公平的分布式训练形式，效果进一步提高】</p>
<h2 id="3-3-Extension-to-Large-Student-Cohorts"><a href="#3-3-Extension-to-Large-Student-Cohorts" class="headerlink" title="3.3 Extension to Large Student Cohorts"></a>3.3 Extension to Large Student Cohorts</h2><p>提出的DML方法可以自然的扩展到学生群组中的更多网络。给定K个网络θ1:K，优化θk的目标函数为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004032158.png"></p>
<p>上式表明，当有K个网络时，每个学生的DML会有效的将群组中其他K-1个网络作为教师网络来提供学习经验。公式4是K=2时的一种特殊情况。注意到，我们增加了一个系数1/(K-1)来确保训练主要由监督学习的真实标签来指导。对于具有两个以上网络的DML优化时对算法1的直接拓展。它可以通过在一台设备上学习每个网络并在设备之间传递小概率向量来实现分布式。【？】</p>
<p>两个以上网络时，DML的一个有趣的备选学习策略是将其他所有K-1个网络的集合作为一个单独的教师网络来提供平均的学习经验，这将与蒸馏方法相似，但是在每个mini-batch模型更新时进行。θk的目标函数可以写作：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004033201.png"></p>
<p>在我们的实验中（4.8节），我们发现使用一个集合教师网络(DML_e)的DML策略比使用K-1个教师网络的DML策略效果要差。这是因为建立教师集合时的平均步骤(公式9)使得教师网络在正确分类上的后验概率峰值更高，从而降低了所有类别的<code>posterior entropy</code>。【？】因此，它与DML的其中一个目标——产生具有高<code>posterior entropy</code>的鲁棒解，相矛盾。</p>
<h2 id="3-4-Extension-to-Semi-supervised-Learning"><a href="#3-4-Extension-to-Semi-supervised-Learning" class="headerlink" title="3.4 Extension to Semi-supervised Learning"></a>3.4 Extension to Semi-supervised Learning</h2><p>DML可以直接扩展到半监督学习上。 在半监督学习的设置下，我们只对标记的数据激活交叉熵损失，而对所有训练数据计算基于KL距离的模仿损失。这是因为KL距离的计算不需要类别标签，所有也可以使用无标签数据。</p>
<p>将带标签数据集表示为L，将无标签数据集表示为U，X=L∪U，学习网络θ1的目标函数可以重新定义为：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005124316.png"></p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="4-1-数据集和设置"><a href="#4-1-数据集和设置" class="headerlink" title="4.1 数据集和设置"></a>4.1 数据集和设置</h2><h3 id="4-1-1-数据集"><a href="#4-1-1-数据集" class="headerlink" title="4.1.1 数据集"></a>4.1.1 数据集</h3><p>我们实验用到4个数据集：</p>
<ul>
<li><p>ImageNet</p>
<p>包括1000个类别的1.2 million张训练图像，50000张验证图像。</p>
<ul>
<li>CIFAR-10数据集包含10个类别的大小为32x32的彩色图片，同下</li>
<li>CIFAR-100数据集包含100个类别的大小为32x32的彩色图片，数据集被划分为50000张训练图片和10000张测试图片。报告了Top-1分类精度。【？】</li>
</ul>
</li>
<li><p>Market-1501数据集在<code>person re-identification问题</code> [Person Re-Identification] 中作为benchmark广泛使用【？】。与CIFAR上的目标类别分类问题不同，re-id的目标是将不同的非重叠相机视角下的人联系起来。</p>
<p>该数据集包含了32668张图片，从6个相机视角拍摄，1501个身份(identities)，其中751个身份用于训练，750个身份用于测试。</p>
<p>根据解决此问题的最新方法[28]，我们训练网络进行751-way分类，并且用最后一个池化层的结果特征作为测试时的最近邻匹配的representation。【？】</p>
<p>这是一个比CIFAR-100更具挑战性的数据集，因为它的任务是<code>instance recognition</code>，因此更<code>fine-grained</code>，数据集更小，类别更多。为了评估，我们使用标准的<code>Cumulative Matching Characteristic (CMC) Rank-k accuracy</code>和<code>mean average precision (mAP) metrics</code>[27]。</p>
</li>
</ul>
<h3 id="4-1-2-实现细节"><a href="#4-1-2-实现细节" class="headerlink" title="4.1.2 实现细节"></a>4.1.2 实现细节</h3><p>我们在TensorFlow框架[1]下实现所有网络和训练过程，在一个NVIDIA GeForce GTX 1080 GPU上跑所有试验。</p>
<ul>
<li><p>对于CIFAR-100，我们使用[25]中的所有试验设置。具体来讲，我们使用<code>SGD with Nesterov momentum</code>，将初始学习率设置为0.1，动量设置为0.9，mini-batch大小设置为64。每60 epochs学习率下降0.1【？那不是一次就无了，确定不是0.01？】，我们训练了200个epochs。数据增广包括水平翻转和随机剪裁（每边填充4个像素，用原图中的reflections来填补缺失的元素【？】）。</p>
</li>
<li><p>对于Market-1501，我们使用<code>Adam optimisor</code>【？】，学习率lr=0.0002，β1=0.5，β2=0.999，mini-batch size=16。<code>我们训练所有模型进行100000次迭代【这里没有epoch吗？】。我们还报告了使用/未使用在ImageNet上预训练的结果。</code></p>
</li>
<li><p>对于ImageNet，我们使用<code>RMSProp</code>【？】，decay设置为0.9，mini-batch为64，初始学习率为0.1。学习率每20个epochs衰减一次，使用<code>0.16的指数率</code>。</p>
</li>
</ul>
<h3 id="4-1-3-网络"><a href="#4-1-3-网络" class="headerlink" title="4.1.3 网络"></a>4.1.3 网络</h3><p>我们实验中使用的网络包括典型学生规模的紧凑网络：Resnet-32[6]和MobileNet[8]；和典型教师规模的大型网络：Inception V1[21]和Wide ResNet WRN-28-10[25]。表1对比了CIFAR-100数据集上所有网络的参数数量：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211004041537.png"></p>
<h2 id="4-2-CIFAR-100上的结果"><a href="#4-2-CIFAR-100上的结果" class="headerlink" title="4.2 CIFAR-100上的结果"></a>4.2 CIFAR-100上的结果</h2><p>表2比较了不同架构的2网络的DML群组在CIFAR-100数据集上的top-1准确性。从表中可以看出：</p>
<ol>
<li><code>DML-Independent</code>一栏说明：与独立学习相比，ResNet-32、MobileNet和WRN-28-10在群组中学习性能更好。</li>
<li>容量较小的网络（ResNet-32，MobileNet）通常在DML中获益更多</li>
<li>尽管WRN-28-10是一个比ResNet-32、MobileNet大得多的网络，却仍然比它小的网络的互学习训练中获益</li>
<li>与单独训练相比，大网络的群组训练仍是有益的</li>
</ol>
<p>因此，与模型蒸馏相反，我们看到，一个大型的预训练教授网络不是获益的必需，多个大型网络仍然可以从我们这个类似蒸馏的过程中获益。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005115037.png"></p>
<p><strong>此后都是正式版内容</strong></p>
<h2 id="4-3-Market-1501上的结果"><a href="#4-3-Market-1501上的结果" class="headerlink" title="4.3 Market-1501上的结果"></a>4.3 Market-1501上的结果</h2><p>在这个实验中，我们使用MobileNet in a two-network DML cohort。表3总结了在Market-1501上MobileNet带/不带DML训练时的mAP(%)和rank-1正确率(%)，并与当前最先进方法作比较。我们可以看到，在这个更具挑战性的<code>instance recognition</code>问题上，DML相比独立训练，能显著提高MobileNet的性能，无论是否用ImageNet进行预训练。还可以看到，我们使用两个MobileNet的方法显著优于当前最先进的<code>deep re-id</code>方法。值得注意的是，MobileNet是一个简单的、小型的、通用网络。相比之下，许多最近提出的deep re-id网络，诸如[31,23,34]里的网络，都具有复杂且专门设计的架构，以处理在不同相机视角下匹配人物时发生的剧烈<code>pose-changes</code>和<code>body-part mis-alignment</code>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005170219.png"></p>
<h2 id="4-4-ImageNet上的结果"><a href="#4-4-ImageNet上的结果" class="headerlink" title="4.4 ImageNet上的结果"></a>4.4 ImageNet上的结果</h2><p>图2(a)比较了在ImageNet上，独立训练和DML训练时，MobileNet和InceptionV1的准确性。我们可以看到这两种网络结构的DML版始终比独立版的表现更好。这些结果表明，DML适用于large-scale问题。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005170605.png"></p>
<h2 id="4-5-DML分布式训练"><a href="#4-5-DML分布式训练" class="headerlink" title="4.5 DML分布式训练"></a>4.5 DML分布式训练</h2><p>为了研究训练策略对DML的影响，我们对比了两种DML variants：</p>
<ol>
<li>Sequential：根据算法1在一个GPU上训练两个网络；两个网络依次更新；</li>
<li>distributed：每一个网络在一个单独的GPU上训练，使用CPU进行KL散度通信；这样，两个网络的预测和参数可以同步更新。</li>
</ol>
<p>我们在Market-1501上使用2个MobileNet进行实验，并在图2(b)中展示了收敛和mAP结果。</p>
<p>有趣的是，DML性能可以通过分布式训练得到进一步提升。比较这两个变体，在分布式训练时两个网络变得更加<code>euqal</code>，因为他们总是有完全相同的训练迭代次数。因此，这一结果表明，当学生网络的学习进度差异最小时，他们从DML pee-teaching中获益最大。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005173150.png"></p>
<h2 id="4-6-和模型蒸馏相对比"><a href="#4-6-和模型蒸馏相对比" class="headerlink" title="4.6 和模型蒸馏相对比"></a>4.6 和模型蒸馏相对比</h2><p>由于我们的方法与模型蒸馏[8]非常相关，我们将二者作进一步的比较。表4对比了我们DML和模型蒸馏。其中教师网络(Net1)是预训练的，并为学生网络(Net2)提供固定的后验目标【soft-max】。正如预期的那样，与独立学习相比，在传统的蒸馏方法中，强大的预训练的教授网络确实提高了学生网络的性能。（1蒸馏到2，对比2独立）</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005173612.png"></p>
<p>然而，结果表明，与蒸馏方法相比，使用DML一起训练两个网络的效果更好。（1蒸馏到2，对比2DML）这意味着在互相学习的过程中，扮演教师角色的网络通过与未经训练的学生网络互动学习，实际效果比预训练的教师网络要好。</p>
<h2 id="4-7-DML-with-Large-Student-Cohorts"><a href="#4-7-DML-with-Large-Student-Cohorts" class="headerlink" title="4.7 DML with Large Student Cohorts"></a>4.7 DML with Large Student Cohorts</h2><p>之前的实验群组中都只有两个学生网络。我们接下来就研究群组中有多个学生时的DML。图3(a)展示了在Market-1501上用DML训练越来越多的MobileNets群组规模的结果。图中显示了平均mAP和标准差。从图3(a)我们可以看出，平均单个网络的性能随着DML群组中网络数量的增加而增加，因此它与独立训练的网络有差别。这表明，当与越来越多的同伴一起学习时，学生的泛化能力得到了提高。不同网络的表现也随着群组规模的扩大而更加一致，这表现在变小的标准差上。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005174750.png"></p>
<p>在训练多个网络时，一个常见的技术是将它们作为一个集合体来进行综合预测【啥？】。在图3(b)中，我们使用了与图3(a)相同的模型，但基于集合体进行预测（根据所有成员的concatenated feature进行匹配），而不是报告每个个体的平均预测结果。从结果中我们可以看到，正如预期的那样，集合预测的结果优于单个网络预测（图3（b）对比（a））。此外，集合的性能也能随着DML群组数量而增加（图3（b）DML集合，对比Independent集合）。</p>
<p>【dml_e是啥？训练时使用K-1个学生网络的集合教师版训练方式】<br>【这里b的集合是一种综合预测的方法——需要进一步查一下细节】<br>【咋串联特征】</p>
<h2 id="4-8-How-and-Why-does-DML-Work"><a href="#4-8-How-and-Why-does-DML-Work" class="headerlink" title="4.8 How and Why does DML Work?"></a>4.8 How and Why does DML Work?</h2><p>在这一节中，我们试图给出一些关于我们的深度互学习策略是如何以及为什么发挥作用的见解。</p>
<p>最近有一波关于 “Why Deep Nets Generalise”的研究[4,30,10]，这些研究提供了一些见解，比如：</p>
<ol>
<li>虽然经常有许多解决方案（深度网络参数设置）产生零训练误差，但是其中一些泛化效果比其他好[4,10]的原因是因为处于<code>wide valleys</code>而非<code>narrow crevices</code>，因此小的扰动不会使预测效果发生巨大变化；</li>
<li>深度网络在寻找这些好的解决方案方面比预期的要好[30]，但通过使深度网络偏向具有较高后验熵的解决方案，可以增强寻找稳健最小值的趋势[4,17]。【？？？】</li>
</ol>
<h3 id="4-8-1-DML-Leads-to-Better-Quality-Solutions-with-More-Robust-Minima"><a href="#4-8-1-DML-Leads-to-Better-Quality-Solutions-with-More-Robust-Minima" class="headerlink" title="4.8.1 DML Leads to Better Quality Solutions with More Robust Minima"></a>4.8.1 DML Leads to Better Quality Solutions with More Robust Minima</h3><p>考虑到这些见解，我们对DML的过程做了一些观察。首先我们注意到，在我们的实验中，网络通常完美适合训练数据：训练准确率达到100%，分类损失变得最小（例如，图4（a））。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005184323.png"></p>
<p>然而，正如我们前面所看到的，DML在测试数据上的表现更好【？】。因此，与其说DML帮助我们找到了一个better（deeper）训练损失的最小值，不如说它帮助我们找到了一个wider/更robust的最小值，可以更好地推广到测试数据。</p>
<p>受[4, 10]的启发，我们进行了一个简单的测试，以分析使用MobileNet在CIFAR-100上发现的最小值的稳健性。对于DML和独立模型，我们比较了在每个模型参数中加入标准差为σ的独立高斯噪声前后所学模型的训练损失。我们看到，两个最小值的深度是一样的（图4(b)），但是在加入这个扰动后，独立模型的训练损失跳升了，而DML模型的损失增加得更少。这表明，DML模型找到了一个更宽的最小值，这有望提供更好的泛化性能[4,17]。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005191157.png"></p>
<h3 id="4-8-2-How-a-Better-Minima-is-Found"><a href="#4-8-2-How-a-Better-Minima-is-Found" class="headerlink" title="4.8.2 How a Better Minima is Found"></a>4.8.2 How a Better Minima is Found</h3><p>当要求每个网络与其同伴的概率估计相匹配时，不匹配：一个给定网络预测zero和该网络教师/同伴的预测 non-zero，会受到很大的惩罚【非零？】。因此，DML的总体效果是，每个网络单独将一个<code>小mass</code>放在一小部分的次要概率，而DML中的所有网络倾向于汇总他们对次要概率的预测，并且</p>
<ol>
<li>总体上将更多mass放在了次要概率上</li>
<li>对更多有区分度的次要概率赋予了非零的mass</li>
</ol>
<p>【和蒸馏的soft思路有点像】</p>
<p>我们通过比较在CIFAR-100上由DML训练的ResNet-32模型与独立训练的ResNet-32模型，所得到的<code>top-5 highest ranked classes</code>来说明这一效果，见图4（c）。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005191553.png"></p>
<p>对于每个训练样本，根据模型产生的后验概率对前5个类进行排序（第1类是真正的类，第2类是第二大可能的类，等等）。在这里我们可以看到，独立训练时对Top-1以下概率的mass分配比DML衰减得更快。这可以通过DML训练的模型和独立训练的模型的所有训练样本的平均熵值来量化，分别为<code>1.7099</code>和<code>0.2602</code>。因此，我们的方法与<code>entropy regularisation-based approaches</code>[4,17]有联系，可以找到<code>wide minima</code>【？】，但是我们的方法是通过对 “reasonable”的备选方案进行<code>mutual probability matching</code>，而不是盲目的选择<code>high-entropy</code>偏好。</p>
<p>表5进一步显示，DML与基于熵的正则化方法[4]相比，是更有效的学习更泛化模型的方法（DML相比于独立，熵）。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005192451.png"></p>
<p>【Single-Query results？表5是个啥玩意？？？】</p>
<h3 id="4-8-3-DML-with-Ensemble-Teacher"><a href="#4-8-3-DML-with-Ensemble-Teacher" class="headerlink" title="4.8.3 DML with Ensemble Teacher"></a>4.8.3 DML with Ensemble Teacher</h3><p>在DML中，不论群组里有多少学生，每个学生都由所有同组的其他学生独立教导（公式（8））。在第3.3节中，讨论了另一种DML策略（DML_e），即要求每个学生与<code>群组中所有其他学生的集合</code>的预测相匹配（公式（9））。</p>
<p>人们有理由期待这种方法会更好：因为集合预测比单个预测要好，它应该提供一个cleaner、stronger的教学信号——更像传统的蒸馏法。</p>
<p>但在实践中，集合教学结果更差（见图3）。通过分析集合教学与同伴教学的教学信号，集合的目标相比同伴的目标在真实标签上的峰值要尖锐得多，导致DML的预测熵值（0.2805）比DML_e（0.1562）大。因此，虽然集合的<code>noise-averaging property</code>【？】对做出正确的预测是有效果的，但它实际上不利于提供教学信号，因为二级类概率是教学信号中的突出线索。拥有高熵的后验可以使得模型训练的解决方案更加稳健。</p>
<h2 id="4-9-DML使模型更相似？"><a href="#4-9-DML使模型更相似？" class="headerlink" title="4.9 DML使模型更相似？"></a>4.9 DML使模型更相似？</h2><p>我们知道，在相同的训练目标下，对于DML群组中的不同模型，预测的类别后验会很相似。问题是，这些模型是否也会产生相似的特征，特别是当模型具有相同的架构时？</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005210045.png"></p>
<p>图5展示了两个MobileNets在Market-1501测试集上的特征分布的<code>t-SNE可视化</code>【？】。我们可以看到，无论是有还是没有DML，两个MobileNets确实产生了不同的特征，表明得到了不同的模型。这有助于解释为什么DML群组中的不同模型可以互相教导：每个人都学到了别人没有学到的东西。</p>
<p>我们注意到，在一些模型蒸馏的研究中 [1, 19]中，加入了一个<code>feature distance loss</code>，以迫使学生网络在相应层产生与教师相似的特征。当教师是预训练且固定，而学生的目的是模仿教师时，这是有意义的。然而，在DML中，校准internal representations不同的DML模型会减少群组的多样性，从而损害每个网络教导同伴的能力。</p>
<p>表5显示，when a feature L2 loss is introduced，DML变得不那么有效（DML对比DML，L2）。【在两个模型之间引入L2损失，就是减小两个网络的特征距离】</p>
<p>【KD散度的作用不是拉近距离吗—是，但不是拉近中间层提取的特征的距离，而是拉近预测结果的距离】</p>
<h2 id="4-10-半监督学习"><a href="#4-10-半监督学习" class="headerlink" title="4.10 半监督学习"></a>4.10 半监督学习</h2><p>我们最后在CIFAR-100和Market-1501中探索半监督学习。</p>
<p>对于CIFAR-100，我们随机选择训练集每个类别的一个子集（从10%到100%）作为有标签的数据，其余的作为无标签数据。对于Market1501，我们随机选择M个身份作为有标签的数据，M从100到751不等。</p>
<p>实验以3种不同的训练策略进行：</p>
<ol>
<li>只用单一网络对标签的数据进行训练；</li>
<li>只用DML对有标签的数据进行训练（DML-labelled）；</li>
<li>用DML对所有数据进行训练，其中分类损失只对标记数据进行计算，而KL损失则对所有训练数据进行计算（DML-all）</li>
</ol>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20211005221720.png"></p>
<p>从图6的结果中，我们可以看出：</p>
<ol>
<li>用DML训练两个网络都比单独训练的一个网络表现要好——这点和以前一样，但现在有不同数量的带标签数据。</li>
<li>与只对标签数据添加DML（DML-labelled）相比，DML-all通过使用基于KL-distance的模仿损失来利用未标记的数据，进一步提高了性能。当标记数据的百分比较小时，改善幅度更大。这证实了DML对监督和半监督的学习场景都有好处。</li>
</ol>
<h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>我们提出了一种简单且普遍适用的方法，通过在有peers的群组中训练深度神经网络并进行相互蒸馏来提高其性能。通过这种方法，我们可以获得compact的网络，其性能比那些从强大但静态的教师中蒸馏出来的网络更好。DML的一个应用是获得紧凑、快速和有效的网络。</p>
<p>我们还表明，这种方法也有希望提高大型强大网络的性能，以这种方式训练的网络群组可以组合成一个集合体，进一步提高性能。</p>
<h1 id="6-思考"><a href="#6-思考" class="headerlink" title="6. 思考"></a>6. 思考</h1><ol>
<li>个人理解：互学习本质上还是soft-target在起作用。和KD对比一下，两种soft-target来源不同罢了，KD的soft-target来自于强大的教师蒸馏后的结果，互学习soft-target来源，是利用学生之间存在差异可以互相提供信息的特性。</li>
<li>为什么不能产生相似特征：如果学生都一样了会影响soft-target的供给，因为互学习的soft-target本质上来源于学生的差异性</li>
<li>PPT里加一下和知识蒸馏中学生-老师模型的异同，然后在这部分加一下以上两点的思考。<ol>
<li>另外，二者目标不同：知识蒸馏的目的是产生更强的学生，直接目的是让学生和老师越来越像，方法除了在输出上做文章还可以在中间层做文章；但是互学习的目的虽然也是产生强大的学生，但是直接目的是让学生互相帮助获得soft-target一起进步。</li>
<li>因此一些知识蒸馏的做法不能用到互学习上，2</li>
<li>为什么效果更好，可能因为知识蒸馏的教师网络还不够强大，蒸馏后的soft-target并没有直接使用多个学生来的全面，学生网络增加效果变好可以侧面验证这一点</li>
</ol>
</li>
<li>优点：不只得到了结果验证了方法有效性，还对方法有效性进行了深入的分析</li>
<li>缺点：4.7有点糊：里面的ensemble和前面方法论中提到的dml_e需要区分性说明</li>
<li>缺点：不能在无监督学习中使用，虽然文章提到互学习可以用于半监督学习，而且带标签数据占比越小效果越好，但是这个效果是有限度的，不能扩展到无监督领域。因为损失函数包括分类损失和模仿损失两部分，分类损失引导模型走向正确的分类方向；如果是无监督情况，模型只会服从于groupthink，并不能产生正确的预测。</li>
<li>发表在CVPR上的版本比arxiv上的版本更详尽，增加了imagenet上的实验，增加了对模型相似性的讨论，增加了半监督学习上的应用，增加了分布式训练部分</li>
<li> 分布式的训练实质上进一步保证学生网络的同步，因为算法1实际上是有先后顺序的</li>
</ol>
<p>   但是，为什么同步训练会更好？<br>9. 能不能用小网络对网络蒸馏来提高大网络性能？可以但没必要，因为这样需要训练大网络，但是知识蒸馏的初衷本身就是大网络难以训练。</p>
<ol start="10">
<li>共献<ol>
<li> 训练高效的小网络</li>
<li> 进一步提升大网络性能</li>
<li> 容易扩展到多网络学习</li>
<li> 容易扩展到半监督学习</li>
</ol>
</li>
</ol>
<p><a href="https://www.jiqizhixin.com/articles/2019-07-31-3">参考</a></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>NUS入学需知</title>
    <url>/NUS%E5%85%A5%E5%AD%A6%E9%9C%80%E7%9F%A5/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li>附件0 清单<ul>
<li>步骤</li>
<li>NUS有用链接</li>
<li>疫情通知</li>
</ul>
</li>
<li>附件1 校历<ul>
<li>总时间表</li>
<li>活动时间表</li>
<li>军训</li>
</ul>
</li>
<li>附件2 课程及其他要求<ul>
<li>课程要求</li>
<li>Seminar要求</li>
<li>QE</li>
<li>论文提交</li>
<li>英语要求</li>
<li>EG5911R: Information Literacy Skills for Research</li>
</ul>
</li>
<li>附件3 体检<ul>
<li>入学前体检</li>
<li>学生准证申请体检</li>
<li>注意事项</li>
</ul>
</li>
<li>附件4 2021-2022学年学费</li>
<li>附件5 国际生须知<ul>
<li>出国前<ul>
<li>入境 Immigration</li>
<li>入学</li>
<li>资产</li>
<li>私人物品</li>
<li>生活费预算</li>
<li>住宿</li>
</ul>
</li>
<li>抵达新加坡<ul>
<li>海关申报表</li>
<li>相关通知</li>
</ul>
</li>
<li>前往NUS(Kent Ridge)</li>
<li>抵达NUS</li>
<li>其他事务<ul>
<li>保险</li>
<li>医疗</li>
<li>大使馆/最高委员会登记</li>
<li>银行</li>
<li>国际生服务ISS</li>
</ul>
</li>
<li>出发前核对清单<ul>
<li>完成表格</li>
<li>重要文件</li>
<li>申请宿舍</li>
<li>工作日/周末后到达</li>
<li>国际生介绍会</li>
<li>Visa需求(仅适用于ICA网站上所列出的国家)</li>
<li>行程安排</li>
<li>免疫检查(可选)</li>
<li>居家通知和检测制度</li>
<li>体检</li>
<li>医保</li>
<li>财务准备</li>
</ul>
</li>
<li>注意事项</li>
</ul>
</li>
<li>附件6 更多信息<ul>
<li>候选期 Period of candidature</li>
<li>居住要求 Residency requirement</li>
<li>医保计划 Medical Insurance Scheme</li>
<li>电子设备</li>
<li>手册和论文</li>
</ul>
</li>
<li>附件7 在线答复offer</li>
<li>附件8 奖学金</li>
<li>附件9 其他<ul>
<li>地图</li>
</ul>
</li>
</ul>
<hr>
<h1 id="附件0-清单"><a href="#附件0-清单" class="headerlink" title="附件0 清单"></a>附件0 清单</h1><h2 id="一、步骤"><a href="#一、步骤" class="headerlink" title="一、步骤"></a>一、步骤</h2><p>收到offer后，</p>
<ol>
<li><p>在 <a href="https://inetapps.nus.edu.sg/GDA2">https://inetapps.nus.edu.sg/GDA2</a> 完成<strong>在线答复</strong>。</p>
</li>
<li><p>你会收到 NUS Graduate School 的指示，请参考 Registrar’s Office Guideline 给的<a href="https://nus.edu.sg/registrar/docs/info/registration-guides/registration-guide-for-graduate-research-students.pdf">注册指南</a> 在截止日期前完成状态更新：</p>
<p>完成海外旅行申报单<strong>Overseas Travel Declaration (OTD)</strong>(自2021.6起)</p>
<ul>
<li>对于新生：<br>   请用以下方式登录<a href="https://myaces.nus.edu.sg/OverseasTravelDecl/applicantLogin">OTD申请系统</a><ol>
<li>申请号</li>
<li>申请密码/PIN</li>
<li>选择学生类型</li>
</ol>
</li>
<li>对于老生：<br>   使用NUSNET ID登录</li>
</ul>
</li>
<li><p>必须完成<strong>注册（part1）</strong> <a href="https://myregistration.nus.edu.sg/">https://myregistration.nus.edu.sg/</a> </p>
<p>注册系统从<strong>2021-6-14</strong>开放，使用GDA2申请号和密码登录pre-registration系统。</p>
<p>需要更新的资料和声明如下：</p>
<ul>
<li>学生行为守则Code of Student Conduct</li>
<li>犯罪记录声明</li>
<li>健康和support声明</li>
<li>可接受的IT资源使用政策Acceptable Use Policy for IT Resources</li>
<li>不端性行为惩罚</li>
<li>NUS拒收电话政策(Do-Not-Call)</li>
<li>NUS知识产权政策</li>
<li>NUS学生隐私政策</li>
<li>费用补贴(Fee Subsidy)申报</li>
<li>承诺书(适用于奖学金获得者)</li>
<li>解除债务</li>
<li>医疗程序授权</li>
<li>Appointment of Local Representative</li>
</ul>
</li>
<li><p>入境申请(Entry Approval)：<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">STO-STP网站</a></p>
</li>
<li><p>获得有效护照，雇佣护照(适用于part-time学生)，IPA信(如果有必要的话，部分国家又要求)。</p>
<p>带上关键文件的原件，如成绩单、出生证明/身份证/护照等，以及录取信中列出的任何未完成的文件。</p>
</li>
<li><p>注册诊断英语考试Diagnostic English Test(DET)（如果offer里列出你需要参加的话）</p>
<ol>
<li>在线注册时间：2021.7.2-2021.7.8</li>
<li>参加考试：2021.7.21(<a href="http://www.nus.edu.sg/celc/">地点</a>)</li>
</ol>
</li>
<li><p>在每个新的学期前需要进行在线modules注册（新学期开始两周前开始module注册）。</p>
<p>请关注系办公室(Department Office)通知。</p>
<p>module(s)的选择需要和导师/部门讨论。</p>
</li>
<li><p>在新加坡国立大学健康中心(UHC)进行入学前体检(如果您没有在本国完成的话)(在注册前完成)</p>
</li>
<li><p>教授迎新讲座(Faculty’s Orientation Talk)(具体细节8月前会发邮件确认)。</p>
</li>
<li><p>参加适用于国际生的国际生迎新电子简报会(<a href="http://nus.edu.sg/osa/studentlife/international-students/briefing">OSA website</a>)</p>
</li>
<li><p>向系办公室(Department Office)报到完成**注册(part2)**：具体细节7月初会通知你。</p>
<ul>
<li>收(Collect)学生卡</li>
<li>从office那里获得讲座时间表</li>
<li>参加部门迎新讲座(Department’s Orientation Talk)(请和您的部门确认)</li>
</ul>
</li>
<li><p>向导师报到。</p>
<p>导师会对你的科研给出指导和更多的信息。（仅适用于有指定导师的学生）<a href="https://myregistration.nus.edu.sg/">https://myregistration.nus.edu.sg/</a></p>
</li>
</ol>
<h2 id="二、有用链接"><a href="#二、有用链接" class="headerlink" title="二、有用链接"></a>二、有用链接</h2><ul>
<li><p>RO 注册办公室(Registrar’s Office)</p>
<p><a href="http://www.nus.edu.sg/registrar/">http://www.nus.edu.sg/registrar/</a></p>
</li>
<li><p>NUS研究生院(Graduate School)：</p>
<p><a href="https://nusgs.nus.edu.sg/">https://nusgs.nus.edu.sg/</a></p>
</li>
<li><p>OGP 工程学院-研究生课程办公室(Faculty of Engineering – Office of Graduate Programmes)</p>
<p><a href="https://www.eng.nus.edu.sg/graduate/">https://www.eng.nus.edu.sg/graduate/</a></p>
</li>
<li><p>NUS图书馆</p>
<p><a href="http://libportal.nus.edu.sg/frontend/ms/central-library/about-central-library">http://libportal.nus.edu.sg/frontend/ms/central-library/about-central-library</a></p>
</li>
<li><p>OSA 学生事务办公室(Office of Student Affairs)</p>
<p><a href="http://www.nus.edu.sg/osa/">http://www.nus.edu.sg/osa/</a></p>
</li>
<li><p>UHC 大学健康中心(University Health Centre)</p>
<p>  <a href="http://www.nus.edu.sg/uhc/">http://www.nus.edu.sg/uhc/</a></p>
</li>
<li><p>大学健康中心(Graduate&amp;Non-Graduate保险计划)</p>
<p> <a href="http://www.nus.edu.sg/uhc/services/billing-insurance/insurance-matters">http://www.nus.edu.sg/uhc/services/billing-insurance/insurance-matters</a></p>
</li>
<li><p>NUS校园地图</p>
<p> <a href="http://map.nus.edu.sg/">http://map.nus.edu.sg/</a></p>
</li>
</ul>
<h2 id="三、疫情通知"><a href="#三、疫情通知" class="headerlink" title="三、疫情通知"></a>三、疫情通知</h2><p>鉴于COVID-19的现状，学生需要检查：</p>
<ol>
<li>国际学生需要查看 <a href="https://www.moh.gov.sg/covid-19">卫生部Ministry of Health(MOH)网站</a> 上的”Measures which apply to Inbound Travellers”，规划如何入境。</li>
<li>定期查看收件箱中NUS的邮件，关注最新动态。</li>
<li>参考以下NUS网站:<ol>
<li>应对COVID-19的其他措施 <a href="https://www.nus.edu.sg/registrar/academic-activities/registration/administrative-matters">RO注册办公室</a></li>
<li><a href="https://emergency.nus.edu.sg/">OSHE</a></li>
<li>关于保险：<a href="https://nus.edu.sg/uhc/general-health/billing-insurance/insurance-matters">大学健康中心UHC</a></li>
</ol>
</li>
</ol>
<hr>
<h1 id="附件1-校历"><a href="#附件1-校历" class="headerlink" title="附件1 校历"></a>附件1 校历</h1><h2 id="一、总时间表"><a href="#一、总时间表" class="headerlink" title="一、总时间表"></a>一、总时间表</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210508001604.png"><br><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210508002007.png"></p>
<h2 id="二、活动时间表"><a href="#二、活动时间表" class="headerlink" title="二、活动时间表"></a>二、活动时间表</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210508092125.png"></p>
<p><a href="http://www.nus.edu.sg/ModReg">http://www.nus.edu.sg/ModReg</a></p>
<p><a href="http://www.nus.edu.sg/registrar/academic-activities/special-term">http://www.nus.edu.sg/registrar/academic-activities/special-term</a></p>
<p><a href="http://www.nus.edu/sg/commencement">http://www.nus.edu/sg/commencement</a></p>
<h2 id="三、军训"><a href="#三、军训" class="headerlink" title="三、军训"></a>三、军训</h2><p>针对来自SAF/SPF/SCDF的本科NSmen，学院跟国防部、内政部一起组织了ICT训练(In-Camp Training)，该训练仅在学校假期的一段特殊时间段内进行。</p>
<p>ICT期间学校不会安排或进行任何强制性的学术项目。需要参加ICT的本科NSmen不会因为学术承诺而获得延期。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210508093229.png"></p>
<hr>
<h1 id="附件2-课程及其他要求"><a href="#附件2-课程及其他要求" class="headerlink" title="附件2 课程及其他要求"></a>附件2 课程及其他要求</h1><p>针对MEng/EngD/PhD学生的课程及其他要求，以下内容仅列出与PhD相关部分。</p>
<h2 id="一、课程要求"><a href="#一、课程要求" class="headerlink" title="一、课程要求"></a>一、课程要求</h2><p>候选人需就其感兴趣的领域进行研究，并在考试的最长期限前提交论文。除了论文需求，候选人还需要参加规定数量的课程，这些课程是导师认为对候选人有用的。</p>
<p>课程组成部分：</p>
<ol>
<li><p>模块学分(Modular Credits)：</p>
<p>每个包括39课时的研究生模块(module)，都被分配了4个模块学分(MC)。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Each graduate module of 39 lecture hours is to be assigned 4 modular credits (MC).</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>等级和绩点<br>字母等级对应绩点如下所示：</p>
<table>
<thead>
<tr>
<th align="left">Grade</th>
<th align="left">Grade Point</th>
</tr>
</thead>
<tbody><tr>
<td align="left">A+</td>
<td align="left">5.00</td>
</tr>
<tr>
<td align="left">A</td>
<td align="left">5.00</td>
</tr>
<tr>
<td align="left">A-</td>
<td align="left">4.50</td>
</tr>
<tr>
<td align="left">B+</td>
<td align="left">4.00</td>
</tr>
<tr>
<td align="left">B</td>
<td align="left">3.50</td>
</tr>
<tr>
<td align="left">B-</td>
<td align="left">3.00</td>
</tr>
<tr>
<td align="left">C+</td>
<td align="left">2.50</td>
</tr>
<tr>
<td align="left">C</td>
<td align="left">2.00</td>
</tr>
<tr>
<td align="left">D+</td>
<td align="left">1.50</td>
</tr>
<tr>
<td align="left">D</td>
<td align="left">1.00</td>
</tr>
<tr>
<td align="left">F(Fail)</td>
<td align="left">0</td>
</tr>
</tbody></table>
<p><strong>Phd要求CAP必须&gt;=3.50</strong></p>
</li>
<li><p>学术警告、留校察看、退学与毕业 </p>
<ol>
<li><p>关于继续学习(continuation)：</p>
<ul>
<li>在第一学期的学习中，CAP不得低于1.50；or，</li>
<li>对于PhD学生： CAP不得连续两学期低于3.00；or， CAP不得连续三学期低于3.50。</li>
</ul>
<p>CAP不满足要求的学生会被建议退学。</p>
<p>任一学期中，如果CAP低于毕业要求(PhD要求3.50以上)，学生会收到学术警告或者留校察看通知。此外，在资格考试(QE)、科研或者其他课程要求中表现不佳的学生同样可能会收到学术警告或者留校察看通知。</p>
</li>
<li><p>关于毕业(Graduation)：</p>
<p><strong>针对ECE的PhD学生</strong></p>
<ul>
<li>获得至少24 MCs（不包括博士研讨会(Doctoral Seminars)和研究生英语课程模块），这24个MCs必须是该学科或者相关科目的研究生水平，须经系里批准。MCs包括<strong>2个EE5000模块</strong>、<strong>2个EE6000模块</strong>，和<strong>2个无限制模块</strong>。<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">每个模块module (约39h的课时) 包括4个模块学分MC，即1模块=4MC。</span><br><span class="line">毕业需要完成24个MC，等于6个模块。</span><br><span class="line">这6个模块包括2个EE5000模块、2个EE6000模块、2个任意模块。</span><br></pre></td></tr></tbody></table></figure></li>
<li>4 MCs(2x2) 的EE6990 Research Attachment需要获得令人满意的成绩。</li>
<li>最好的6个模块(=24 MCs)的CAP&gt;=3.50.</li>
<li>通过资格考试（Qualifying Examination, QE）。</li>
<li>通过Phd论文和口头考核。</li>
<li>在研究生英语课程（中等水平）中获得最低为C的等级。</li>
<li>在ES5101 Technical Communication for Engineers（0 MC, graded）中获得最低为C的等级。</li>
<li>在EG5911R Literacy Skills for Research中获得令人满意的成绩。</li>
<li>在Doctoral Seminars中获得令人满意的成绩。</li>
</ul>
</li>
</ol>
</li>
</ol>
<p><em>注意：CAP = Cumulative Average Point累计绩点 = Sum(Grade Pt x MC)/Sum(MC)</em></p>
<p>对于尚未达到最小毕业MC的学生而言，所有字母等级需要换算成CAP。</p>
<p>而对于已达毕业MC的学生，the CAP is computed using the best modules equivalent to minimum number of modular credits(inclusive of foundation/core modules, where required)。评级为F的模块不获得任何MC。</p>
<p>在计算完成课程需求的绩点时，CELC模块（比如，研究生英语课程）不被计入其中。</p>
<p>所有课程一般会在周一至周五的晚上6:00-9:00进行。关于需要上啥课，Candidates需要和导师讨论并获得导师的批准。</p>
<h2 id="二、Seminar要求"><a href="#二、Seminar要求" class="headerlink" title="二、Seminar要求"></a>二、Seminar要求</h2><p>所有研究型学生都需要参加研讨会(Seminar)。在你注册成一个NUS的学生后，请从你的Department/Division处获得关于这部分要求的更多信息。</p>
<h2 id="三、QE"><a href="#三、QE" class="headerlink" title="三、QE"></a>三、QE</h2><p>除了课程要求，所有工程学院的博士候选人都需要参加并通过资格考试(QE)，该考试包括以下内容：</p>
<ol>
<li>综合资格考试；</li>
<li>提交一份30-40页的关于正在进行的科研的研究报告；</li>
<li>在至少3个成员（包括你的导师）组成的小组面前，完成有关你研究报告的口头答辩。</li>
</ol>
<p><strong>关于QE的时间：</strong></p>
<p>对于全日制和非全日制的直博候选人，综合资格考试将在第一年内（自申请开始之日起？）进行，而口头资格考试需要在24个月（2年）内通过。</p>
<p>未通过QE的候选人将被提名到研究生委员会，终止其资格（退学）。</p>
<p><strong>针对获得NUS研究型奖学金的学生</strong><br>自通过口头资格考试之日起，学生可以每月额外获得$500的助学津贴。</p>
<p>追加的款项将在奖学金申请的第四年/截止日期（取较早者）内发放。</p>
<h2 id="四、论文提交"><a href="#四、论文提交" class="headerlink" title="四、论文提交"></a>四、论文提交</h2><p>对于PhD学生：within 4 years of your candidature to be in time for the degree to be conferred to you by 5 years.</p>
<p>更详细的指导请参考：<a href="https://myportal.nus.edu.sg/studentportal/nusgs/gd/docs/Thesis%20Submission%20and%20Examination.pdf">https://myportal.nus.edu.sg/studentportal/nusgs/gd/docs/Thesis%20Submission%20and%20Examination.pdf</a></p>
<h2 id="五、英语需求"><a href="#五、英语需求" class="headerlink" title="五、英语需求"></a>五、英语需求</h2><p>针对需要参加DET考试的学生，但雨我无瓜。</p>
<h2 id="六、EG5911R"><a href="#六、EG5911R" class="headerlink" title="六、EG5911R"></a>六、EG5911R</h2><p>EG5911R: Information Literacy Skills for Research</p>
<p>自2013/14的第一学期起，PhD学生被要求注册这个模块。该模块需在第一学期进行。该模块为工程研究的学生提供如何高效查找期刊论文的背景知识。是有关剽窃问题和实验室安全的最好实践。</p>
<p>该模块是完成毕业要求的一部分。它是0 MCs课程，使用令人满意的完成/不令人满意的完成（CS/SC）的标准来评分。学生需要获得CS。</p>
<p>图书馆指导员仅提供一个上午的有关期刊搜索的讲座。</p>
<p>对于PhD学生而言，该模块需要在口头资格考试前完成。</p>
<hr>
<h1 id="附件3-体检"><a href="#附件3-体检" class="headerlink" title="附件3 体检"></a>附件3 体检</h1><p>入学条件是健康且没有器官疾病，在入学前，所有学生都必须完成强制的预入学体检。</p>
<h2 id="一、入学前体检"><a href="#一、入学前体检" class="headerlink" title="一、入学前体检"></a>一、入学前体检</h2><p>在入学前，所有研究生都必须经过下述的pre-enrolment体检：</p>
<ul>
<li><p>当地学生（略）</p>
</li>
<li><p>国际生</p>
<p>除了NUS pre-enrolment体检，申请学生准证(Student’s Pass)还需要进行HIV检测。</p>
<ul>
<li>HIV检测和体检可以在原国家或者新加坡完成。原始的HIV检测和胸部X光检查报告，必须和用于学生准证的ICA体检报告一起，在发布后三个月内上交到移民与关卡局(Immigration &amp; Checkpoints Authorit, ICA)。 <strong>注意：体检报告、测试报告和X光报告都必须是英文版的。胸部X光报告上需要有学生的姓名、身份证号、出生日期等个人信息。</strong></li>
<li>如果学生选择在UHC做NUS入学前体检和学生准证体检，在GST（消费税）后费用是$55(包括胸部X光合HIV检查)。请注意检查结果只会在5个工作日后给出。</li>
<li>请带上 <a href="https://www.ica.gov.sg/docs/default-source/ica/forms/medical-examination-report.pdf">ICA学生准证体检表</a> 和 <a href="http://www.nus.edu.sg/uhc/docs/default-source/default-document-library/graduate.pdf?sfvrsn=7f37a850_2">入学前体检表</a>(包括第一部分的自我声明和第二部分的医生证明) 以供医生填写。请参考<a href="http://www.nus.edu.sg/uhc/services/medical-examination/pre-admission/pre-admission-medical-exam">UHC官网</a>的最新版表格，不要使用旧版。</li>
</ul>
</li>
</ul>
<p>选择在UHC进行学生准证体检的同学必须进行医疗预约，以便在ICA Off Site Enrolment (OSE)前至少3天上传体检报告。</p>
<p>从2021.4起，学生可以在UHC进行预约。请参考<a href="http://www.nus.edu.sg/uhc/services/medical-examination/pre-admission/pre-admission-medical-exam">UHC官网</a>获取更多信息。</p>
<p>针对入学前体检，请参考<a href="https://nus.edu.sg/registrar/docs/info/registration-guides/registration-guide-for-graduate-research-students.pdf">注册指南</a>（<em>此为旧版</em>）第四页。如果你计划在UHC做这个体检，请参考 <a href="http://www.nus.edu.sg/uhc/services/medical-examination/pre-admission/pre-admission-medical-exam-proces">http://www.nus.edu.sg/uhc/services/medical-examination/pre-admission/pre-admission-medical-exam-proces</a> 获得如何在线预约的相关指引。</p>
<h2 id="二、STP申请体检"><a href="#二、STP申请体检" class="headerlink" title="二、STP申请体检"></a>二、STP申请体检</h2><p><strong>重要：只要在获得学生准证的In-Principal Approval(IPA)信后，才能进行旅行。</strong></p>
<p>对于国际生，如果要申请学生准证，（尤其当你要在自己国家完成体检时）你必须下载并提交由医生认证的两个表格</p>
<p>   <code>Admission Medical Examination Report for</code></p>
<ul>
<li><a href="https://www.nus.edu.sg/uhc/docs/default-source/default-document-library/graduate.pdf?sfvrsn=7f37a850_2">Graduate Students Form (UHC)</a>;</li>
<li><a href="https://www.ica.gov.sg/docs/default-source/ica/forms/medical-examination-report.pdf">ICA Student Pass Medical Examination Form</a></li>
</ul>
<p>对于具有SOLAR状态，IPA(Pending Document Submission)或 IPA(Pending Issuance Fee)的学生，必须在OSE前至少3天在网上提交未完成的材料和/或支付必要的费用。</p>
<p>体检包括以下内容：</p>
<ul>
<li>Physical检查</li>
<li>Laboratory检查*</li>
<li>胸部X光</li>
<li>HIV测试（仅适用于国际生）</li>
</ul>
<p>*<em>注意：女生请在例假结束后至少5-7天后再来体检，因为月经会影响尿检结果。</em></p>
<p>体检费用需要由学生自己负担。UHC的费用如下：</p>
<ul>
<li>标准体检（包括胸部X光和尿检）—$40.00</li>
<li>标准体检与学生准证体检（国际生）—$55.00</li>
</ul>
<p>学生准证的签发将视体检的结果而定。那些被发现患有活动性肺结核或HIV感染的人将不能获得相关通行证，因此，他们必须自费返回各自的国家。</p>
<h2 id="三、注意事项"><a href="#三、注意事项" class="headerlink" title="三、注意事项"></a>三、注意事项</h2><ol>
<li><p>总结一下就是国际生要交俩表：</p>
<ul>
<li><p><a href="https://www.nus.edu.sg/uhc/docs/default-source/default-document-library/graduate.pdf">Graduate Students Form (UHC)</a></p>
</li>
<li><p><a href="https://www.ica.gov.sg/docs/default-source/ica/forms/medical-examination-report.pdf">ICA Student Pass Medical Examination Form</a>。</p>
</li>
</ul>
<p>表1自己写part1。</p>
<p>在当地国际旅行卫生保健中心体检就行，提前电话预约，准备相关资料，带上1000现金，空腹。</p>
</li>
<li><p>鉴于当前COVID-19的国际形势，学生需要关注:</p>
<ol>
<li>对于国际生，请参考<a href="https://www.moh.gov.sg/covid-19">Ministry of Health (MOH) 官网</a>上的“Measures which apply to Inbound Travelers”内容，规划前往新加坡的行程。</li>
<li>定期检查邮箱中来自NUS的邮件以获得最新指示。</li>
</ol>
</li>
<li><p><code>E:\留学\准备事宜\附件3-1.pdf</code> = <a href="https://www.nus.edu.sg/uhc/docs/default-source/default-document-library/graduate.pdf">Graduate Students Form (UHC)</a></p>
</li>
</ol>
<hr>
<h1 id="附件4-学费"><a href="#附件4-学费" class="headerlink" title="附件4 学费"></a>附件4 学费</h1><p>政府资助的硕博项目</p>
<p>已更新，详情可见 <a href="https://www.nus.edu.sg/registrar/docs/info/administrative-policies-procedures/">https://www.nus.edu.sg/registrar/docs/info/administrative-policies-procedures/</a> =  <code>E:\留学\准备事宜\入学指南\2021-22学费.pdf</code>。</p>
<table>
<thead>
<tr>
<th align="left">学院</th>
<th align="left">新加坡居民</th>
<th align="left">新加坡永久居住者</th>
<th align="left">有服务义务(3,7)的国际生/研究生助教奖学金项目</th>
<th align="left">不在MOE资助范围内(1.2,4,5)的新加坡居民</th>
<th align="left">不在MOE资助范围内的新加坡永久居住者和国际生</th>
</tr>
</thead>
<tbody><tr>
<td align="left">工程学院</td>
<td align="left">9500</td>
<td align="left">14000</td>
<td align="left">21400</td>
<td align="left">38550</td>
<td align="left">40600</td>
</tr>
</tbody></table>
<p>注意事项：</p>
<ol>
<li><p>以上学费单位都是S$/y，由于需要逐年审查，因此可能有所调整。</p>
<ol>
<li><p>在2020/21学年入学的新加坡学生的授课型和研究型项目的剩余课程期间的学费计划将在2021年学年开始前审核。</p>
</li>
<li><p>在余下的课程期间，2020/21学年入学的新加坡永久居民(PR)和国际生(IS)的授课型和研究型项目的学费每年会分别增加1%和1.5%，capped at the dollar increase in AY2020.</p>
</li>
<li><p>自2019/20学年起，授课型学生如果延期毕业，在剩余的学期内需缴纳无资助的全款学费(无MOE版本)。</p>
<p><a href="http://www.nus.edu.sg/registrar/prospective-students/graduate/scholarship">延毕参考链接</a></p>
</li>
<li><p>对于建筑系硕士balabala，雨我无瓜。</p>
</li>
</ol>
</li>
<li><p>对于新加坡居民和永久居民balabala，雨我无瓜。</p>
</li>
<li><p>国际生应支付应缴费用的现行消费税GST(当前为7%)，上述学费已包含了GST。</p>
</li>
<li><p>无MOE资助的学生需缴纳上列的无资助版学费（已包含GST）。</p>
</li>
<li><p>之前在硕博项目中享受过政府资助或政府机构（比如Ministries, Public Service Commission和Statutory Board）资助的学生，需要参考 <a href="http://nus.edu.sg/registrar/docs/info/administrative-policies-procedures/gd-eligibility-guidelines.pdf">Eligibility Guidelines for MOE Subsidy</a> = <code>E:\留学\准备事宜\入学指南\附件4-1.pdf</code>。不符合MOE资助的学生也不符合大多数奖学金要求。获取更多信息请参考<a href="http://www.nus.edu.sg/registrar/prospective-students/graduate/scholarship">Scholarships/Awards</a>。</p>
</li>
<li><p><a href="http://www.skillsfuture.sg/enhancedsubsidy">SkillsFuture Mid-Career Enhanced Subsidy</a>，针对40+新加坡居民，雨我无瓜。</p>
</li>
<li><p><a href="http://www.nus.edu.sg/registrar/prospective-students/graduate/service-obligation-(so)-scheme">服务义务方案</a></p>
<p>服务义务包含在一些MOE资助项目中。基于工业需求，符合条件并签署了服务义务协议的国际生将减免学费。在服务义务下应支付的费用和教育部补贴的数额将反映在他们的<a href="http://www.nus.edu.sg/finance/NoticeonPaymentofFees.html">个人学生帐单</a>中。最终解释权贵学校所有。</p>
<ol>
<li>自2017/18学年起，获得MOE奖学金资助的国际生，如果有研究生助教奖学金项目(Graduate Assistantship Programme, GAP)要求—(包括NGSS、PGF、NUS研究型奖学金RS、TFA和SINGA)，则不需要签署服务协议。<strong>简言之，我奖学金的一部分属于NUS研究型奖学金，只需要做GAP项目，不用管服务协议。</strong></li>
<li>自2018/19学年起，中国人民大学(RUC)工商管理硕士(MBA)和房地产理学硕士(MRE)双学位项目的新录取国际生不符合MOE，因此，不签署服务协议。</li>
<li>学生如果违反协议需要付违约赔偿金(LD)，且每学年需要支付服务协议下的MOE资助总额，外加10%的复利。</li>
</ol>
</li>
<li><p>雨我无瓜的学院的学费查找途径。</p>
</li>
<li><p><a href="http://www.nus.edu.sg/registrar/academic-activities/special-term/graduate-special-term">特殊学期</a><br> 特殊学期中，课程计划的阅读模块的费用需学生支付。</p>
<p> 不过据说雨我无瓜。</p>
</li>
</ol>
<hr>
<h1 id="附件5-国际生须知"><a href="#附件5-国际生须知" class="headerlink" title="附件5 国际生须知"></a>附件5 国际生须知</h1><h2 id="一、出国前"><a href="#一、出国前" class="headerlink" title="一、出国前"></a>一、出国前</h2><h3 id="1-入境"><a href="#1-入境" class="headerlink" title="1. 入境"></a>1. 入境</h3><p>国际生如果想进入新加坡需要有效护照。对于全日制学生，学校会通过学生准证在线申请和注册系统(Student Pass On-Line Application &amp; Registration, <code>SOLAR</code>)为他们申请学生准证(Student Pass, <code>STP</code>)，该系统由移民与关卡局(Immigration &amp; Checkpoints Authority, <code>ICA</code>) 管理。如果学生接受了offer，我们会写信告诉他们如何完成最后的申请步骤。获得条件型offer(con offer)的学生需要在收到满足入学要求的确认后，才能开始申请学生准证。ICA会收取S$30的手续费。学生必须在提交<code>eForm 16</code>时通过信用卡或国际银行缴纳费用。如果没有付款，ICA将不处理该申请，申请将在SOLAR中继续等待。即使学生撤销申请，费用也不会退还。如果学生在提交了eForm 16、缴纳了申请费后发现姓名或者申请时期有误，需要重新申请并重新缴费。</p>
<p>对于入境需要<code>entry visa</code>的一些国家的同学（具体细节请参考<a href="https://www.ica.gov.sg/visitor/visitor_entryvisa">移民与关卡局</a>）,学校也会通过SOLAR系统为他们申请visa cum student pass。如果申请通过，我们会给他们寄<code>In-Principle Approval信</code>(ICA出具)。入境时，该信需要和有效护照一起递交给移民与关卡局的执勤人员(Duty Officer)。</p>
<p>在新加坡，国际生最快两周会收到<code>居留许可</code>(Social Visit Pass)。在这期间，学生在注册办公室(Registrar’s Office, RO)注册并获得<code>学生注册卡</code>(student registration card)后，必须带着In-Principle Approval信去ICA，换取他的学生准证。ICA官员将在NUS帮助学生完成其学生准则申请手续（日期在serialised信中给出）。未能在指定日期前完成的同学，需要在线预约并前往ICA大楼完成他们的学生准证申请。</p>
<p><strong>重要：只有通过<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">STO-STP网站</a>获得<code>入境许可</code>(Entry Approval)以及用于申请学生准证STP的In-Principal Approval(IPA)信(<code>STP-IPA</code>)后，才能安排旅行。</strong></p>
<p>有关STP和STP-IPA持有者的入境和逗留通知，请参考<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">这里</a>。</p>
<p>非全日制国际学生不符合学生准证的资格，必须持有有效的就业准证才能留在新加坡。</p>
<p><em><strong>体检</strong></em><br>所有获得用于申请6个月以上学生准证/居留许可的IPA的学生，在获得证件前必须先通过体检。没有通过体检的人会被自费遣返。<strong>国际生需要在他们的国家完成体检。</strong> 国际生需要要求医师完成两份体检报告(<code>NUS入学体检表</code>和ICA体检表)，但是只需要做一次体检。详情请参考附件3.</p>
<p>全日制研究生的配偶和/或子女(未婚且年龄在17岁以下)可以得到学校的资助，获得居留许可后留在新加坡。研究生须在大学注册并取得学生准证后才可提交申请。详情参考<a href="http://www.nus.edu.sg/registrar/administrative-policies-procedures/graduate/social-visit-pass-for-spouse-and-child-of-full-time-graduate-research-students">这里</a>。</p>
<p>有关入境事宜的详情，可以参考<a href="http://www.ica.gov.sg/">移民与关卡局官网</a>。</p>
<h3 id="2-入学"><a href="#2-入学" class="headerlink" title="2. 入学"></a>2. 入学</h3><p>请带上你的offer信，护照/身份证，学历证明(Education Certificates)和其他寄给你的必要文件（如果你在离开你的国家前收到他们的话）。</p>
<p>根据学校电子邮件的要求，在规定时间内到报到地点完成报到。</p>
<h3 id="3-资产"><a href="#3-资产" class="headerlink" title="3. 资产"></a>3. 资产</h3><p>在新加坡，新币(Singapore dollars, S$) 是当前使用的币种。1 dollars=100 cents。你带入新加坡的国际货币和旅行支票(traveller’s cheques)没有额度限制。你可以以<code>银行汇票(bank draft)</code>的形式带上一笔可观的钱，用于学费和住宿费用。大多数国际信用卡都是可以用的（比如American Express, Diners, Visa, MasterCard）。</p>
<p>请准备足够的现金，以支付旅行、交通、临时住宿和来时的生活开销（比如餐饮）。如果你带了现金，但是不能在你的国家换成新币，你可以到<code>Changi机场</code>的银行兑换柜台进行兑换，该柜台向所有航班开放。或者，你可以去银行或者外币兑换处(licensed money changer)。有关生活费的预算，请参考后面的第5条。此外，你需要研究向新加坡银行转账的程序，购买旅行支票,如有需要，准备好信用卡。最好使用在新加坡有分行的银行开具的新币银行汇票。请注意，海外银行开具的银行汇票大约需要3周才能兑现。</p>
<h3 id="4-私人物品"><a href="#4-私人物品" class="headerlink" title="4. 私人物品"></a>4. 私人物品</h3><ol>
<li>带上一打护照大小的照片(35mm x 43mm)。</li>
<li>非正式的轻便服装。值得一提的还是购物中心的衣服价格还算合理。</li>
<li>照相机、吹风机、洗漱用品、闹钟、雨伞、浴巾和计算器。可以自己带也可以在这里买，价格相对便宜。<strong>新加坡电压：230 volts, 50 cycles。</strong></li>
<li>请注意，违禁文献、管控药品、版权作品和音频(CSs/VCDs/DVDs)的复印版，一旦发现会被海关没收。电影、录像、磁带、书籍、报纸、杂志只有在获得有关部门的进口许可证或授权后才能进入。</li>
</ol>
<h3 id="5-生活费预算"><a href="#5-生活费预算" class="headerlink" title="5. 生活费预算"></a>5. 生活费预算</h3><p>除了学费，国际学生每月的开销预算如下：</p>
<table>
<thead>
<tr>
<th align="left">类别</th>
<th align="left">开销</th>
<th align="left">金额</th>
</tr>
</thead>
<tbody><tr>
<td align="left">住宿</td>
<td align="left">校内(每学期)：<br>NUS研究生宿舍 PGPR/GDR/<a href="http://www.nus.edu.sg/ohs/properties/utown/units_gr.php">uTown</a> <br><br><a href="http://www.nus.edu.sg/osa/has/other-accommodation">校外</a>(每月)：<br>租屋租金<br>公寓租金</td>
<td align="left">s$1980-6480（详情请参考<a href="http://www.nus.edu.sg/osa/has/graduate/hostel-rates">这里</a>）<br><br><br><br>S$500-800 <br>S$1800-2500</td>
</tr>
<tr>
<td align="left">食品(每月)</td>
<td align="left">学校食堂/美食街(Food Courts)<br>校外餐饮</td>
<td align="left">S$250-400<br>S$400-800</td>
</tr>
<tr>
<td align="left">书籍/材料(每月)</td>
<td align="left">取决于所学课程</td>
<td align="left">S$200-350</td>
</tr>
<tr>
<td align="left">交通(每月)</td>
<td align="left">公交/MRT</td>
<td align="left">S$200-300</td>
</tr>
<tr>
<td align="left">私人开销(每月)</td>
<td align="left">洗漱化妆、衣服、生活用品、娱乐等</td>
<td align="left">S$250-350</td>
</tr>
<tr>
<td align="left">学生杂费(每学期)(含GST)</td>
<td align="left">学生服务费SSF(Lab-Based)<br>健康服务费HSF</td>
<td align="left">S$176.45(全日制)/132.15(非全日制)<br>S$67.95</td>
</tr>
</tbody></table>
<h3 id="6-住宿"><a href="#6-住宿" class="headerlink" title="6. 住宿"></a>6. 住宿</h3><p>国际生可以考虑住在NUS研究生宿舍或者校外的私人住所。</p>
<p>如果想申请NUS研究生宿舍，可以在<a href="http://nus.edu.sg/osa/has">这里</a>查看相关信息并完成在线注册。学生之后可以通过线上大学宿舍管理系统(UHMS)查看他们的申请结果。如果申请人在提交在线申请时遇到困难，可以在<a href="http://www.hosteladmission.nus.edu.sg/app/utils/login_form/redirect/ask">这里</a>提交你的问题。</p>
<p>如果想找校外的私人住所，可以在<a href="http://nus.edu.sg/osa/has/other-accommodation">这里</a>获取一些有用的联系方式。</p>
<p>如果你在住宿服务方面需要帮助，可以看看<a href="http://www.nus.edu.sg/osa/has">学生事务办公室网站</a>或者在<a href="http://www.hosteladmission.nus.edu.sg/app/utils/login_form/redirect/ask">这里</a>提交你的问题。</p>
<h2 id="二、抵达新加坡"><a href="#二、抵达新加坡" class="headerlink" title="二、抵达新加坡"></a>二、抵达新加坡</h2><h3 id="1-海关申报表"><a href="#1-海关申报表" class="headerlink" title="1. 海关申报表"></a>1. 海关申报表</h3><p>完成<code>海关申报表(Customs Declaration form)</code>，你会在飞机上收到该表，请认真填写不要有遗漏。到达新加坡机场后，你要将该表和护照一起交给移民局官员(Immigration Officers)。</p>
<h3 id="2-相关通知"><a href="#2-相关通知" class="headerlink" title="2. 相关通知"></a>2. 相关通知</h3><p>有关学生准证STP和STP-IPA持有者的入境和居留通知，请参考<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">这里</a>获取详细信息。</p>
<h2 id="三、前往NUS-Kent-Ridge"><a href="#三、前往NUS-Kent-Ridge" class="headerlink" title="三、前往NUS(Kent Ridge)"></a>三、前往NUS(Kent Ridge)</h2><p>NUS(Kent Ridge)</p>
<ol>
<li>从Changi机场</li>
</ol>
<ul>
<li><p>乘坐地铁(MRT)。</p>
<p>从Changi Airport站出发，乘坐开往Joo Koon的列车（西线）。在Clementi MRT站下车。然后走到Clementi公交车换乘站，乘坐96路公交车前往校园。</p>
<p><strong>注意：MRT规定乘客行李不能超过81cm x 58cm x30cm。</strong></p>
</li>
<li><p>乘坐出租车。</p>
<p>车费大约S$30。如果从机场出发，需要额外收取S$3.5-5。从午夜到6 a.m.，需要加收计费表上50%的深夜附加费。高峰期和公共假期的附加费也是如此。更多细节请参考<a href="https://www.cdgtaxi.com.sg/web/corp/ride-with-us/fares">这里</a>。</p>
</li>
</ul>
<ol start="2">
<li>从公交总站(对于来自Malaysia的公交)</li>
</ol>
<ul>
<li><p>Golden Mile 购物中心(Beach Road)</p>
<p>走到Victoria街(10min)，乘坐33路公交车前往校园。在Clementi路上的工程学院下车。</p>
</li>
<li><p>Ban San街</p>
<p>走到Victoria街，乘坐33路公交车前往校园。在Clementi路上的工程学院下车。</p>
<p>或者，走到Bugis MRT站，乘坐列车前往Clementi MRT站，然后在Clementi公交换乘点乘坐96路公交车前往校园。</p>
</li>
<li><p>Kallang Bahru / Lavender Street Junction</p>
<p>走到Lavender MRT站，乘坐MRT前往Clementi MRT站，再乘坐96路公交车前往校园。</p>
</li>
</ul>
<ol start="3">
<li><p>从Railway站<br>从Woodlands Train Checkpoint乘坐911/913路公交车前往Woodlands MRT站，乘坐MRT前往Clementi MRT站，然后走到CLementi公交换乘点乘坐96路公交车前往校园。</p>
</li>
<li><p>从Mass Rapid Transit(MRT)站<br>从Clementi MRT站走到Clementi公交换乘点，然后坐96路公家车前往NUS。</p>
</li>
</ol>
<h2 id="四、抵达NUS"><a href="#四、抵达NUS" class="headerlink" title="四、抵达NUS"></a>四、抵达NUS</h2><ol>
<li><p>按照计划入住您的住处或者亲友的家。</p>
</li>
<li><p>Register：在预定的时间和地点报到，领取<code>学生注册卡(Student Registration Card)</code>。</p>
</li>
<li><p>报到后，你需要在你的<code>居留许可Social Visit Pass</code>到期前至少4个工作日，前往一下任一地点申请<code>学生准证</code>。</p>
<ul>
<li><p>ICA booth</p>
<ul>
<li><p><strong>多功能体育馆1(Multi-Purpose Sports Hall 1, MPSH1)-临时</strong></p>
</li>
<li><p>为OSE(Off Site Enrolment)准备的文件清单如下:</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210515174608.png"></p>
</li>
</ul>
</li>
<li><p>Student’s Pass Issuance</p>
<ul>
<li>日期：2021 8.12&amp;13</li>
<li>时间：9:00 am-12:30 pm；2:00 pm-5:00 pm（8.13下午4:30提前结束）</li>
<li>地点Venue：MPSH1</li>
</ul>
</li>
<li><p>ICA</p>
<ul>
<li>ICA大楼 4th Storey, 10 Kallang Road Singapore 208718</li>
<li>电话：63916100</li>
<li>网址：<a href="http://www.ica.gov.sg/">http://www.ica.gov.sg/</a></li>
</ul>
</li>
</ul>
</li>
</ol>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">注意事项</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">如在2021.7.26-30在MPSH1-临时报到点报到，需要在OSE在线预约系统(Online Appointment Booking System for OSE)上提前预约日期和时间</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">在申请学生准证前，在<a href="https://eservices.ica.gov.sg/solar/index.xhtml">SOLAR网站上</a>完成付款并打印<a href="https://www.ica.gov.sg/reside/STP/apply/ihl">收据</a>；带上收据和其他ICA要求的文件、以及符合ICA<a href="https://www.ica.gov.sg/common/photo-guidelines">照片要求</a>的照片</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">学生没有在预约时间到达或者没有带<a href="https://www.ica.gov.sg/pass/studentpass/ihl">ICA要求的文件和收据</a>的会被拒绝</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">在OSE, MPSH1, 2021.7.26-30期间，也提供付费的摄影、复印、打印服务</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">错过预约的学生，或者没有按时取得体检报告，亦或是申请学生准证后计划离开新加坡的学生，建议通过<a href="https://icaeservices.ica.gov.sg/ibook/index.do">线上预约</a>在ICA总部申请他们的学生准证</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">ICA会登记申请人的虹膜图像，请不要佩戴有色、有图案、装饰作用的隐形眼镜。学生需要摘下眼镜，方可拍摄红膜图像</td>
</tr>
<tr>
<td align="left">7</td>
<td align="left">在前往ICA大楼前，需要在<a href="https://eservices.ica.gov.sg/solar/index.xhtml">ICA网站</a>上进行线上预约。你的护照将在一周内退回，此外还要带上你的IPA信(须有大学认可)</td>
</tr>
</tbody></table>
<p>以下列出申请学生准证时需要提交的材料：</p>
<ul>
<li>在SOLAR上提交的eForm 16的打印版(要求签名并附上1张近期拍摄的护照大小的白底彩色照片)</li>
<li>有效护照和居留许可(Social Visit Pass)</li>
<li>离机/入境卡(Disembarkation/Embarkation Card)—进入新加坡时获得</li>
<li>IPA信</li>
<li>学生登记卡(Student Registration Card)</li>
<li>ICA体检表 &amp; 实验室报告(包括X-光合HIV血检报告)</li>
<li>学生准证的条款 &amp; 条件(Term &amp; Condition)的复印件</li>
<li>费用<ul>
<li>手续费：S$30</li>
<li>发证费：S$60</li>
<li>更换费 (遗失学生准证时补领)：S$100</li>
<li>多次入境签证(Multiple-Entry Visa) (适用于需要签证的国民)：S$30</li>
</ul>
</li>
</ul>
<p>3.1 延长逗留时间</p>
<p>如果你想在学生准证过期后再在新加坡待一小段时间(少于两周)，你可以在学生准证过期前，通过<code>Visitor Services Centre, ICA(4th Storey)</code>申请居留许可(Social Visit Pass)。你会被要求出示你已确定的机票来证明你的离境时间。</p>
<p>如果你的居留许可到期了，你会被视为在新加坡非法居留，你需要缴纳罚款，否则会被起诉。</p>
<p>3.2 注销Cancellation</p>
<p>如果学生在学校的学习提前终止（比如退学或拒绝再入学），需要带上学生准证，在停止学业后7天内前往ICA办理注销手续。</p>
<p>如果你关于学生准证或居留许可有任何疑问，请咨询学生事务办公室(Office of Student Affairs)的负责国际生服务(ISS)的工作人员，或者发邮件给<code>international@nus.edu.sg</code>。</p>
<ol start="4">
<li><p>在注册办公室(Registrar’s Office)报到后，需到系办公室(Department Office)报到。</p>
<p>（不适用于Biomedical Eng, Civil &amp; Environmental Eng, and Industrial Systems Eng &amp; Management）</p>
</li>
<li><p>向你的导师报到，他可能会提供一些指导或者关于科研的信息。</p>
</li>
<li><p>在跟<code>导师/系里</code>讨论过你应该参加的课程类型后，在系办公室处拿<code>lectures时间表</code>。</p>
</li>
<li><p>在你选择的银行开户。详见下述的”其他事务”部分。</p>
</li>
<li><p>参加<code>Orientations</code>：</p>
<ol>
<li>工程学院副院长关于硕博项目的<code>Orientation Talk</code>（具体细节之后会邮件通知）。</li>
<li>国际生Orientation（请关注学生事务办公室网站上的相关内容<a href="http://nus.edu.sg/osa/student-life/international-students/briefing%EF%BC%89%E3%80%82">http://nus.edu.sg/osa/student-life/international-students/briefing）。</a></li>
<li>图书馆Orientation，熟悉学校的设备和服务。</li>
</ol>
</li>
</ol>
<h2 id="五、其他事务"><a href="#五、其他事务" class="headerlink" title="五、其他事务"></a>五、其他事务</h2><h3 id="1-保险"><a href="#1-保险" class="headerlink" title="1. 保险"></a>1. 保险</h3><p>研究生保险计划(Graduate Insurance Scheme)是所有全日制国际生都必须参加的。</p>
<p><strong>COVID-19 保险</strong><br><a href="http://nus.edu.sg/uhc/docs/default-source/insurance/frequently-asked-questions-on-covid-19.pdf">保险范围</a></p>
<h3 id="2-医疗"><a href="#2-医疗" class="headerlink" title="2. 医疗"></a>2. 医疗</h3><p>UHC(University Health Centre)的诊所提供医疗服务。此外，Government Polyclinic、Outpatient Dispensary也提供医疗服务。</p>
<h3 id="3-大使馆-高级委员会登记"><a href="#3-大使馆-高级委员会登记" class="headerlink" title="3. 大使馆/高级委员会登记"></a>3. 大使馆/高级委员会登记</h3><p>抵达新加坡后，学生可以向其<code>驻新加坡大使馆/高级委员会</code>报到。在离家前，学生可以了解一下其驻新加坡大使馆/高级委员会(High Commission)提供的设施和服务的相关信息。</p>
<h3 id="4-银行"><a href="#4-银行" class="headerlink" title="4. 银行"></a>4. 银行</h3><p>银行通常在周一至周五的8:30 am-4:30 pm; 周六的8:30 am-1:00pm营业。</p>
<p>学校附近的银行(Clementi Branch)：</p>
<ul>
<li><p>Development Bank of Singapore (DBS)</p>
<p> Blk 450, Clementi Avenue 3</p>
<p> #01-293/295</p>
<p> Singapore 120450</p>
<p> Tel: (65) 1800-111 1111</p>
</li>
<li><p>United Overseas Bank (UOB)</p>
<p> Blk 450, Clementi Avenue 3</p>
<p> #01-287/289</p>
<p> Singapore 120450</p>
<p> Tel: (65) 1800-222 2121</p>
</li>
<li><p>Post Office Savings Bank (POSB)</p>
<p> Blk 449, Clementi Avenue 3</p>
<p> #01-243</p>
<p> Singapore 120449</p>
<p> Tel: (65) 6339 6666</p>
</li>
<li><p>Oversea-Chinese Banking     Corporation (OCBC)</p>
<p> 3155 Commonwealth Avenue West</p>
<p> #04-52/55</p>
<p> The Clementi Mall</p>
<p> Singapore 129588</p>
<p> Tel: (65) 6530 5930</p>
</li>
</ul>
<p>校内的ATMs分别位于</p>
<ul>
<li>Faculty of Arts &amp; Social Sciences (LT9附近)</li>
<li>Faculty of Science (LT27和S16外面，Science Canteen附近)</li>
<li>Yusof Ishak House (Level 3)</li>
<li>National University Hospital</li>
</ul>
<h3 id="5-国际生服务ISS"><a href="#5-国际生服务ISS" class="headerlink" title="5. 国际生服务ISS"></a>5. 国际生服务ISS</h3><p>ISS部门的建立是为了监督国际生在大学的福利。有关入境immigration、财务、学业、社交、健康、招聘和其他事务，该部门能提供咨询和帮助。国际生的个人问题也能得到其关注。</p>
<p>如果需要更多信息和帮助，可以联系他们：</p>
<ul>
<li>电话：(65) 6516 4817</li>
<li>传真：(65) 6778 5902</li>
<li>邮箱：<a href="mailto:international@nus.edu.sg">international@nus.edu.sg</a></li>
</ul>
<p>有关国际生迎新简报会、文化活动、离校简报会以及社会活动等项目的一般问题：</p>
<ul>
<li>Mr Alvin Wong </li>
<li>邮箱：<a href="mailto:Alvin.wong@nus.edu.sg">Alvin.wong@nus.edu.sg</a></li>
<li>地址：International Student Services, Office of Student Affairs (3rd Storey, Yusof Ishak House)<br>National University of Singapore<br>31 Lower Kent Ridge Road, Singapore 119078</li>
<li>网站：<a href="http://www.nus.edu.sg/osa/">http://www.nus.edu.sg/osa/</a></li>
<li>工作时间: <ul>
<li>周一至周四 8.30 am – 6.00 pm</li>
<li>周五 8.30 am –– 5.30 pm</li>
<li>周末及季节日不工作</li>
</ul>
</li>
</ul>
<h2 id="六、出发前核对清单"><a href="#六、出发前核对清单" class="headerlink" title="六、出发前核对清单"></a>六、出发前核对清单</h2><h3 id="1-完成表格"><a href="#1-完成表格" class="headerlink" title="1. 完成表格"></a>1. 完成表格</h3><p>在规定的截止日期前，完成录取材料并把它交给注册办公室Registrar’s Office。</p>
<h3 id="2-重要文件"><a href="#2-重要文件" class="headerlink" title="2. 重要文件"></a>2. 重要文件</h3><ul>
<li>有效签证 &amp; 雇佣证明(仅适用于非全日制)</li>
<li><strong>通过<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">STO-STP网站</a>和IPA信(如果需要的话)获得入境许可(Entry Approval)</strong></li>
<li>原始成绩单</li>
<li>出生证明/护照</li>
<li>offer信</li>
<li>等</li>
</ul>
<h3 id="3-申请宿舍"><a href="#3-申请宿舍" class="headerlink" title="3. 申请宿舍"></a>3. 申请宿舍</h3><p>在线提交宿舍申请，收到OSA(住宅服务)发的有关分配结果的邮件。有关住宿的具体细节可以参考 <a href="http://www.nus.edu.sg/osa/has%E3%80%82">http://www.nus.edu.sg/osa/has。</a></p>
<h3 id="4-工作日或周末后到达"><a href="#4-工作日或周末后到达" class="headerlink" title="4. 工作日或周末后到达"></a>4. 工作日或周末后到达</h3><p>如果你成功申请到了硕博生宿舍，请将你的预计到达日期和时间告知你所分配到的住处，并安排延迟入住(如果必要)。如果未能申请到宿舍，请提前安排好youth hostle, hotel或者亲友家之类的住所。</p>
<h3 id="5-国际生介绍会"><a href="#5-国际生介绍会" class="headerlink" title="5. 国际生介绍会"></a>5. 国际生介绍会</h3><p>请注意，国际生可以参加<a href="http://nus.edu.sg/osa/student-life/international-students/briefing">OSA网站</a>上的国际生Orientation电子简报会。根据我们的经验，参加了该介绍会的学生，在后续的适应和居住方面会少些困难。</p>
<h3 id="6-Visa需求"><a href="#6-Visa需求" class="headerlink" title="6. Visa需求"></a>6. Visa需求</h3><p>仅适用于<a href="http://www.ica.gov.sg/">ICA网站</a>上所列出的国家</p>
<p>如果你来自有visa需求的国家，需要通过SOLAR完成visa的线上申请，并打印Form 16。请与你的入境部门确认你是否需要申请出境visa。</p>
<p><strong>提交form16后，如果审核通过，你会收到进入新加坡所需的IPA(In-Principal Approval)信。</strong></p>
<h3 id="7-行程安排"><a href="#7-行程安排" class="headerlink" title="7. 行程安排"></a>7. 行程安排</h3><p>通过信誉良好的traval agency安排你的行程，在offer信中给出的规定日期抵达。</p>
<p>如果有必要，填好并上交更改报到日期/时间的表格。</p>
<h3 id="8-免疫检查-可选"><a href="#8-免疫检查-可选" class="headerlink" title="8. 免疫检查(可选)"></a>8. 免疫检查(可选)</h3><p>根据世界卫生组织WHO的规定，在新加坡旅行、居住需要接受防疫要求。你可能需要带上你的医疗记录、眼科处方、或药品信息的副本。</p>
<h3 id="9-居家通知和检测制度"><a href="#9-居家通知和检测制度" class="headerlink" title="9. 居家通知和检测制度"></a>9. 居家通知和检测制度</h3><p>Stay-Home Notice and Testing Regime</p>
<p>请参考<a href="https://safetravel.ica.gov.sg/stpl/requirements-and-process">这里</a>获得更多有关学生准证STP和STP-IPA持有者的入境许可及居家通知的相关信息。</p>
<h3 id="10-体检"><a href="#10-体检" class="headerlink" title="10. 体检"></a>10. 体检</h3><p>申请学生准证和入学前体检都需要一份体检报告。该报告必须以英文形式完成。你可以选择在新加坡或者在本国完成体检。体检报告不得早于提交前三个月以上出具。（请参考附件3获取更多信息）</p>
<h3 id="11-医保"><a href="#11-医保" class="headerlink" title="11. 医保"></a>11. 医保</h3><p>根据个人需要，在NUS就读期间，你可以参加额外的医疗保险。对于全日制学生而言，除非有其他医疗/保险计划的证明，否则必须参加NUS的<code>health insurance scheme</code>。更多详情请参考<a href="http://www.nus.edu.sg/uhc/services/billing-insurance/insurance-matters">这里</a>。</p>
<h3 id="12-财务准备"><a href="#12-财务准备" class="headerlink" title="12. 财务准备"></a>12. 财务准备</h3><p>请准备好足够的金钱带去新加坡，包含旅行开销、临时居住和抵达后的生活费。（详情请参考page 2的Int’l Students for estimated cost）</p>
<p>研究如何向新加坡银行转账的程序。如有需要，可以购买旅行支票(traveller’s cheques)、准备信用卡。最好选择在新加坡有分行的银行开具新加坡币的银行汇票(bank draft)。汇票上需要印有新加坡分行地址。请注意，向海外银行开出的银行汇票需要大约3周才能兑现。</p>
<p><em>注意：(可选)你可能希望安排personal and credit references，这在租房和在银行开户等方面都很有用</em></p>
<h3 id="七、注意事项"><a href="#七、注意事项" class="headerlink" title="七、注意事项"></a>七、注意事项</h3><ul>
<li><p>更多国际生需注意的信息，请参考<a href="http://www.nus.edu.sg/registrar/academic-activities/registration/information-for-international-students">注册办公室网站</a></p>
</li>
<li><p>鉴于COVID-19，学生需要：</p>
<ul>
<li>查看<a href="https://www.moh.gov.sg/covid-19">卫生部(Ministry of Health, MOH)网站</a>上<code>Measures which apply to Inbound Travelers</code>内容，安排前往新加坡的行程。</li>
<li>定期检查来自NUS的邮件以获得最新通知。</li>
<li>参考以下NUS网站：<ul>
<li>应对COVID-19的其他措施: <a href="https://www.nus.edu.sg/registrar/academic-activities/registration/administrative-matters">RO网站</a></li>
<li><a href="https://emergency.nus.edu.sg/">OSHE网站</a></li>
<li>关于保险: <a href="https://nus.edu.sg/uhc/general-health/billing-insurance/insurance-matters">UHC网站</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="附件6-更多信息"><a href="#附件6-更多信息" class="headerlink" title="附件6 更多信息"></a>附件6 更多信息</h1><h2 id="一、候选期"><a href="#一、候选期" class="headerlink" title="一、候选期"></a>一、候选期</h2><p>你的候选人资格于学期第一天(如，2021.8.2)或报到那天（二者取其晚）开始。MEng的候选期最长为3年。EngD或PhD的候选期最长为5年。你的论文提交时间不得晚于最长候选期。</p>
<p>请注意，除非学校同意，否则学生不得同时作一个以上学位的候选人，也不得同时作NUS和其他学校或机构的候选人。</p>
<h2 id="二、居住要求"><a href="#二、居住要求" class="headerlink" title="二、居住要求"></a>二、居住要求</h2><p>不在新加坡居住的候选人必须在候选期内完成居住要求，MEng和EngD/PhD的居住要求分别是6个月和18个月。</p>
<h2 id="三、医保计划"><a href="#三、医保计划" class="headerlink" title="三、医保计划"></a>三、医保计划</h2><p>所有全日制的学生都必须参加团体医保计划，该计划可以帮他们支付由于住院/手术、意外而产生的医疗费用(部分或全部)。对于研究学者(research scholars)，我们将按学期从他们的津贴中扣除保险费。</p>
<p>更多信息请参考<a href="http://www.nus.edu.sg/uhc/services/billing-insurance/insurance-matters">这里</a></p>
<h2 id="四、电子设备"><a href="#四、电子设备" class="headerlink" title="四、电子设备"></a>四、电子设备</h2><p>建议学生带上一台可用的笔记本电脑到学校里使用。因为NUS大部分区域都覆盖免费Wi-Fi，因此电脑将会是有用的。有时，你的课程会要求你使用笔记本电脑并下载资料，以便上课时实操。当然你还需要带上一部手机。</p>
<h2 id="五、手册和论文"><a href="#五、手册和论文" class="headerlink" title="五、手册和论文"></a>五、手册和论文</h2><p>建议你熟悉下列的候选期要求：</p>
<ul>
<li><a href="http://www.nus.edu.sg/registrar/administrative-policies-procedures/graduate/nus-statutes-and-regulations">NUS章程</a>(Statutes of the National University of Singapore)</li>
<li>研究生手册，<a href="https://myportal.nus.edu.sg/studentportal/eng/gd/">下载链接</a></li>
<li><a href="http://www.nus.edu.sg/registrar/administrative-policies-procedures/graduate/acceptance-record#IP">NUS知识产权政策</a>(NUS Intellectual Property Policy)</li>
</ul>
<hr>
<h1 id="附件7-在线答复offer"><a href="#附件7-在线答复offer" class="headerlink" title="附件7 在线答复offer"></a>附件7 在线答复offer</h1><p>这部分已经搞过了，所以简写一下~</p>
<p>在研究生招生系统中在线答复offer：<a href="https://inetapps.nus.edu.sg/GDA2">https://inetapps.nus.edu.sg/GDA2</a></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210519205448.png"></p>
<p>有关研究生招生系统的更多细节请参考以下链接：<a href="https://nus.edu.sg/registrar/docs/info/academic-activities/registration/applicant-guide-for-graduate-admission-system.pdf">Applicant Guide For Graduate Admission System</a></p>
<hr>
<h1 id="附件8-奖学金"><a href="#附件8-奖学金" class="headerlink" title="附件8 奖学金"></a>附件8 奖学金</h1><p><em>仅适用于已获得的学生</em></p>
<ul>
<li>校长奖学金 PRESIDENT’S GRADUATE SCHOLARSHIP</li>
<li>NUS研究型奖学金 NUS RESEARCH SCHOLARSHIP</li>
<li>联邦奖学金 COMMONWEALTH SCHOLARSHIP</li>
</ul>
<p>PhD学生奖学金期限最长为4年。奖学金有效期为1年，每半年/一年需延长一次。然而，奖学金随时可能由于表现不佳而暂停。</p>
<p>2014/15年起，获得NUS研究型奖学金、校长奖学金、联邦奖学金及以下列出的其他财政资助的国际学生，需要满足奖学金条款与条件中所附的<strong>研究生助教计划GAP</strong>的要求。</p>
<table>
<thead>
<tr>
<th align="left">奖学金</th>
<th align="left">条款与条件</th>
</tr>
</thead>
<tbody><tr>
<td align="left">NUS Research Scholarship (with GAP)</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-research-scholarship-terms-conditions/">https://nusgs.nus.edu.sg/nus-research-scholarship-terms-conditions/</a></td>
</tr>
<tr>
<td align="left">NUS Research Scholarship (GAP exempted)</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-research-scholarship-terms-conditions-gap-exempted/">https://nusgs.nus.edu.sg/nus-research-scholarship-terms-conditions-gap-exempted/</a></td>
</tr>
<tr>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-research-scholarship-terms-conditions-gap-exempted/President's">President’s</a> Graduate Fellowship (with GAP)</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/president-graduate-fellowships-terms-conditions/">https://nusgs.nus.edu.sg/president-graduate-fellowships-terms-conditions/</a></td>
</tr>
<tr>
<td align="left">President’s Graduate Fellowship(GAP exempted)</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/president-graduate-fellowship-terms-conditions-gap-exempted/">https://nusgs.nus.edu.sg/president-graduate-fellowship-terms-conditions-gap-exempted/</a></td>
</tr>
<tr>
<td align="left"><a href="https://nusgs.nus.edu.sg/president-graduate-fellowship-terms-conditions-gap-exempted/Commonwealth">Commonwealth</a> Scholarship</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions/">https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions/</a></td>
</tr>
<tr>
<td align="left">Tuition Fee Allowance (GAP exempted)</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions-gap-exempted/">https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions-gap-exempted/</a></td>
</tr>
<tr>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions-gap-exempted/Tuition">Tuition</a> Fee Allowance</td>
<td align="left"><a href="https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions/">https://nusgs.nus.edu.sg/nus-tuition-fee-allowance-terms-conditions/</a></td>
</tr>
</tbody></table>
<p>如果候选人目前有工作，其奖学金只有在他暂停从当前雇主处领取薪水后才开始发放。</p>
<p>如果你在新加坡或马来西亚工作，在你注册前请提交一封信说明你最后一天的工资。这封信应该是你的雇主所写，或者是你给雇主的辞职信的副本。建议你在雇佣期的最后一天后再进行注册。否则，你会被认为是自费生，并被要求支付学费。</p>
<p>如果你希望被录取到大学的全日制研究型项目，而你正在接受奖学金或奖金，亦或是你已经获得了scholarship/award for which you are bonded，你需要取得有关部门的批准并向我们提交相关证明文件。</p>
<p>学生需要在新加坡的银行开户。不同的银行有不同的要求，也有不同类型的储蓄和支票账户。如果想获得更多信息，请浏览各自银行的网站。当你决定好在哪个银行开户时，请确保你带了足够的现金来支付费用。以下是你开户所需的文件：</p>
<ul>
<li>护照(原版和影印版)</li>
<li>学生准证 或 STP-IPA信(In-Principle student’s pass approval letter)</li>
<li>学生卡(如果有的话) 或者 来自NUS的录取通知(Offer Letter of Admission)</li>
<li>最低限度的初始存款(具体多少取决于银行的要求)</li>
</ul>
<p>已获得奖学金的学生，需要在<a href="https://myedurec.nus.edu.sg/">Education Records System (EduRec)</a>更新你的信用银行账户信息，以避免延迟发放奖学金。</p>
<p>有关银行开户的更多信息，请参考学生事务办公室网页: <a href="http://www.askstudentservice.nus.edu.sg/app/answers/detail/a_id/2042">http://www.askstudentservice.nus.edu.sg/app/answers/detail/a_id/2042</a></p>
<p>如果你在规定的日期完成注册，你会在2021.8.31收到第一笔津贴。如果没有，你就只能再2021.9.18收到8月和9月份的津贴。建议国际生在收到津贴前带些钱(支付1-1.5个月的开销)。</p>
<p>有关生活费的预算请参考附件5。</p>
<hr>
<h1 id="附录9-其他"><a href="#附录9-其他" class="headerlink" title="附录9 其他"></a>附录9 其他</h1><table>
<thead>
<tr>
<th align="left">部门/项目办公室</th>
<th align="left">位置</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Office of Graduate Programmes, Faculty of Eng[1]</td>
<td align="left">EA 06-16</td>
</tr>
<tr>
<td align="left">Biomedical Eng[3]</td>
<td align="left">E4 04-08</td>
</tr>
<tr>
<td align="left">Chemical &amp; Biomolecular Eng[4]</td>
<td align="left">E5 02-09</td>
</tr>
<tr>
<td align="left">Civil &amp; Env Eng[2]</td>
<td align="left">E1A 07-03</td>
</tr>
<tr>
<td align="left">Electrical &amp; Computer Eng[3]</td>
<td align="left">E4 05-45</td>
</tr>
<tr>
<td align="left">Industrial Systems Eng &amp; Management[2]</td>
<td align="left">E1A 06-25</td>
</tr>
<tr>
<td align="left">Materials Science &amp; Eng[1]</td>
<td align="left">EA 03-09</td>
</tr>
<tr>
<td align="left">Mechanical Eng[1]</td>
<td align="left">EA 07-08</td>
</tr>
</tbody></table>
<ul>
<li><p>University Health Centre[5]    </p>
<ul>
<li>UHC Clinic (medical examination)</li>
</ul>
</li>
<li><p>Yusof Ishak House[6]</p>
<ul>
<li>Level 1: Student Service Centre (paying fees)</li>
<li>Level 3: Office of Student Affairs (apply student pass)</li>
</ul>
</li>
<li><p>E6 Techno Edge Canteen</p>
</li>
<li><p>EW1 Engineering Workshop 1</p>
</li>
<li><p>EW2 Engineering Workshop 2</p>
</li>
</ul>
<p><strong>NUS Faculty of Engineering, 10 Kent Ridge Crescent</strong></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210520004227.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210520004411.png"></p>
<h2 id="一、地图"><a href="#一、地图" class="headerlink" title="一、地图"></a>一、地图</h2><p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/blog_files/img/Pic-Github/OpenGLconfig/20210520004643.png"></p>
<p><a href="http://www.nus.edu.sg/campusmap/">最新地图</a></p>
<p><a href="http://www.nus.edu.sg/oca/Transport-and-Parking/Getting-around-NUS.html">校内班车 internal shuttle bus services</a></p>
]]></content>
      <categories>
        <category>留学</category>
      </categories>
      <tags>
        <tag>留学</tag>
        <tag>NUS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：paying more attention to attention</title>
    <url>/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9Apaying-more-attention-to-attention/</url>
    <content><![CDATA[<h2 id="Paying-more-attention-to-attention-improving-the-performance-of-convolutional-neural-networks-via-attention-transfer"><a href="#Paying-more-attention-to-attention-improving-the-performance-of-convolutional-neural-networks-via-attention-transfer" class="headerlink" title="Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer"></a>Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</h2><h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><h2 id="1）-NLP："><a href="#1）-NLP：" class="headerlink" title="1） NLP："></a>1） NLP：</h2><p>自然语言处理</p>
<h2 id="2）-encoder-decoder："><a href="#2）-encoder-decoder：" class="headerlink" title="2） encoder-decoder："></a>2） encoder-decoder：</h2><p>   编码器：现实问题转化为数学问题；</p>
<p>   解码器：求解数学问题，并转化为现实世界的解决方案；</p>
<p>   <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/20210912005008.png"></p>
<ul>
<li>主要应用：文字-文字；音频-文字；图片-文字</li>
<li>主要问题：中间向量长度必须固定，导致输入信息过长时会丢失部分信息。</li>
<li>解决方案：attention机制，中间向量被编码成了一个向量的序列：</li>
</ul>
<pre><code>![](https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/20210912005634.png)
</code></pre>
<h2 id="3）-seq2seq："><a href="#3）-seq2seq：" class="headerlink" title="3） seq2seq："></a>3） seq2seq：</h2><p>encoder-decoder的一种。输入一个序列，输出另一个序列；这两个序列的长度都可变</p>
<h2 id="4）-attention"><a href="#4）-attention" class="headerlink" title="4） attention"></a>4） attention</h2><p>   attention最早是2014年neural machine translation中提出的，该文章在seq2seq中引入attention，将attention分为soft attention和hard attention两种。</p>
<p>   <a href="https://www.bilibili.com/video/BV1Wi4y1b7NS">李宏毅 attention based model</a></p>
<p>   <a href="https://www.zhihu.com/question/68482809">参考</a></p>
<p>   <a href="https://zhuanlan.zhihu.com/p/91839581">一文看懂 Attention（本质原理+3大优点+5大类型）</a></p>
<p>   在人类的视觉系统中，attention机制是将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。</p>
<p>   在AI领域，attention最早在cv里用，后来伴随着BERT和GPT的优秀在NLP领域大放异彩。</p>
<ul>
<li><p>优点：</p>
<ul>
<li>参数少</li>
<li>速度快</li>
<li>效果好</li>
</ul>
</li>
<li><p>原理：</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/SLAM/20210912105322.png"></p>
<p> 第一步：query和key计算相似度得到权值</p>
<p> 第二步：将权值归一化得到可用的权重</p>
<p> 第三步：将权重和value加权求和</p>
</li>
<li><p>类型：</p>
<p> <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912105629.png"></p>
<p> 值得一提的是其中的<code>CNN+Attention</code>：</p>
<p> 首先我们先分析一下CNN：CNN的卷积操作可以提取重要特征，attention差不多也是这种思想，但是CNN的卷积感受野是局部的，需要叠很多层去扩大视野。此外，CNN中的max pooling直接提取数值最大的特征，有点像hard attention的直接选中某特征。</p>
<p> 将Attention机制引入CNN：（以2016年的<a href="https://zhuanlan.zhihu.com/p/48254913">ABCNN</a>为例和attention pooling）</p>
<ol>
<li><p>在卷积操作前做attention。比如attention-based BCNN-1，这个任务是文本蕴含任务需要处理两段文本，同时对两段输入的序列向量做attention，计算出特征向量，再拼接到原始向量中，作为卷积层的输入（等于卷积层的输入多了一些两段input的attention信息）</p>
</li>
<li><p>在卷积操作后做attention。</p>
<p>比如在ABCNN-2中，对两段文本的卷积层的输出做attention，作为pooling层的输入。</p>
</li>
<li><p>在pooling层做attention来取代max pooling。</p>
<p>比如attention pooling，首先用LSTM学习到一个比较好的句向量作为query，然后用CNN学习到一个特征矩阵key，再用query对key计算权重，进行attention，得到最后的句向量。</p>
</li>
</ol>
<ul>
<li>attention在CNN上的应用主要分为两种：spatial attention和channel attention。</li>
</ul>
<p> <code>纯attention</code>：比如attention is all you need里面的transformer，用的是self-attention</p>
</li>
</ul>
<h2 id="5）-CNN"><a href="#5）-CNN" class="headerlink" title="5） CNN"></a>5） CNN</h2><p>   <a href="https://zhuanlan.zhihu.com/p/251068800">CNN卷积核与通道讲解</a></p>
<h2 id="6）-transformer"><a href="#6）-transformer" class="headerlink" title="6） transformer"></a>6） transformer</h2><p>   encoder-decoder结构.</p>
<p>   由于RNN不能做并行，而CNN只能部分的提特征(除非叠很多层)，故使用self-attention来替代RNN。</p>
<p>   transformer，在原先的seq2seq中加入attention机制</p>
<p>   以前用seq2seq的地方都可以用transformer来替代。</p>
<p>   transformer进阶：使用multi-head self-attention；universal transformer…</p>
<p>   transformer也不是只能用在文字上，cv上也可以，比如self-attention GAN</p>
<p>   <a href="https://www.bilibili.com/video/av56239558">李宏毅-Transformer</a></p>
<p>   <a href="https://www.bilibili.com/video/BV19t411S74Z">【Transformer】Attention Is All You Need</a></p>
<h2 id="7）-知识蒸馏"><a href="#7）-知识蒸馏" class="headerlink" title="7） 知识蒸馏"></a>7） 知识蒸馏</h2><p><a href="https://www.bilibili.com/s/video/BV1s7411h7K2">神经网络知识蒸馏 Knowledge Distillation</a></p>
<p>步骤：</p>
<ol>
<li>采用传统的方式训练一个教师网络</li>
<li>建立学生网络模型，模型的输出采用传统的softmax函数，拟合目标为one-hot形式的训练集输出，输出和目标之间的距离记为loss1</li>
<li>将训练完成的教师网络的softmax分类器加入温度参数，作为具有相同温度参数softmax分类器的学生网络的拟合目标，将教师与学生网络的温度版softmax分类器之间的距离记为loss2</li>
<li>引入参数$\alpha$，将训练的损失函数设为$loss1*(1-\alpha)+loss2*\alpha$，训练网络</li>
</ol>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913144723.png"></p>
<p>  <img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913144841.png"> 其中CE为交叉熵。</p>
<p>可见t与平缓程度相关，增大t可以放大教师网络的暗知识；$\alpha$与学生网络训练时参考教师网络蒸馏来的知识的比重有关，该值越大，越依赖教师网络的暗知识。</p>
<h2 id="8）spatial-attention-和-channel-attention"><a href="#8）spatial-attention-和-channel-attention" class="headerlink" title="8）spatial attention 和 channel attention"></a>8）spatial attention 和 channel attention</h2><p><a href="https://blog.csdn.net/u014134327/article/details/109669042">Spatial Attention和Channel Attention的个人理解</a></p>
<p><a href="https://blog.csdn.net/weixin_42560055/article/details/85225111">参考2</a></p>
<p>attention机制本质上是学习一个权重分布，再拿这个权重分布施加在原来的特征图上，简单来讲：</p>
<p>(1) 这个加权可以是保留所有分量均做加权—soft attention；也可以是在分布中以某种采样策略选取部分分量—hard attention；</p>
<p>(2) 这个加权可以作用在空间尺度上，给不同的区域加权—spatial attention；也可以作用在channel尺度上，给不同的通道特征加权—channel attention；还可以给特征图上每个元素加权；</p>
<p>(3) 这个加权可以作用在不同时刻历史特征上，如machine translation</p>
<h2 id="9）迁移学习Transfer-learning"><a href="#9）迁移学习Transfer-learning" class="headerlink" title="9）迁移学习Transfer learning"></a>9）迁移学习Transfer learning</h2><p>知识蒸馏被广泛应用于模型压缩和迁移学习中，常见的迁移学习方式就是在一个大的数据集上先做预训练，然后使用预训练得到的参数在一个小的数据集上做微调（两个数据集往往领域不同或者任务不同）</p>
<p>例如：在联邦广告中，先通过大量曝光和点击数据进行 CTR 建模（大的数据集），然后通过知识蒸馏，将这部分信息迁移到 CVR 模型中（小的数据集）</p>
<h2 id="10）关于DL里面各种classifier线性层的称谓："><a href="#10）关于DL里面各种classifier线性层的称谓：" class="headerlink" title="10）关于DL里面各种classifier线性层的称谓："></a>10）关于DL里面各种classifier线性层的称谓：</h2><ul>
<li>linear：线性层。没有隐层的单层结构。</li>
<li>dense：密集层。可以指单层linear也可以指多层线性层的堆叠，可以没有隐层也可以有，隐层一般大于2。</li>
<li>MLP：多层感知机。多层linear的堆叠，有隐层。</li>
<li>FC：全连接层。单层多层均可，是对linear classifer的一种笼统称谓。<h2 id="11）top-1和top-5"><a href="#11）top-1和top-5" class="headerlink" title="11）top-1和top-5"></a>11）top-1和top-5</h2></li>
</ul>
<p>top-1：预测结果中排名第一的类别与实际结果相符的准确率；</p>
<p>top-5：预测结果中排名前五的类别包含实际结果的准确率。</p>
<h2 id="12）反向传播算法"><a href="#12）反向传播算法" class="headerlink" title="12）反向传播算法"></a>12）反向传播算法</h2><p>反向传播算法由两个阶段组成：激励传播+权重更新</p>
<ul>
<li>激励传播（每次迭代的传播阶段分两步）：<ul>
<li>前向传播：将训练输入送入网络以获得激励响应</li>
<li>反向传播：将激励响应同输入所对应的目标输出求差。以获得输出层和隐藏层的响应误差</li>
</ul>
</li>
<li>权重更新<ul>
<li>先将输入激励和误差响应相乘，获得权重的梯度</li>
<li>再将这个梯度乘上一个比例（训练因子）并取反后加到权重上<h2 id="13）正则化"><a href="#13）正则化" class="headerlink" title="13）正则化"></a>13）正则化</h2><a href="https://daimajiaoliu.com/daima/56a4f0d1f3c8808">深度学习下的正则化</a></li>
</ul>
</li>
</ul>
<p>我们需要通过最小化误差来拟合训练数据。正则化是解决高方差问题的方案之一，也是减小过拟合的方法。</p>
<ul>
<li>l0、l1、l2、l无穷: 限制模型的学习能力<ul>
<li>l2正则化：倾向于使网络的权值接近0，使得前一层神经元对后一层神经元的影响降低，实质上是对权值做线性衰减。</li>
</ul>
</li>
<li>其他方法：<ul>
<li>dropout：在训练过程中，对于每个神经元，概率p保持其为激活状态，概括1-p直接关闭。由于每个神经元都可能会被丢弃，模型训练的时候，模型不会给任何神经元过大的参数</li>
<li>early-stopping<h2 id="14）批正则化Batch-Normalization"><a href="#14）批正则化Batch-Normalization" class="headerlink" title="14）批正则化Batch Normalization"></a>14）批正则化Batch Normalization</h2></li>
</ul>
</li>
</ul>
<p>对数据做批正则化，使得数据满足均值为0，方差为1的正态分布，可以缓解CNN等训练中梯度消失或梯度爆炸的现象，加快模型的训练速度。</p>
<h2 id="15）automatic-differentiation自动微分"><a href="#15）automatic-differentiation自动微分" class="headerlink" title="15）automatic differentiation自动微分"></a>15）automatic differentiation自动微分</h2><p>现代dl系统（比如MXNet，TensorFlow等）都用到了自动微分技术。</p>
<p>自动微分是一种可以借由计算机程序计算一个函数的导数。</p>
<p>自动微分认为，所有数值计算归根结底是一系列有限的可微算子的组合，它是一种介于符号微分和数值微分的方法，本质上是一种图计算。</p>
<h2 id="16）ZCA白化"><a href="#16）ZCA白化" class="headerlink" title="16）ZCA白化"></a>16）ZCA白化</h2><p>白化是一种重要的数据预处理过程，目的是降低输入数据的冗余性，白化后的数据：</p>
<ul>
<li>特征间的相关性较低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p>白化处理分为PCA白化和ZCA白化。前者用于保证数据各维度的方差为1，可以降维也可以去相关；后者保证数据各维度的方差相同，主要用于去相关，且尽量使白化后的数据接近原始输入数据。</p>
<ul>
<li><p>PCA白化：使用PCA求出特征向量，然后把数据X映射到新的特征空间，以去除特征之间的相关性。</p>
</li>
<li><p>ZCA白化，把PCA白化的结果又变换到原来的坐标系下。</p>
</li>
</ul>
<h2 id="17）WRN：Wide-residual-networks"><a href="#17）WRN：Wide-residual-networks" class="headerlink" title="17）WRN：Wide residual networks"></a>17）WRN：Wide residual networks</h2><p>WRN-16-1，0.2M：表示深度16，宽度1，参数量0.2M</p>
<h2 id="18）seed：随机种子"><a href="#18）seed：随机种子" class="headerlink" title="18）seed：随机种子"></a>18）seed：随机种子</h2><p><a href="https://blog.csdn.net/qq_41375609/article/details/99327074">参考</a></p>
<h2 id="19）CUB数据集："><a href="#19）CUB数据集：" class="headerlink" title="19）CUB数据集："></a>19）CUB数据集：</h2><p>Caltech-UCSD Birds-200-2011，FGCV中广泛使用的数据集，包括鸟类的200个子类别的11788张图像，其中5994张用于训练，5794张用于测试。每张图像的注释包括：1 subcategory label, 15 Part Locations, 312 Binary Attributes, 1 Bounding Box。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913172708.png"></p>
<p>需要注意，此数据集的图像与ImageNet中的图像重叠，在使用ImageNet预训练网络时需要小心，因为CUB的测试集可能与原始网络的训练集重叠。</p>
<h2 id="20）Fine-Grained-Visual-Categorization"><a href="#20）Fine-Grained-Visual-Categorization" class="headerlink" title="20）Fine-Grained Visual Categorization"></a>20）Fine-Grained Visual Categorization</h2><p>细粒度视觉分类FGCV，即识别细分类的任务，一般需要同时使用全局图像信息与局部图像信息精准识别图像子类别。</p>
<h2 id="21）MIT-indoor-scene-classification数据集："><a href="#21）MIT-indoor-scene-classification数据集：" class="headerlink" title="21）MIT indoor scene classification数据集："></a>21）MIT indoor scene classification数据集：</h2><p>包括67个室内类别，共15620张图像。</p>
<p><a href="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913173104.png"></a></p>
<h2 id="22）ImageNet数据集"><a href="#22）ImageNet数据集" class="headerlink" title="22）ImageNet数据集"></a>22）ImageNet数据集</h2><p>ImageNet 是根据 WordNet 层次结构组织的图像数据集。WordNet 中的每个有意义的概念，可能由多个单词或单词短语描述，称为同义词集或 synset。</p>
<p>ImageNet 有大约 100 K 个同义词集，平均每个同义词集约有 1,000 个人工注释图像。 ImageNet 仅存储对图像的引用，而图像存储在互联网上的原始位置。在深度学习论文中，ImageNet-1K 是指作为 ImageNet 的大规模视觉识别挑战（ ILSVRC ）的一部分发布的数据集，用于将数据集分类为 1,000 个类别。</p>
<p>简而言之，就是一个巨大的图像数据集！</p>
<h1 id="0-摘要"><a href="#0-摘要" class="headerlink" title="0. 摘要"></a>0. 摘要</h1><p>本文是attention在知识蒸馏中的应用，即，如何定义CNN中的attention，使得学生网络可以通过mimic教师网络的attention maps来提高自己的性能。</p>
<p>为此，本文提出了几种transfer attention的方法，实验证实这些方法在多种数据集和CNN结构中<strong>都</strong>表现良好。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>作为人类，attention在visual experience中很关键，同时也与感知密切相关。</p>
<p>在AI方面，artificial attention mechanisms比较流行。</p>
<p>有一种假设是：存在non-attentional perception processes和attentional perception processes。前者可以在观察场景的过程中获取高层次的信息，将这些信息与其他思维过程相关联，可以帮助控制attention processes——将注意力集中于场景中的特定区域。从另一个角度来讲，不同的观察者（教师网络）会拥有不同的知识和不同目标，针对同一个场景，不同的attentional strategies看到的东西也会不一样。这是本文的启发点：人工视觉中的注意力有何不同？我们能否利用注意力信息来提高CNN的性能？更具体来讲，一个教师网络是否可以通过向另一个学生网络提供”它将注意力集中在哪里”的信息，来提高学生网络的性能？</p>
<p>为了解决上述问题，我们首先需要明确的点是：在一个给定的CNN中，注意力机制是如何定义的。我们可以认为注意力是一组spatial maps，这组maps试图encode on网络在做出输出决定时最关注的那些输入区；此外，这组maps可以根据网络的各个层来定义，以便它们能够同时capture低、中、高层的representation information。<strong>更具体来讲，我们定义了两类spatial attention maps：activation-based和gradient-based，然后探索了这两种注意力地图是如何在不同的数据集和网络结构中变化的，并说明了这两种注意力地图确实包含了一些有用信息（这些信息可以显著提高CNN性能）。为此，我们还提出了几种新颖的方法来将注意力从教室网络transfer到学生网络上，来提高后者的性能。</strong></p>
<ul>
<li>【换言之，本文首先定义了两种空间注意力地图，然后论证两种空间注意力地图都对提高CNN性能有帮助，然后提出了几种在知识蒸馏中transfer attention from teacher to students的办法】</li>
</ul>
<p>本文的贡献：</p>
<ul>
<li>我们认为注意力是一种将知识从一个网络转移到另一个网络的机制</li>
<li>我们提出了使用activation-based和gradient-based 空间注意力地图</li>
<li>我们通过实验表明，我们的方法在各种数据集和深度网络架构上都有显著的改进，包括残差和非残差网络</li>
<li>我们发现activation-based attention transfer比full-activation transfer有更好的改善，并且可以与知识蒸馏相结合</li>
</ul>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><h1 id="3-attention-transfer"><a href="#3-attention-transfer" class="headerlink" title="3. attention transfer"></a>3. attention transfer</h1><h2 id="3-1-activation-based-attention-transfer"><a href="#3-1-activation-based-attention-transfer" class="headerlink" title="3.1 activation-based attention transfer"></a>3.1 activation-based attention transfer</h2><p>考虑一个CNN层和对应的激活张量<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912231918.png">，C个通道（特征面），每个特征面维度是H x W。activation-based 映射函数F将该3d激活张量作为输入，输出一个2维的spatial attention  map：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912232300.png"></p>
<p>在定义这个空间注意力映射函数F之前，我们需要认可一个潜在的假设：hidden neuron activation（给定输入时网络预测出的结果）的绝对值可以用于指示该神经元的重要性。因此，我们可以通过计算A的绝对值在通道维度上统计结果来构建spatial attention map。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912233056.png"></p>
<p>具体来讲，本文使用以下spatial attention maps：</p>
<ul>
<li>绝对值求和：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912233315.png"></li>
<li>绝对值指数求和：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912233336.png"></li>
<li>绝对值指数求最大值：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912233351.png"></li>
</ul>
<p>我们可视化了多个数据集上的多种网络，包括ImageNet分类和定位，COCO目标检测，人脸识别，以及fine-grained识别。我们主要关注没有顶层dense linear layer的现代结构，比如Network-In-Network，ResNet和Inception，这些网络都是流线型的卷积结构。此外，我们还使用不同的框架，在相同结构、宽度和深度的网络上，通过训练得到了不同的性能。</p>
<p>我们发现，隐层激活的统计信息不但与图像层面上的预测目标具有空间相关性，而且相关性与准确度正相关，即准确度高的网络，相关性也高，更强大的网络有更尖锐的注意力。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210912235644.png"></p>
<p>此外，attention maps针对不同的网络层关注不同的部分。在前面的层，神经元的激活水平对低层次的梯度点来说很高（？）；在中间层，激活水平对最具辨识度的区域（比如眼睛、鼻子、轮廓）来说比较高；而在顶层则反映了完整的物体。例如，为人脸识别而训练的网络的中层注意图在眼睛、鼻子和嘴唇周围会有较高的激活，而顶层的激活将对应于全脸。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913014806.png"></p>
<p>对于上面说的不同的注意力映射函数，有一些属性上的小差别：</p>
<ul>
<li>与<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913015113.png">相比，<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913015139.png">对有更高激活的神经元的空间位置有更大权重，比如，对最具辨识度的部分设置了更大的权重（p越大，对高激活区域的关注度越高）；</li>
<li>此外，在同一个空间位置的所有神经元激活中，<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913015823.png">只会考虑其中之一来为该空间位置设置权重；相反，<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913015933.png">更倾向于多个神经元高激活的区域（？？？）。</li>
</ul>
<p>为了进一步说明这些函数的区别，我们可视化了3个性能不同的网络：Network-In-Network (62% top-1 val accuracy), ResNet-34 (73% top-1 val accuracy) and ResNet-101 (77.3% top-1 val accuracy)，每个网络取最后一个下采样前的activation maps，上图左边是中间层，右边是顶层平均池化前的激活图。顶层地图略模糊，因为其原始分辨率只有7*7，很明显，具有辨识度的区域有更高的激活级别。比如，狼的脸。当p减小时，可以看到形状细节消失了。</p>
<p>在attention transfer中，给定教师网络的spatial attention maps（该map使用上面的attention映射函数计算得到），我们的目标是训练一个学生网络，该网络不只能做正确的预测，还要有和教师相似的attention maps。通常，可以在每一层上计算attention maps的transfer losses。比如，在ResNet中，根据教师和学生的深度考虑以下两种情况：</p>
<ul>
<li>相同深度：在每个残差块后做attention transfer；</li>
<li>不同深度：在每个残差块组后做attention transfer。（如图）</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913022058.png"></p>
<p>类似的情况也可以应用于其他网络结构（比如NIN，一组是指一个3 × 3,1 × 1,1 × 1的卷积块）。</p>
<h3 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h3><p>我们假设转移损失被置于相同空间分辨率的学生和教师的注意力图之间，但如果需要，attention maps可以插值以匹配它们的形状（？？？）。</p>
<p>学生网络j-th激活图的向量表示：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913091853.png"></p>
<p>教师网络j-th激活图的向量表示：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913091931.png"></p>
<p>损失函数定义为：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913023100.png"></p>
<p>其中W是权重，<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913023219.png">是标准交叉熵【前面那部分可以理解为likelihood，后面那部分是教师网络给的先验知识？？】</p>
<p>attention transfer也可以和知识蒸馏相结合，损失函数定义为：<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913023825.png"></p>
<h1 id="3-2-Gradient-based-attention-transfer"><a href="#3-2-Gradient-based-attention-transfer" class="headerlink" title="3.2 Gradient-based attention transfer"></a>3.2 Gradient-based attention transfer</h1><p>我们将注意力定义为输入的梯度。这里输入的梯度可以看做输入敏感地图，输入空间位置对应的注意力编码了输出预测对该输入位置变化的灵敏度（比方说，如果一个像素的小变化可以对网络的输出造成较大的影响，我们就可以合理假设网络“paying attention to”那个像素）。下式定义了教师和学生的输入损失的梯度：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913093505.png"></p>
<p>然后，为了让学生的gradient attention与教师的更接近，我们最小化二者的距离：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913100632.png"></p>
<p>因为WT和x都是给定的，我们求上式关于Ws的导数：[??????????????]</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913100826.png"></p>
<p>所以，为了做一次更新，我们需要先做一个前向传播和反向传播来得到Js和JT（？？？），计算第二部分那个二范数误差，然后再做一次传播。第二次传播与这里的前向传播相似，包括二阶混合偏导数。上述计算类似于D&amp;L的双反向传播技术。此外，它可以在支持automatic differentiation的框架里实现，而在具有复杂图的现代框架中也可以实现。除了正向传播外，第二次反向传播的代价与第一次反向的代价差不多。<br>所以，为了做一次更新，我们需要先做一个前向传播和反向传播来得到Js和JT（？？？），计算第二部分那个二范数误差，然后再做一次传播。第二次传播与这里的前向传播相似，包括二阶混合偏导数。上述计算类似于D&amp;L的双反向传播技术。此外，它可以在支持automatic differentiation的框架里实现，而在具有复杂图的现代框架中也可以实现。除了正向传播外，第二次反向传播的代价与第一次反向的代价差不多。（？？？？？？）</p>
<p>我们还提出了要使gradient attention map上水平翻转不变。为了做到这点，我们将原图和水平翻转后的图像都做传播，反向传播并翻转gradient attention maps。在得到的attentions和输出的二范数损失，再做第二次BP：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913104353.png"></p>
<p>这与Cohen &amp;Welling(2016)的Group Equivariant CNN方法相似，但并不是硬性约束。我们实验发现，这对训练有正则化作用。</p>
<p>值得注意的是，在本文中我们只考虑输入层的梯度，但是一般情况下，可能需要在更高层的网络中考虑attention transfer和对称约束。</p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><p>我们在多种图像分类数据集上作做attention transfer。第一部分中，我们在CIFAR数据集上做activation-based attention transfer和gradient-based attention transfer的实验；在第二部分中，我们在更大的数据集上做activation-based attention transfer的实验。</p>
<p>对于activation-based attention transfer，我们使用相比于AlexNet或者VGG而言性能更好的NIN和ResNet-based结构（Wide Residual Network），在Scenes，CUB和ImageNet数据集上，我们使用ResNet-18和ResNet-34进行实验。</p>
<p>对于gradient-based attention transfer，由于需要复杂的自动微分，我们将实验限制在没有批正则化的NIN网络和CIFAR数据集上。</p>
<h2 id="4-1-CIFAR实验"><a href="#4-1-CIFAR实验" class="headerlink" title="4.1 CIFAR实验"></a>4.1 CIFAR实验</h2><p>CIFAR数据集，包含60000张彩色图片，分为10个类型，图片大小为32*32。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913140826.png"></p>
<p>下采样后，顶层的activations的分辨率会更小，因此看上去并没有很多的空间留给attention transfer。但有趣的是，即便在这种情况下，attention transfer依然可以提高所有cases的性能。我们使用水平翻转和随机剪裁来做数据增强，并且所有的网络都进行了批正则化。我们发现ZCA whitening对有效的准确率有负面影响，因此为了更简单的meanst normalization而省略了它。</p>
<p>我们将ResNet transfer的知识蒸馏的温度t提高到4(温度越高，概率分布越平滑)，使用 $\alpha=0.9$。</p>
<h3 id="4-1-1-Activation-based-attention-transfer"><a href="#4-1-1-Activation-based-attention-transfer" class="headerlink" title="4.1.1 Activation-based attention transfer"></a>4.1.1 Activation-based attention transfer</h3><p>在CIFAR-10数据集上，多种网络的attention transfer（使用<img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913145430.png">注意力地图）结果如表所示：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913145517.png"></p>
<ul>
<li>教师网络与学生网络具有相同的深度：WRN-16-1/WRN-16-2</li>
<li>教授网络与学生网络具有不同的深度：WRN-16-1/WRN-40-1; WRN-16-2/WRN-40-2</li>
</ul>
<p>表中数据单位是<code>Percentage error</code>，是使用不同的随机seed跑了5次后的平均分类错误率，AT都有显著提升(对比原students)，当AT+KD时，网络效果最好【！但是单纯的AT好像没有KD效果好】。</p>
<p>F-ActT是full-activation transfer</p>
<p>为了验证在WRN transfer中每组有至少一个activation-based attention transfer是否重要，我们分别在组1、组2和组3中训练了每个网络只有一个转移损失的三个网络，并与用所有三个损失训练的网络进行比较。相应的结果是8.11、7.96、7.97（单独的损失）和综合损失的7.93（使用WRN16-2/WRN-16-1作为教师/学生对）。每项损失都提供了某种程度的额外注意力转移。【<strong>这个步骤是为了验证每个group的AT都是有一些作用的</strong>】</p>
<p>我们还探索了使用WRN-16-1和WRN16-2对时哪种注意力映射函数表现最好，结果如下表所示：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913154439.png"></p>
<p>有趣的是，基于总和的函数效果非常相似，而且比基于最大值的函数更好【<strong>why？</strong>】。从现在起，我们将使用注意力映射函数F2的平方之和。至于公式2中的参数β，它通常变化在0.1左右，因为我们设置为：$10^3/attention map上的元素数量和每层的batch size$ 。在结合AT和KD的情况下，为了简化学习较难的例子，我们在transfer的过程中衰减了它。</p>
<h3 id="4-1-2-Activation-based-AT-对比-Transfering-full-activation"><a href="#4-1-2-Activation-based-AT-对比-Transfering-full-activation" class="headerlink" title="4.1.2 Activation-based AT 对比 Transfering full activation"></a>4.1.2 Activation-based AT 对比 Transfering full activation</h3><p>为了检查从full-activation转移信息是否比从attention map转移信息更有利，我们实验了FitNets-style hints，直接在全激活层上使用l2损失，用1×1卷积层来匹配张量形状，发现相比其他transfer，这种方法对学生baseline的改进是最小的（见表1的F-ActT列）。</p>
<p>对于相同宽度不同深度的网络，我们尝试直接回归到激活，没有1×1卷积。我们还在转移损失之前使用l2正则化，并在训练期间衰减β，因为这些都能提供更好的性能。如图所示，我们发现，AT和全激活转移一样，都大大加快了收敛速度，但AT相比full-activation transfer对准确率的提高效果更好。由此可见，attention maps相比于full activation，携带了更多的对transfer比较重要的信息。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913160555.png"></p>
<ul>
<li>bold line是测试结果，dashed line是训练结果，可以看到在WRN-16-2—WRN-16-1的transfer中，AT和F-ActT收敛都挺快，但是AT最终的结果略好一些。</li>
</ul>
<h3 id="4-1-3-Gradient-based-attention-transfer"><a href="#4-1-3-Gradient-based-attention-transfer" class="headerlink" title="4.1.3 Gradient-based attention transfer"></a>4.1.3 Gradient-based attention transfer</h3><p>为了简单起见，我们在这些实验中使用了thin Network-in-Network模型，并且没有做批量正则化中的随机剪裁，只是进行了水平翻转的数据增强【因为梯度求导啥的比较难算】。我们也只使用确定性的算法和固定种子的采样，所以实验结果来自单次运行的实验【前面是5次的平均】。我们发现，在这种设置下，网络已经很难适应训练数据了，所以即使在基线实验中也要关闭权重衰减。</p>
<p>在未来，我们计划使用批量归一化来探索师生对的gradient-based attention，因为到目前为止还不清楚批量归一化在gradient-based attention transfer中的第二个反向传播步骤中应该如何表现（例如，不知道它是有助于批量归一化参数，or is a separate forward propagation with fixed parameters needed【？？？】）。</p>
<p>我们探索了以下方法：</p>
<ul>
<li>最小化输入梯度的l2范数，即双重反向传播方法【双反向传播正则化使Jacobian 矩阵偏小】 </li>
<li>gradient attention map的对称范数（公式Lsym）</li>
<li>学生-老师gradient-based attention transfer（不加对称约束的gradient版本）</li>
<li>学生-老师activation-based attention transfer</li>
</ul>
<p>各种方法的结果显示在下表中：</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913163920.png"></p>
<p>有趣的是，仅仅最小化梯度的l2准则就已经很好了。另外，对称norm也是表现最好的注意力准则之一，我们计划将来在其他数据集上也进行研究。我们还观察到，与activation-based AT类似，使用gradient-based AT也会使性能提高。我们还在相同的训练条件下用activation-based AT训练了一个网络，结果在所有方法中表现最好。<strong>我们应该注意到，没有批量规范化的学生NIN的结构与教师网络略有不同，它在池化层之前没有ReLU激活，这导致没有批量规范化的性能更好</strong>。因此，为了达到activation-based AT的最佳性能，我们必须训练一个新的教师，在池化层之前没有ReLU激活，并且在卷积层的输出上有AT损失。【结果呢？】</p>
<h2 id="4-2-Large-input-image-network"><a href="#4-2-Large-input-image-network" class="headerlink" title="4.2 Large input image network"></a>4.2 Large input image network</h2><p>在本节中，我们在输入图像大小为224 × 224的ImageNet网络上进行了隐层activation-based transfer的实验。据推测，在这类网络中，注意力更为重要，因为注意力地图的空间分辨率更高。</p>
<h3 id="4-2-1-transfer-learning"><a href="#4-2-1-transfer-learning" class="headerlink" title="4.2.1 transfer learning"></a>4.2.1 transfer learning</h3><p>【微调就是预训练一个网络，然后在新的数据集上微调】<br>为了了解attention transfer在微调中的作用，我们选择了两个数据集。Caltech-UCSD Birds-200-2011 fine-grained classification（CUB）和MIT室内场景分类（Scenes），两者都包含大约5K的图像训练图像。我们采用ResNet-18和ResNet-34在ImageNet上进行预训练，并在这两个数据集上进行了微调。在CUB上，我们裁剪边界框，在一个维度上重新缩放到256，然后采取随机裁剪。微调时，固定批量正则层，冻结第一组残余块。然后，我们把经过微调的ResNet-34网络作为在ImageNet上预训练的ResNet-18的老师，使用最后两组的F2sum attention loss 【第一组被冻结了】。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913174040.png"></p>
<p>如表所示，在这两种情况下，attention transfer都提供了明显的改进，缩小了ResNet-18和ResNet-34在准确性上的差距。在Scenes上，AT的效果和KD一样好，而在CUB上，AT的效果要好得多，我们推测这是因为中间注意力对细粒度识别的重要性。此外，经过微调后，学生的注意力图确实与教师的注意力图更加相似。【对于细粒度的重要性可不可以有别的验证方式？？？至少也像下面这样给个图吧orz】</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913174420.png"></p>
<ul>
<li>顶层activation attention map：列1是微调前的（会有分类错误），列2是学生网络，列3是AT后的，列4是教师网络，效果理论上该越来越好。比如行1就越来越接近室内的场景结构【可以用线框模型画一下图！attention map能不能用到室内场景场景建模上，即使是分类错误的预训练模型，也感觉可以得到结构信息！！！】</li>
</ul>
<h3 id="4-2-2-ImageNet"><a href="#4-2-2-ImageNet" class="headerlink" title="4.2.2 ImageNet"></a>4.2.2 ImageNet</h3><p>为了在ImageNet上展示activation-based AT，我们把ResNet-18作为学生，把ResNet-34作为老师，并试图提高ResNet-18的准确性。</p>
<p>我们在最后两组残余块中增加了两个损失，并使用F2sum。我们也没有时间调整任何超参数，并阻止它们进行微调。尽管如此，带有AT的ResNet-18取得了1.1% top-1和0.8% top-5的较好validation accuracy，<strong>我们计划用所有的4组残余块的损失来进阶我们的论文</strong>。</p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913180258.png"></p>
<p><img src="https://raw.githubusercontent.com/OrderAngel/Picture/main/study/paper/20210913180400.png"></p>
<ul>
<li>solid lines代表top-5 validation error，dashed lines代表top-5 training error。只用了AT损失，没用KD。</li>
</ul>
<p>我们没能用KD在ImageNet上取得积极的结果。在ResNet-18-ResNet-34师生对中，它实际上损害了与CIFAR上相同的超参数的收敛性【？】。如果教师和学生有不同的架构/深度，KD就很难发挥作用（我们在CIFAR上观察到同样的情况）【why？？？】，因此我们尝试使用相同的架构和深度来transfer attention【不知道是不是笔误，但是AT没用相同的深度吧？，还是说KD必须深度相同，但是AT不用？应该是笔误，深度相同结构也相同还迁移个啥！】。</p>
<p>在CIFAR上，AT和KD在这种情况下都工作得很好，并且提高了收敛性和最终的准确性，但在ImageNet上，KD的收敛速度明显较慢（由于缺乏计算资源，我们没有训练到最后）。我们在文献中也找不到FitNets、KD或类似方法在ImageNet上的应用。鉴于此，我们可以认为提出的基于激活的AT是第一个在ImageNet上成功应用的知识转移方法。【为啥那俩用不了？？？】</p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>我们提出了几种（2种）将注意力从一个网络转移到另一个网络的方法，并对几个图像识别数据集进行了实验结果。有趣的是，在空间信息更重要的情况下（例如目标检测或弱监督定位）attentio transfer是如何工作的，这也是我们<strong>计划在未来探索的问题</strong>。</p>
<p>总的来说，我们认为我们的方法将有助于知识蒸馏的进一步发展，也有助于卷积神经网络的普遍理解。</p>
<h1 id="6-问题"><a href="#6-问题" class="headerlink" title="6. 问题"></a>6. 问题</h1><ol>
<li><p>这两种注意力地图有啥区别</p>
</li>
<li><p>知识蒸馏是迁移学习的一种，attention在transfer learning中的其他应用</p>
</li>
<li><p>数据集ImageNet等</p>
</li>
<li><p>beita是什么</p>
</li>
<li><p>教师网络给学生网络注意力地图不是已经是知识蒸馏了吗？为何还可以与知识蒸馏相结合重新定义损失函数？</p>
<p>solution：文章刚出炉那会，AT还不属于知识蒸馏的方法，文中的KD指的就是开山的那篇soft-target辅助hard-target一起训练的文章；</p>
</li>
<li><p>水平翻转不变性有啥作用？对于训练有正则化作用？？</p>
<p>此处的水平翻转是数据增广的一种方案，数据增广可以减少过拟合</p>
<p>不过，为什么只做水平翻转？？？</p>
<p>因为有的动物图片翻转后就是倒着的了，没有实际意义；但是对于人脸这种具有对称结构的情况来说，只做左右翻转没有太大意义，无对称结构的图还有有用的！</p>
<p>除了水平翻转，或许还可以部分置黑</p>
</li>
<li><p>如何在更高层的网络中考虑attention transfer</p>
</li>
<li><p>文章中的attention蒸馏适用于教师模型和学生模型整体结构类似的情况</p>
</li>
<li><p> 本文的ATLoss可以与RKD、PKT、SP、VID等用来学习网络最终输出蒸馏的方式结合使用，仿佛：AT和PKT结合会产生不错的效果</p>
</li>
<li><p>可以介绍一下其他知识蒸馏的方法，再说结合使用的好处</p>
</li>
<li><p>文章是分类，能否在目标检测中使用</p>
</li>
<li><p>从第一个实验的结果来看，层数更高AT效果更好，wider的话KD效果更好，但是也有结合后效果不好的，文章没有专门对比KD和AT，也没有对结合后效果不好的情况专门做讨论。</p>
</li>
<li><p>使用批量归一化来探索师生对的gradient-based attention</p>
</li>
<li><p>symmetry norm在其他数据集上的表现</p>
</li>
<li><p>l1正则化也行：文章只说了也行，但确实没做实验，未来可以探索一下</p>
</li>
<li><p>查一下作者对gradient和symmetry的探索</p>
</li>
<li><p>白化不好，但没实验</p>
</li>
<li><p>4.2.2是不是笔误，因为网络结构相同可以理解，但是深度肯定要不同啊。</p>
</li>
<li><p>如果网络结构不同，怎么知识蒸馏？为什么结构不同不行？</p>
</li>
</ol>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>科研</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
</search>
